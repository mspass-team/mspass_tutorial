{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# This notebook should run under HPC environments, it's not gonna work on your local computer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 4:  Parallel Processing\n",
    "This is a placeholder to preserve the RF estimation section that was first in session 1 but we realized it was too much for an overview session. This workflow, however, is a good prototype for something that does something useful and demonstrates mspass in a parallel setting."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "from mspasspy.db.database import Database\n",
    "from mspasspy.db.client import DBClient\n",
    "hostname = os.environ.get('HOSTNAME')\n",
    "dbclient = DBClient(hostname)\n",
    "db=Database(dbclient,'SG2021')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare functions that will be used in the workflow\n",
    "\n",
    "### arrival_slowness_vector\n",
    "Given an obspy arrival array member return the mspass SlownessVector.\n",
    "\n",
    "Obspy's taup calculator returns travel time data as a list with one class member for \n",
    "each seismic phase. Inside that thing is a ray parameter, which is slowness in sec/degree.  \n",
    "A slowness vector has direction so we need to compute the direction from the azimuth."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from obspy.geodetics import degrees2kilometers\n",
    "import math\n",
    "from mspasspy.ccore.seismic import SlownessVector\n",
    "\n",
    "def arrival_slowness_vector(obspy_arrival,azimuth=0.0):\n",
    "    \"\"\"\n",
    "    :param obspy_arrival: list member for which the slowness vector is to be computed.\n",
    "    :param azimuth:  azimuth in degrees of propagation direction at receiver. \n",
    "    \n",
    "    :return: SlownessVector form model estimate for this phase.\n",
    "    \n",
    "    \"\"\"\n",
    "    # theta is the standard angle in math definition of polar coordinate angle (degrees)\n",
    "    theta=90.0-azimuth\n",
    "    rtheta=math.radians(theta)   # radians needed for math calculations\n",
    "    p=obspy_arrival.ray_param_sec_degree\n",
    "    u=p/degrees2kilometers(1.0)\n",
    "    ux=u*math.cos(rtheta)\n",
    "    uy=u*math.sin(rtheta)\n",
    "    return SlownessVector(ux,uy,0.0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### set_P_time\n",
    "Sets a predicted P wave arrival time using source and receiver coordinates and \n",
    "    model passed as a parameter and time shifts data so time 0 is the predicted P wave arrival time."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from mspasspy.util.decorators import mspass_func_wrapper\n",
    "# We need this function to handle setting arrival times.\n",
    "@mspass_func_wrapper\n",
    "def set_P_time(d,model):\n",
    "    stalat=d['site_lat']\n",
    "    stalon=d['site_lon']\n",
    "    srclat=d['source_lat']\n",
    "    srclon=d['source_lon']\n",
    "    depth=d['source_depth']\n",
    "    otime=d['source_time']\n",
    "    georesult=gps2dist_azimuth(srclat,srclon,stalat,stalon)\n",
    "    # obspy's function we just called returns distance in m in element 0 of a tuple\n",
    "    # With their travel time calculator it is degrees so we need this conversion\n",
    "    dist=kilometers2degrees(georesult[0]/1000.0)\n",
    "    baz=georesult[2]  # gps2dist_azimuth returns back azimuth as 2 of tuple.  We need azimuth\n",
    "    azimuth=baz+180.0\n",
    "    if azimuth>360.0:\n",
    "        azimuth -= 360.0\n",
    "    # the taup calculator fails if we ask for P in the core shadow.  This is a rough \n",
    "    # way to handle this for this example that works for the one event we are processing here\n",
    "    # A more elegant method would worry about source depth\n",
    "    if dist>95.0:\n",
    "        d.kill()\n",
    "        d.elog.log_error('session1_RF_script','No P wave - station is in the core shadow',\n",
    "                         ErrorSeverity.Invalid)\n",
    "    else:\n",
    "        arrivals=model.get_travel_times(source_depth_in_km=depth,distance_in_degree=dist,phase_list=['P'])\n",
    "        # Arrivals are returned in time order 0 is always the first arrival\n",
    "        # This computes arrival time as an epoch time and shifts the data to put 0 at that time\n",
    "        a=arrivals[0]\n",
    "        atime=a.time\n",
    "        # Post the time used to Metadata\n",
    "        d['P_iasp91']=atime   # Illustrates a made up key for Metadata\n",
    "        d.ator(otime+atime)\n",
    "        # We also post the slowness data - computed by this function\n",
    "        u=arrival_slowness_vector(a,azimuth)\n",
    "        d['ux']=u.ux\n",
    "        d['uy']=u.uy\n",
    "    return d"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### apply_free_surface_transformation\n",
    "Computes and applies the Kennett [1991] free surface transformation matrix.\n",
    "\n",
    "Kennett [1991] gives the form for a free surface transformation operator\n",
    "that reduces to a nonorthogonal transformation matrix when the wavefield is\n",
    "not evanescent.  On output x1 will be transverse, x2 will be SV (radial),\n",
    "and x3 will be longitudinal."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "@mspass_func_wrapper\n",
    "def apply_free_surface_transformation(d,vp0,vs0):\n",
    "    \"\"\"\n",
    "    Thin wrapper for free_surface_transformation method of Seismogram that assumes\n",
    "    the components of a slowness vector for the transformation are in the Metadata \n",
    "    of d stored with the keys ux and uy\n",
    "    \"\"\"\n",
    "    if d.dead():\n",
    "        return d\n",
    "    if 'ux' in d and 'uy' in d:\n",
    "        ux=d['ux']\n",
    "        uy=d['uy']\n",
    "        u = SlownessVector(ux,uy,0.0)\n",
    "        d.free_surface_transformation(u,vp0,vs0)\n",
    "    else:\n",
    "        d.elog.log_error('session1_RF_script','Slowness vector components were not set',\n",
    "                         ErrorSeverity.Invalid)\n",
    "        d.kill()\n",
    "    return d"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### More functions\n",
    "More functions can be found in our source code"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RF Estimation workflow:  Serial version\n",
    "Above we assembled data into Seismogram objects and saved them to the database.  In this example workflow we will generate a set of receiver function estimates driven by Seismogram inputs.  The serial job is a data driven loop over all Seismogram objects stored in the database.  For each seismogram we will do the following calculations:\n",
    "1.  Detrend the data (for a Seismogram that means channel by channel)\n",
    "2.  Lightly taper the ends to reduce filter startup transients.\n",
    "3.  Bandpass filter the data.\n",
    "4.  Window the data around the P wave arrival time.\n",
    "5.  Run the deconvolution algorithm.\n",
    "6.  Save the results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### parameter setting\n",
    "MsPASS allows parameters to be placed in a Antelope Pf format file.  We use that here as an example of how to put parameters for a workflow in one place\n",
    "\n",
    "When using a pf to define constants always do that up front in case there are errors in the file\n",
    "\n",
    "Example: session1.pf\n",
    "\n",
    "data_taper_length 10.0 \\\n",
    "filter_high_corner 2.0 \\\n",
    "filter_low_corner 0.02 \\\n",
    "analysis_window_starttime -200.0 \\\n",
    "analysis_window_endtime 200.0 \\\n",
    "vp0 6.0 \\\n",
    "vs0 3.5"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import time\n",
    "from mspasspy.algorithms.RFdeconProcessor import RFdeconProcessor\n",
    "from mspasspy.algorithms.RFdeconProcessor import RFdecon\n",
    "from mspasspy.ccore.utility import AntelopePf\n",
    "from mspasspy.algorithms.window import WindowData\n",
    "from mspasspy.algorithms.signals import (filter, detrend)\n",
    "from mspasspy.ccore.algorithms.basic import TimeWindow,CosineTaper\n",
    "from mspasspy.ccore.utility import ErrorSeverity\n",
    "from obspy.taup import TauPyModel\n",
    "model = TauPyModel(model=\"iasp91\")\n",
    "from obspy.geodetics import gps2dist_azimuth,kilometers2degrees\n",
    "\n",
    "pfhandle=AntelopePf('session1.pf')\n",
    "\n",
    "dtaperlength=pfhandle.get_double(\"data_taper_length\")\n",
    "fmax=pfhandle.get_double(\"filter_high_corner\")\n",
    "fmin=pfhandle.get_double(\"filter_low_corner\")\n",
    "awin_start=pfhandle.get_double(\"analysis_window_starttime\")\n",
    "awin_end=pfhandle.get_double(\"analysis_window_endtime\")\n",
    "vp0=pfhandle.get_double('vp0')\n",
    "vs0=pfhandle.get_double('vs0')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create a RFdeconProcessor instance(used in deconvolution method)\n",
    "There is a fair amount of overhead to create the slepian tapers used in \n",
    "the multitaper method. We create an instance that defines the operator\n",
    "once and use it in the workflow below\n",
    "\n",
    "Supported algorithms:\n",
    "1. LeastSquares\n",
    "2. WaterLevel\n",
    "3. MultiTaperXcor\n",
    "4. MultiTaperSpecDiv\n",
    "5. GeneralizedIterative"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "decon_operator=RFdeconProcessor(alg=\"MultiTaperXcor\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Obtain a record in wf_Seismogram collection"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# the size of input seismograms\n",
    "doc=db.wf_Seismogram.find_one({})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# see what it looks like\n",
    "print(doc)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'_id': ObjectId('610d546d5719f54c84c12904'), 'cardinal': False, 'delta': 0.025, 'hang': 0.0, 'nbytes': 143360, 'site_id': ObjectId('600fff404b4f9e654b4dd645'), 'dfile': 'file5', 'orthogonal': False, 'channel_endtime': 1367193599.0, 'vang': 0.0, 'channel_elev': 0.185, 'chan': 'BHZ', 'sampling_rate': 40.0, 'channel_starttime': 1349740800.0, 'channel_lon': -90.571503, 'source_id': ObjectId('61076db5ad4e0df4015f547c'), 'channel_edepth': 0.0, 'last_packet_time': 1356825909.865, 'npts': 24001, 'channel_id': ObjectId('600fff404b4f9e654b4dd647'), 'foff': 0, 'starttime': 1356822806.9258306, 'tmatrix': [0.0, 0.0, 1.0, 2.6484540326036093e-14, 1.0, 2.6484540326036093e-14, 1.0, 0.0, 2.6484540326036093e-14], 'time_standard': 'UTC', 'utc_convertible': True, 'dir': '/tmp/data_files', 'channel_lat': 37.361099, 'storage_mode': 'file', 'history_object_id': 'bd660c4f-f5e3-4a4a-8511-cc0f49196ada', 'data_tag': 'rawdata'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Read the record and return a Seismogram class object"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "normlist=['source','site']\n",
    "d=db.read_data(doc,collection='wf_Seismogram',normalize=normlist)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "72003\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Detrend the Seismogram"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "detrend(d)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Seismogram({'_id': ObjectId('610d546d5719f54c84c12904'), 'calib': 1.000000, 'cardinal': False, 'chan': 'E', 'channel_edepth': 0.000000, 'channel_elev': 0.185000, 'channel_endtime': 1367193599.000000, 'channel_id': ObjectId('600fff404b4f9e654b4dd647'), 'channel_lat': 37.361099, 'channel_lon': -90.571503, 'channel_starttime': 1349740800.000000, 'data_tag': 'rawdata', 'delta': 0.025000, 'dfile': 'file5', 'dir': '/tmp/data_files', 'endtime': 1356823406.925831, 'foff': 0, 'hang': 0.000000, 'history_object_id': 'bd660c4f-f5e3-4a4a-8511-cc0f49196ada', 'last_packet_time': 1356825909.865000, 'loc': '', 'nbytes': 143360, 'net': 'ZL', 'npts': 24001, 'orthogonal': False, 'processing': [\"ObsPy 1.2.2: detrend(options={}::type='simple')\"], 'sampling_rate': 40.000000, 'site_elev': 0.185000, 'site_endtime': 1367193599.000000, 'site_id': ObjectId('600fff404b4f9e654b4dd645'), 'site_lat': 37.361099, 'site_lon': -90.571503, 'site_starttime': 1349740800.000000, 'source_depth': 32.800000, 'source_id': ObjectId('61076db5ad4e0df4015f547c'), 'source_lat': 37.019400, 'source_lon': 141.289500, 'source_magnitude': 4.900000, 'source_time': 1356822322.840000, 'sta': 'N27M', 'starttime': 1356822806.925831, 'storage_mode': 'file', 'time_standard': 'UTC', 'tmatrix': [0.0, 0.0, 1.0, 2.6484540326036093e-14, 1.0, 2.6484540326036093e-14, 1.0, 0.0, 2.6484540326036093e-14], 'utc_convertible': True, 'vang': 0.000000})"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bandpass filtering the Seismogram"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "filter(d,'bandpass',freqmax=fmax,freqmin=fmin)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Seismogram({'_id': ObjectId('610d546d5719f54c84c12904'), 'calib': 1.000000, 'cardinal': False, 'chan': 'E', 'channel_edepth': 0.000000, 'channel_elev': 0.185000, 'channel_endtime': 1367193599.000000, 'channel_id': ObjectId('600fff404b4f9e654b4dd647'), 'channel_lat': 37.361099, 'channel_lon': -90.571503, 'channel_starttime': 1349740800.000000, 'data_tag': 'rawdata', 'delta': 0.025000, 'dfile': 'file5', 'dir': '/tmp/data_files', 'endtime': 1356823406.925831, 'foff': 0, 'hang': 0.000000, 'history_object_id': 'bd660c4f-f5e3-4a4a-8511-cc0f49196ada', 'last_packet_time': 1356825909.865000, 'loc': '', 'nbytes': 143360, 'net': 'ZL', 'npts': 24001, 'orthogonal': False, 'processing': [\"ObsPy 1.2.2: detrend(options={}::type='simple')\", \"ObsPy 1.2.2: filter(options={'freqmax': 2.0, 'freqmin': 0.02}::type='bandpass')\", \"ObsPy 1.2.2: filter(options={'freqmax': 2.0, 'freqmin': 0.02}::type='bandpass')\", \"ObsPy 1.2.2: filter(options={'freqmax': 2.0, 'freqmin': 0.02}::type='bandpass')\"], 'sampling_rate': 40.000000, 'site_elev': 0.185000, 'site_endtime': 1367193599.000000, 'site_id': ObjectId('600fff404b4f9e654b4dd645'), 'site_lat': 37.361099, 'site_lon': -90.571503, 'site_starttime': 1349740800.000000, 'source_depth': 32.800000, 'source_id': ObjectId('61076db5ad4e0df4015f547c'), 'source_lat': 37.019400, 'source_lon': 141.289500, 'source_magnitude': 4.900000, 'source_time': 1356822322.840000, 'sta': 'N27M', 'starttime': 1356822806.925831, 'storage_mode': 'file', 'time_standard': 'UTC', 'tmatrix': [0.0, 0.0, 1.0, 2.6484540326036093e-14, 1.0, 2.6484540326036093e-14, 1.0, 0.0, 2.6484540326036093e-14], 'utc_convertible': True, 'vang': 0.000000})"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use CosineTaper to taper the Seismogram"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "dtaper=CosineTaper(d.t0,d.t0+dtaperlength,d.endtime()-dtaperlength,d.endtime())\n",
    "dtaper.apply(d)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Time Window the Seismogram\n",
    "\n",
    "1. compute delta and azimuth"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "stalat=d['site_lat']\n",
    "stalon=d['site_lon']\n",
    "srclat=d['source_lat']\n",
    "srclon=d['source_lon']\n",
    "depth=d['source_depth']\n",
    "otime=d['source_time']\n",
    "\n",
    "georesult=gps2dist_azimuth(srclat,srclon,stalat,stalon)\n",
    "# obspy's function we just called returns distance in m in element 0 of a tuple\n",
    "# their travel time calculator it is degrees so we need this conversion\n",
    "dist=kilometers2degrees(georesult[0]/1000.0)\n",
    "baz=georesult[2]  # gps2dist_azimuth returns back azimuth as 2 of tuple.  We need azimuth\n",
    "azimuth=baz+180.0\n",
    "if azimuth>360.0:\n",
    "    azimuth -= 360.0\n",
    "print('delta=',dist,' azimuth=',azimuth)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "delta= 91.72518858842062  azimuth= 141.11883513432912\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Compute arrival time and shift t0 to P wave arrival time"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "arrivals=model.get_travel_times(source_depth_in_km=depth,distance_in_degree=dist,phase_list=['P'])\n",
    "# Arrivals are returned in time order 0 is always the first arrival\n",
    "# This computes arrival time as an epoch time and shifts the data to put 0 at that time\n",
    "a=arrivals[0]\n",
    "atime=a.time\n",
    "# Shift time 0 to the P wave arrival time\n",
    "d.ator(otime+atime)\n",
    "# Post the time used to Metadata\n",
    "d['P_iasp91']=atime   # Illustrates a made up key for Metadata"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Window Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "decon_twin=TimeWindow(awin_start,awin_end)\n",
    "print(decon_twin.start,decon_twin.end)\n",
    "print(d.t0,d.endtime())\n",
    "print('sample interval=',d.dt,' and number of points=',d.npts)\n",
    "d=WindowData(d,awin_start,awin_end)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-200.0 200.0\n",
      "-300.0 300.0\n",
      "sample interval= 0.025  and number of points= 24001\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Apply free surface transformation matrix"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "u=arrival_slowness_vector(a,azimuth)\n",
    "d.free_surface_transformation(u,vp0,vs0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Apply deconvolution algorithm"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "decondata=RFdecon(d,decon_operator)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check the Seismogram is alive after processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "decondata.live"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save the Seismogram after workflow"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "db.save_data(decondata, data_tag='example_output')\n",
    "print('The seismogram is saved successfully or not: ', decondata.live)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The seismogram is saved successfully or not:  True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run a serial workflow with 10 Seismograms and measure performance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# the size of input seismograms\n",
    "record_num = 10\n",
    "cursor=db.wf_Seismogram.find({},limit=record_num)\n",
    "\n",
    "t0=time.time()\n",
    "nlive=0\n",
    "normlist=['source','site']\n",
    "for doc in cursor:\n",
    "    d=db.read_data(doc,collection='wf_Seismogram',normalize=normlist)\n",
    "    print('working on data for station=',d['sta'])\n",
    "    # detrend\n",
    "    detrend(d)\n",
    "    # bandpass filter\n",
    "    filter(d,'bandpass',freqmax=fmax,freqmin=fmin)\n",
    "    # cosine taper ends\n",
    "    dtaper=CosineTaper(d.t0,d.t0+dtaperlength,d.endtime()-dtaperlength,d.endtime())\n",
    "    dtaper.apply(d)\n",
    "    # Time windowing - variant of above example \n",
    "    stalat=d['site_lat']\n",
    "    stalon=d['site_lon']\n",
    "    srclat=d['source_lat']\n",
    "    srclon=d['source_lon']\n",
    "    depth=d['source_depth']\n",
    "    otime=d['source_time']\n",
    "    georesult=gps2dist_azimuth(srclat,srclon,stalat,stalon)\n",
    "    # obspy's function we just called returns distance in m in element 0 of a tuple\n",
    "    # their travel time calculator it is degrees so we need this conversion\n",
    "    dist=kilometers2degrees(georesult[0]/1000.0)\n",
    "    baz=georesult[2]  # gps2dist_azimuth returns back azimuth as 2 of tuple.  We need azimuth\n",
    "    azimuth=baz+180.0\n",
    "    if azimuth>360.0:\n",
    "        azimuth -= 360.0\n",
    "    print('DEBUG:  delta=',dist,' azimuth=',azimuth)\n",
    "    if dist>95.0:\n",
    "        d.kill()\n",
    "        d.elog.log_error('session1_serial_script','No P wave - station is in the core shadow',ErrorSeverity.Invalid)\n",
    "        print('Killed this datum - core shadow')\n",
    "        db.save_data(d,data_tag='decon_output')\n",
    "        continue\n",
    "    arrivals=model.get_travel_times(source_depth_in_km=depth,distance_in_degree=dist,phase_list=['P'])\n",
    "    # Arrivals are returned in time order 0 is always the first arrival\n",
    "    # This computes arrival time as an epoch time and shifts the data to put 0 at that time\n",
    "    a=arrivals[0]\n",
    "    atime=a.time\n",
    "    # Shift time 0 to the P wave arrival time\n",
    "    d.ator(otime+atime)\n",
    "    # Post the time used to Metadata\n",
    "    d['P_iasp91']=atime   # Illustrates a made up key for Metadata\n",
    "    decon_twin=TimeWindow(awin_start,awin_end)\n",
    "    print('DEBUG')\n",
    "    print(decon_twin.start,decon_twin.end)\n",
    "    print(d.t0,d.endtime())\n",
    "    print('sample interval=',d.dt,' and number of points=',d.npts)\n",
    "    if decon_twin.start < d.t0:\n",
    "        d.kill()\n",
    "        d.elog.log_error('session_1_serial_script',\n",
    "                         'Windowing failure - window start is before data starttime',ErrorSeverity.Invalid)\n",
    "        print('killed this datum - windowing error')\n",
    "        db.save_data(d,data_tag='decon_output')\n",
    "    else:\n",
    "        d=WindowData(d,awin_start,awin_end)\n",
    "        # We transform the data to R,T,L using Kennett's free surface transformation matrix, which \n",
    "        # is implemented as a method in Seismogram\n",
    "        u=arrival_slowness_vector(a,azimuth)\n",
    "        d.free_surface_transformation(u,vp0,vs0)\n",
    "        # run deconvolution\n",
    "        decondata=RFdecon(d,decon_operator)\n",
    "        # save result with a different data tag - automatically will go to wf_Seismogram\n",
    "        db.save_data(decondata, data_tag='decon_output')\n",
    "        if decondata.live:\n",
    "            nlive+=1\n",
    "print('Total processing time=',time.time()-t0)\n",
    "print('Number of live data save=',nlive)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "72003\n",
      "working on data for station= N27M\n",
      "DEBUG:  delta= 91.72518858842062  azimuth= 141.11883513432912\n",
      "DEBUG\n",
      "-200.0 200.0\n",
      "-300.0 300.0\n",
      "sample interval= 0.025  and number of points= 24001\n",
      "72003\n",
      "working on data for station= N26I\n",
      "DEBUG:  delta= 92.15101729097918  azimuth= 142.262323153661\n",
      "DEBUG\n",
      "-200.0 200.0\n",
      "-300.0 300.0\n",
      "sample interval= 0.025  and number of points= 24001\n",
      "72003\n",
      "working on data for station= N24I\n",
      "DEBUG:  delta= 91.7109735715287  azimuth= 142.39437565942728\n",
      "DEBUG\n",
      "-200.0 200.0\n",
      "-300.0 300.0\n",
      "sample interval= 0.025  and number of points= 24001\n",
      "72003\n",
      "working on data for station= N23I\n",
      "DEBUG:  delta= 91.7479626153363  azimuth= 141.7304510246518\n",
      "DEBUG\n",
      "-200.0 200.0\n",
      "-300.0 300.0\n",
      "sample interval= 0.025  and number of points= 24001\n",
      "72003\n",
      "working on data for station= N22I\n",
      "DEBUG:  delta= 91.27370263356272  azimuth= 141.83984276475735\n",
      "DEBUG\n",
      "-200.0 200.0\n",
      "-300.0 300.0\n",
      "sample interval= 0.025  and number of points= 24001\n",
      "72003\n",
      "working on data for station= N21M\n",
      "DEBUG:  delta= 90.50405114404386  azimuth= 140.85613233059325\n",
      "DEBUG\n",
      "-200.0 200.0\n",
      "-300.0 300.0\n",
      "sample interval= 0.025  and number of points= 24001\n",
      "90003\n",
      "working on data for station= W315\n",
      "DEBUG:  delta= 97.53941853051151  azimuth= 145.24291936269145\n",
      "Killed this datum - core shadow\n",
      "90003\n",
      "working on data for station= W31\n",
      "DEBUG:  delta= 97.76225857824988  azimuth= 145.30206098973673\n",
      "Killed this datum - core shadow\n",
      "90003\n",
      "working on data for station= W30\n",
      "DEBUG:  delta= 97.87654382241564  azimuth= 145.1706986454396\n",
      "Killed this datum - core shadow\n",
      "90003\n",
      "working on data for station= W29\n",
      "DEBUG:  delta= 98.1894652009924  azimuth= 145.26833598747402\n",
      "Killed this datum - core shadow\n",
      "Total processing time= 6.551241159439087\n",
      "Number of live data save= 5\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RF Estimation:  parallel job using Dask\n",
    "MsPASS has support for two schedulers:  Dask and Spark.  In this exercise we are going to use Dask because it is slightly simpler to use.  In a later section we will talk about details of this job script, but for now a key point is to demonstrate that a job script to run a parallel job in MsPASS has only minor differences from the serial version.\n",
    "\n",
    "We do have to make one point here to help you comprehend this job script;  a fundamental idea of both Spark and Dask is the idea of a map operator.  A map operator can be thought of as a function that takes a list of data objects (the dataset), does something to them, and creates a new list (dataset) of the modified data.  The schedulers handle the memory operations so the entire data set does not live in memory simultaneously. \n",
    "\n",
    "With that background, here is the above in parallel form (Note that for this notebook we could have dropped most of the initialization, but we retain it to emphasize the parallel structure):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example use for the map operation in mspass"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import dask.bag as daskbag\n",
    "from dask.distributed import Client as DaskClient\n",
    "\n",
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "daskclient = DaskClient(\"0.0.0.0:8786\")\n",
    "\n",
    "total = 0\n",
    "data_set = daskbag.from_sequence(range(100))\n",
    "data_set = data_set.map(inc)\n",
    "res = data_set.compute()\n",
    "print(res)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parallel workflow with 10 seismograms"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "import time\n",
    "import dask.bag\n",
    "from dask.distributed import Client as DaskClient\n",
    "from mspasspy.algorithms.RFdeconProcessor import RFdeconProcessor\n",
    "from mspasspy.algorithms.RFdeconProcessor import RFdecon\n",
    "from mspasspy.ccore.utility import AntelopePf\n",
    "# These are repeated from above, but useful to make this box standalone so one can more \n",
    "# easily just cut and paste to use it in another workflow\n",
    "from mspasspy.algorithms.window import WindowData\n",
    "from mspasspy.algorithms.basic import cosine_taper, free_surface_transformation\n",
    "from mspasspy.algorithms.signals import (filter, detrend)\n",
    "from mspasspy.ccore.algorithms.basic import TimeWindow\n",
    "from mspasspy.ccore.utility import ErrorSeverity\n",
    "from mspasspy.db.database import read_distributed_data\n",
    "from obspy.taup import TauPyModel\n",
    "model = TauPyModel(model=\"iasp91\")\n",
    "from obspy.geodetics import gps2dist_azimuth, kilometers2degrees\n",
    "normlist=['source','site']\n",
    "\n",
    "# These initializations are identical to the serial version\n",
    "# MsPASS allows parameters to be placed in a Antelope Pf format file.  We use \n",
    "# that here as an example of how to put parameters for a workflow in one place\n",
    "pfhandle=AntelopePf('session1.pf')\n",
    "# When using a pf to define constants always do that up front in case there are\n",
    "# errors in the file\n",
    "dtaperlength=pfhandle.get_double(\"data_taper_length\")\n",
    "fmax=pfhandle.get_double(\"filter_high_corner\")\n",
    "fmin=pfhandle.get_double(\"filter_low_corner\")\n",
    "awin_start=pfhandle.get_double(\"analysis_window_starttime\")\n",
    "awin_end=pfhandle.get_double(\"analysis_window_endtime\")\n",
    "vp0=pfhandle.get_double('vp0')\n",
    "vs0=pfhandle.get_double('vs0')\n",
    "\n",
    "# There is a fair amount of overhead to create the slepian tapers used in \n",
    "# the multitaper method.   We create an instance that defines the operator\n",
    "# once and use it in the loop below\n",
    "decon_operator=RFdeconProcessor(alg=\"MultiTaperXcor\")\n",
    "\n",
    "# initialize the dask client\n",
    "daskclient = DaskClient(hostname + ':8786')\n",
    "\n",
    "record_num = 10\n",
    "cursor=db.wf_Seismogram.find({}, limit=record_num)\n",
    "t0=time.time()\n",
    "\n",
    "# this script is identical to the serial script prior to this point.  \n",
    "# Here is the first fundamental change:  our for loop is replaced by \n",
    "# this parallel reader that builds a Dask bag used to define the data set\n",
    "dataset=read_distributed_data(db, cursor, normalize=normlist)\n",
    "dataset=dataset.map(detrend)\n",
    "dataset=dataset.map(filter,'bandpass', freqmax=fmax, freqmin=fmin)\n",
    "# cosine_taper parameters here are randomly assigned for test\n",
    "dataset=dataset.map(cosine_taper, 0.0, 30.0, 150.0, 180.0)\n",
    "dataset=dataset.map(set_P_time, model)\n",
    "dataset=dataset.map(WindowData, awin_start, awin_end)\n",
    "# slowVector here is randomly created for test\n",
    "dataset=dataset.map(apply_free_surface_transformation, vp0, vs0)\n",
    "dataset=dataset.map(RFdecon, decon_operator)\n",
    "dataset=dataset.map(db.save_data, collection='wf_Seismogram', data_tag='parallel_decon_output_10')\n",
    "save_result = dataset.compute()\n",
    "# number of seismogram saved\n",
    "nlive = 0\n",
    "for seis in save_result:\n",
    "    if seis.live:\n",
    "        nlive += 1\n",
    "\n",
    "print('Total processing time for 10 seismograms=', time.time()-t0)\n",
    "print('Number of live data save=',nlive)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total processing time for 10 seismograms= 4.2814295291900635\n",
      "Number of live data save= 5\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performance Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parallel workflow with 100 seismograms"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "record_num = 100\n",
    "cursor=db.wf_Seismogram.find({}, limit=record_num)\n",
    "t0=time.time()\n",
    "# this script is identical to the serial script prior to this point.  \n",
    "# Here is the first fundamental change:  our for loop is replaced by \n",
    "# this parallel reader that builds a Dask bag used to define the data set\n",
    "dataset=read_distributed_data(db, cursor, normalize=normlist)\n",
    "dataset=dataset.map(detrend)\n",
    "dataset=dataset.map(filter,'bandpass', freqmax=fmax, freqmin=fmin)\n",
    "# cosine_taper parameters here are randomly assigned for test\n",
    "dataset=dataset.map(cosine_taper, 0.0, 30.0, 150.0, 180.0)\n",
    "dataset=dataset.map(set_P_time, model)\n",
    "dataset=dataset.map(WindowData, awin_start, awin_end)\n",
    "# slowVector here is randomly created for test\n",
    "dataset=dataset.map(apply_free_surface_transformation, vp0, vs0)\n",
    "dataset=dataset.map(RFdecon, decon_operator)\n",
    "dataset=dataset.map(db.save_data, collection='wf_Seismogram', data_tag='parallel_decon_output_100')\n",
    "save_result = dataset.compute()\n",
    "# number of seismogram saved\n",
    "nlive = 0\n",
    "for seis in save_result:\n",
    "    if seis.live:\n",
    "        nlive += 1\n",
    "\n",
    "print('Total processing time for 100 seismograms=', time.time()-t0)\n",
    "print('Number of live data save=',nlive)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total processing time for 100 seismograms= 10.175490856170654\n",
      "Number of live data save= 80\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parallel workflow with 1000 seismograms"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "record_num = 1000\n",
    "cursor=db.wf_Seismogram.find({}, limit=record_num)\n",
    "t0=time.time()\n",
    "# this script is identical to the serial script prior to this point.  \n",
    "# Here is the first fundamental change:  our for loop is replaced by \n",
    "# this parallel reader that builds a Dask bag used to define the data set\n",
    "dataset=read_distributed_data(db, cursor, normalize=normlist)\n",
    "dataset=dataset.map(detrend)\n",
    "dataset=dataset.map(filter,'bandpass', freqmax=fmax, freqmin=fmin)\n",
    "# cosine_taper parameters here are randomly assigned for test\n",
    "dataset=dataset.map(cosine_taper, 0.0, 30.0, 150.0, 180.0)\n",
    "dataset=dataset.map(set_P_time, model)\n",
    "dataset=dataset.map(WindowData, awin_start, awin_end)\n",
    "# slowVector here is randomly created for test\n",
    "dataset=dataset.map(apply_free_surface_transformation, vp0, vs0)\n",
    "dataset=dataset.map(RFdecon, decon_operator)\n",
    "dataset=dataset.map(db.save_data, collection='wf_Seismogram', data_tag='parallel_decon_output_1000')\n",
    "save_result = dataset.compute()\n",
    "# number of seismogram saved\n",
    "nlive = 0\n",
    "for seis in save_result:\n",
    "    if seis.live:\n",
    "        nlive += 1\n",
    "\n",
    "print('Total processing time for 1000 seismograms=', time.time()-t0)\n",
    "print('Number of live data save=',nlive)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total processing time for 1000 seismograms= 31.70405912399292\n",
      "Number of live data save= 874\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parallel workflow with 10000 seismograms"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "record_num = 10000\n",
    "cursor=db.wf_Seismogram.find({}, limit=record_num)\n",
    "t0=time.time()\n",
    "# this script is identical to the serial script prior to this point.  \n",
    "# Here is the first fundamental change:  our for loop is replaced by \n",
    "# this parallel reader that builds a Dask bag used to define the data set\n",
    "dataset=read_distributed_data(db, cursor, normalize=normlist)\n",
    "dataset=dataset.map(detrend)\n",
    "dataset=dataset.map(filter,'bandpass', freqmax=fmax, freqmin=fmin)\n",
    "# cosine_taper parameters here are randomly assigned for test\n",
    "dataset=dataset.map(cosine_taper, 0.0, 30.0, 150.0, 180.0)\n",
    "dataset=dataset.map(set_P_time, model)\n",
    "dataset=dataset.map(WindowData, awin_start, awin_end)\n",
    "# slowVector here is randomly created for test\n",
    "dataset=dataset.map(apply_free_surface_transformation, vp0, vs0)\n",
    "dataset=dataset.map(RFdecon, decon_operator)\n",
    "dataset=dataset.map(db.save_data, collection='wf_Seismogram', data_tag='parallel_decon_output_10000')\n",
    "save_result = dataset.compute()\n",
    "# number of seismogram saved\n",
    "nlive = 0\n",
    "for seis in save_result:\n",
    "    if seis.live:\n",
    "        nlive += 1\n",
    "\n",
    "print('Total processing time for 10000 seismograms=', time.time()-t0)\n",
    "print('Number of live data save=',nlive)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total processing time for 10000 seismograms= 294.7401704788208\n",
      "Number of live data save= 8836\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}