{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e483aff",
   "metadata": {},
   "source": [
    "# Session 4:  Parallel Processing\n",
    "This is a placeholder to preserve the RF estimation section that was first in session 1 but we realized it was too much for an overview session. This workflow, however, is a good prototype for something that does something useful and demonstrates mspass in a parallel setting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178adf7e",
   "metadata": {},
   "source": [
    "## RF Estimation workflow:  Serial version\n",
    "Above we assembled data into Seismogram objects and saved them to the database.  In this example workflow we will generate a set of receiver function estimates driven by Seismogram inputs.  The serial job is a data driven loop over all Seismogram objects stored in the database.  For each seismogram we will do the following calculations:\n",
    "1.  Detrend the data (for a Seismogram that means channel by channel)\n",
    "2.  Lightly taper the ends to reduce filter startup transients.\n",
    "3.  Bandpass filter the data.\n",
    "4.  Window the data around the P wave arrival time.\n",
    "5.  Run the deconvolution algorithm.\n",
    "6.  Save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada0cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.db.database import Database\n",
    "from mspasspy.db.client import DBClient\n",
    "dbclient=DBClient()\n",
    "db=Database(dbclient,'shortcourse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f910b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.geodetics import degrees2kilometers\n",
    "import math\n",
    "from mspasspy.ccore.seismic import SlownessVector\n",
    "def arrival_slowness_vector(obspy_arrival,azimuth=0.0):\n",
    "    \"\"\"\n",
    "    Given an obspy arrival array member return the mspass SlownessVector.\n",
    "    \n",
    "    Obspy's taup calculator returns travel time data as a list with one class member for \n",
    "    each seismic phase. Inside that thing is a ray parameter, which is slowness in sec/degree.  \n",
    "    A slowness vector has direction so we need to compute the direction from the azimuth.\n",
    "    \n",
    "    :param obspy_arrival: list member for which the slowness vector is to be computed.\n",
    "    :param azimuth:  azimuth in degrees of propagation direction at receiver. \n",
    "    \n",
    "    :return: SlownessVector form model estimate for this phase.\n",
    "    \n",
    "    \"\"\"\n",
    "    # theta is the standard angle in math definition of polar coordinate angle (degrees)\n",
    "    theta=90.0-azimuth\n",
    "    rtheta=math.radians(theta)   # radians needed for math calculations\n",
    "    p=a.ray_param_sec_degree\n",
    "    u=p/degrees2kilometers(1.0)\n",
    "    ux=u*math.cos(rtheta)\n",
    "    uy=u*math.sin(rtheta)\n",
    "    return SlownessVector(ux,uy,0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f4f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mport time\n",
    "from mspasspy.algorithms.RFdeconProcessor import RFdeconProcessor\n",
    "from mspasspy.algorithms.RFdeconProcessor import RFdecon\n",
    "from mspasspy.ccore.utility import AntelopePf\n",
    "# These are repeated from above, but useful to make this box standalone so one can more \n",
    "# easily just cut and paste to use it in another workflow\n",
    "from mspasspy.algorithms.window import WindowData\n",
    "from mspasspy.algorithms.signals import (filter, detrend)\n",
    "from mspasspy.ccore.algorithms.basic import TimeWindow,CosineTaper\n",
    "from mspasspy.ccore.utility import ErrorSeverity\n",
    "from obspy.taup import TauPyModel\n",
    "model = TauPyModel(model=\"iasp91\")\n",
    "from obspy.geodetics import gps2dist_azimuth,kilometers2degrees\n",
    "normlist=['source','site']\n",
    "\n",
    "# MsPASS allows parameters to be placed in a Antelope Pf format file.  We use \n",
    "# that here as an example of how to put parameters for a workflow in one place\n",
    "pfhandle=AntelopePf('session1.pf')\n",
    "# When using a pf to define constants always do that up front in case there are\n",
    "# errors in the file\n",
    "dtaperlength=pfhandle.get_double(\"data_taper_length\")\n",
    "fmax=pfhandle.get_double(\"filter_high_corner\")\n",
    "fmin=pfhandle.get_double(\"filter_low_corner\")\n",
    "awin_start=pfhandle.get_double(\"analysis_window_starttime\")\n",
    "awin_end=pfhandle.get_double(\"analysis_window_endtime\")\n",
    "vp0=pfhandle.get_double('vp0')\n",
    "vs0=pfhandle.get_double('vs0')\n",
    "\n",
    "# There is a fair amount of overhead to create the slepian tapers used in \n",
    "# the multitaper method.   We create an instance that defines the operator\n",
    "# once and use it in the loop below\n",
    "decon_operator=RFdeconProcessor(alg=\"MultiTaperXcor\")\n",
    "\n",
    "\n",
    "cursor=db.wf_Seismogram.find(query)\n",
    "t0=time.time()\n",
    "nlive=0\n",
    "for doc in cursor:\n",
    "    d=db.read_data(doc,collection='wf_Seismogram',normalize=normlist)\n",
    "    print('working on data for station=',d['READONLYERROR_sta'])\n",
    "    # detrend\n",
    "    detrend(d)\n",
    "    # bandpass filter\n",
    "    filter(d,'bandpass',freqmax=fmax,freqmin=fmin)\n",
    "    # cosine taper ends\n",
    "    dtaper=CosineTaper(d.t0,d.t0+dtaperlength,d.endtime()-dtaperlength,d.endtime())\n",
    "    dtaper.apply(d)\n",
    "    # Time windowing - variant of above example \n",
    "    stalat=d['site_lat']\n",
    "    stalon=d['site_lon']\n",
    "    srclat=d['source_lat']\n",
    "    srclon=d['source_lon']\n",
    "    depth=d['source_depth']\n",
    "    otime=d['source_time']\n",
    "    georesult=gps2dist_azimuth(srclat,srclon,stalat,stalon)\n",
    "    # obspy's function we just called returns distance in m in element 0 of a tuple\n",
    "    # their travel time calculator it is degrees so we need this conversion\n",
    "    dist=kilometers2degrees(georesult[0]/1000.0)\n",
    "    baz=georesult[2]  # gps2dist_azimuth returns back azimuth as 2 of tuple.  We need azimuth\n",
    "    azimuth=baz+180.0\n",
    "    if azimuth>360.0:\n",
    "        azimuth -= 360.0\n",
    "    #print('DEBUG:  delta=',dist,' azimuth=',azimuth)\n",
    "    if dist>95.0:\n",
    "        d.kill()\n",
    "        d.elog.log_error('session1_serial_script','No P wave - station is in the core shadow',ErrorSeverity.Invalid)\n",
    "        print('Killed this datum - core shadow')\n",
    "        db.save_data(d,data_tag='decon_output')\n",
    "        continue\n",
    "    arrivals=model.get_travel_times(source_depth_in_km=depth,distance_in_degree=dist,phase_list=['P'])\n",
    "    # Arrivals are returned in time order 0 is always the first arrival\n",
    "    # This computes arrival time as an epoch time and shifts the data to put 0 at that time\n",
    "    a=arrivals[0]\n",
    "    atime=a.time\n",
    "    # Shift time 0 to the P wave arrival time\n",
    "    d.ator(otime+atime)\n",
    "    # Post the time used to Metadata\n",
    "    d['P_iasp91']=atime   # Illustrates a made up key for Metadata\n",
    "    decon_twin=TimeWindow(awin_start,awin_end)\n",
    "    #print('DEBUG')\n",
    "    #print(decon_twin.start,decon_twin.end)\n",
    "    #print(d.t0,d.endtime())\n",
    "    #print('sample interval=',d.dt,' and number of points=',d.npts)\n",
    "    if decon_twin.start < d.t0:\n",
    "        d.kill()\n",
    "        d.elog.log_error('session_1_serial_script',\n",
    "                         'Windowing failure - window start is before data starttime',ErrorSeverity.Invalid)\n",
    "        print('killed this datum - windowing error')\n",
    "        db.save_data(d,data_tag='decon_output')\n",
    "    else:\n",
    "        d=WindowData(d,decon_twin)\n",
    "        # We transform the data to R,T,L using Kennett's free surface transformation matrix, which \n",
    "        # is implemented as a method in Seismogram\n",
    "        u=arrival_slowness_vector(a,azimuth)\n",
    "        d.free_surface_transformation(u,vp0,vs0)\n",
    "        #  run deconvolution \n",
    "        decondata=RFdecon(d,decon_operator)\n",
    "        # save result with a different data tag - automatically will go to wf_Seismogram\n",
    "        db.save_data(decondata,data_tag='decon_output')\n",
    "        nlive+=1\n",
    "print('Total processing time=',time.time()-t0)\n",
    "print('Number live data save=',nlive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32832e59",
   "metadata": {},
   "source": [
    "## RF Estimation:  parallel job using Dask\n",
    "MsPASS has support for two schedulers:  Dask and Spark.  In this exercise we are going to use Dask because it is slightly simpler to use.  In a later section we will talk about details of this job script, but for now a key point is to demonstrate that a job script to run a parallel job in MsPASS has only minor differences from the serial version.\n",
    "\n",
    "We do have to make one point here to help you comprehend this job script;  a fundamental idea of both Spark and Dask is the idea of a map operator.  A map operator can be thought of as a function that takes a list of data objects (the dataset), does something to them, and creates a new list (dataset) of the modified data.  The schedulers handle the memory operations so the entire data set does not live in memory simultaneously. \n",
    "\n",
    "With that background, here is the above in parallel form (Note that for this notebook we could have dropped most of the initialization, but we retain it to emphasize the parallel structure):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d5d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from mspasspy.algorithms.RFdeconProcessor import RFdeconProcessor\n",
    "from mspasspy.algorithms.RFdeconProcessor import RFdecon\n",
    "from mspasspy.ccore.utility import AntelopePf\n",
    "# These are repeated from above, but useful to make this box standalone so one can more \n",
    "# easily just cut and paste to use it in another workflow\n",
    "from mspasspy.algorithms.window import WindowData\n",
    "from mspasspy.algorithms.signals import (filter, detrend)\n",
    "from mspasspy.ccore.algorithms.basic import TimeWindow,CosineTaper\n",
    "from mspasspy.ccore.utility import ErrorSeverity\n",
    "from obspy.taup import TauPyModel\n",
    "model = TauPyModel(model=\"iasp91\")\n",
    "from obspy.geodetics import gps2dist_azimuth,kilometers2degrees\n",
    "normlist=['source','site']\n",
    "\n",
    "# We need this function to handle setting arrival times. The serial script should be \n",
    "# changed to use this same function.\n",
    "def Ptime_shift(d,model):\n",
    "    \"\"\"\n",
    "    Sets a predicted P wave arrival time using source and receiver coordinates and \n",
    "    model passed as arg 1 and time shifts data so time 0 is the predicted P wave arrival time.\n",
    "    \"\"\"\n",
    "    stalat=d['site_lat']\n",
    "    stalon=d['site_lon']\n",
    "    srclat=d['source_lat']\n",
    "    srclon=d['source_lon']\n",
    "    depth=d['source_depth']\n",
    "    otime=d['source_time']\n",
    "    georesult=gps2dist_azimuth(srclat,srclon,stalat,stalon)\n",
    "    # obspy's function we just called returns distance in m in element 0 of a tuple\n",
    "    # With their travel time calculator it is degrees so we need this conversion\n",
    "    dist=kilometers2degrees(georesult[0]/1000.0)\n",
    "    baz=georesult[2]  # gps2dist_azimuth returns back azimuth as 2 of tuple.  We need azimuth\n",
    "    azimuth=baz+180.0\n",
    "    if azimuth>360.0:\n",
    "        azimuth -= 360.0\n",
    "    # the taup calculator fails if we ask for P in the core shadow.  This is a rough \n",
    "    # way to handle this for this example that works for the one event we are processing here\n",
    "    # A more elegant method would worry about source depth\n",
    "    if dist>95.0:\n",
    "        d.kill()\n",
    "        d.elog.log_error('session1_RF_script','No P wave - station is in the core shadow',\n",
    "                         ErrorSeverity.Invalid)\n",
    "    else:\n",
    "        arrivals=model.get_travel_times(source_depth_in_km=depth,distance_in_degree=dist,phase_list=['P'])\n",
    "        # Arrivals are returned in time order 0 is always the first arrival\n",
    "        # This computes arrival time as an epoch time and shifts the data to put 0 at that time\n",
    "        a=arrivals[0]\n",
    "        atime=a.time\n",
    "        # Post the time used to Metadata\n",
    "        d['P_iasp91']=atime   # Illustrates a made up key for Metadata\n",
    "        d.ator(otime+atime)\n",
    "        # We also post the slowness data - computed by this function\n",
    "        u=arrival_slowness_vector(a,azimuth)\n",
    "        d['ux']=u.ux\n",
    "        d['uy']=u.uy\n",
    "    return d\n",
    "def apply_free_surface_transformation(d,vp0,vs0):\n",
    "    \"\"\"\n",
    "    Thin wrapper for free_surface_transformation method of Seismogram that assumes\n",
    "    the components of a slowness vector for the transformation are in the Metadata \n",
    "    of d stored with the keys ux and uy\n",
    "    \"\"\"\n",
    "    if d.dead():\n",
    "        return d\n",
    "    if 'ux' in d and 'uy' in d:\n",
    "        ux=d['ux']\n",
    "        uy=d['uy']\n",
    "        u = SlownessVector(ux,uy)\n",
    "        d.free_surface_transformation(u,vp0,vs0)\n",
    "    else:\n",
    "        d.elog.log_error('session1_RF_script','Slowness vector components were not set',\n",
    "                         ErrorSeverity.Invalid)\n",
    "        d.kill()\n",
    "    return d\n",
    "# These initializations are identical to the serial version\n",
    "\n",
    "# MsPASS allows parameters to be placed in a Antelope Pf format file.  We use \n",
    "# that here as an example of how to put parameters for a workflow in one place\n",
    "pfhandle=AntelopePf('session1.pf')\n",
    "# When using a pf to define constants always do that up front in case there are\n",
    "# errors in the file\n",
    "dtaperlength=pfhandle.get_double(\"data_taper_length\")\n",
    "fmax=pfhandle.get_double(\"filter_high_corner\")\n",
    "fmin=pfhandle.get_double(\"filter_low_corner\")\n",
    "awin_start=pfhandle.get_double(\"analysis_window_starttime\")\n",
    "awin_end=pfhandle.get_double(\"analysis_window_endtime\")\n",
    "vp0=pfhandle.get_double('vp0')\n",
    "vs0=pfhandle.get_double('vs0')\n",
    "\n",
    "# There is a fair amount of overhead to create the slepian tapers used in \n",
    "# the multitaper method.   We create an instance that defines the operator\n",
    "# once and use it in the loop below\n",
    "decon_operator=RFdeconProcessor(alg=\"MultiTaperXcor\")\n",
    "\n",
    "\n",
    "cursor=db.wf_Seismogram.find(query)\n",
    "t0=time.time()\n",
    "\n",
    "# this script is identical to the serial script prior to this point.  \n",
    "# Here is the first fundamental change:  our for loop is replaced by \n",
    "# this parallel reader that builds a Dask bag used to define the data set\n",
    "dataset=db.read_distributed_data(cursor,collection='wf_Seismogram',normalize=normlist)\n",
    "dataset=dataset.map(detrend)\n",
    "dataset=dataset.map(filter,'bandpass',freqmax=fmax,freqmin=fmin)\n",
    "dtaper=CosineTaper(d.t0,d.t0+dtaperlength,d.endtime()-dtaperlength,d.endtime())\n",
    "dataset=dataset.map(dtaper.apply,inplace_return=True)\n",
    "dataset=dataset.map(set_P_time,model)\n",
    "dataset=dataset.map(WindowData,decon_twin)\n",
    "dataset=dataset.map(apply_free_surface_transformation,vp0,vs0)\n",
    "dataset=dataset.map(RFdecon,decon_operator)\n",
    "dataset.map(save_data,collection='wf_Seismogram',data_tag='parallel_decon_output')\n",
    "dataset.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9826375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
