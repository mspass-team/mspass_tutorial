{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1540c60-8932-4797-8b9a-9d5e1b18da99",
   "metadata": {},
   "source": [
    "# Session 3:  Cloud Computing\n",
    "## *MsPASS Team*\n",
    "\n",
    "## Learning Objectives\n",
    "- Students will learn that object store like AWS S3 can be used like a remote file system\n",
    "- Students will learn the basic uses of the python package boto3 to retrieve S3 data\n",
    "- Students will learn how to use MsPASS to manage information stored in S3 files\n",
    "- Students will learn how to use MsPASS to process data stored on S3 with GeoLab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f3495b-eeef-42d6-9ca3-ed0b0ccc5703",
   "metadata": {},
   "source": [
    "## Before Running this Notebook\n",
    "### Launch the database server\n",
    "\n",
    "As noted in the PrecourseProcessing notebook, an issue we haven't been able to resolve is how to have a private instance of MongoDB launch for each instance of a geolab login.  For that reason you will need to do a shortened version of the commands you used in the PrecourseProcessing notebook.  \n",
    "\n",
    "Launch a terminal window in GeoLab and enter these commands:\n",
    "```\n",
    "cd   #  makes sure you are running this in your home directory\n",
    "mongod --dbpath ./db --logpath ./logs/mongo_log\n",
    "```\n",
    "Note a couple things that can go wrong here:\n",
    "-  The command will block.  If you want to reuse that terminal window put an & at the end of the mongod launch line.\n",
    "-  DO NOT exit the terminal window from which you run the mongod command or you will kill the database server.  \n",
    "-  Each time you reconnect to GeoLab you will need to relaunch the MonogDB server with that  incantation. Be careful you always run the command from the same directory as that way the database files will be written to ~/db and the mongodb log will appear in ~/logs.\n",
    "-  In all cases it is wise to launch a second Terminal window and verify that worked by typing `ps -A`  You should see a line where the CMD field is \"mongod\".   If not, contact me by email or slack if you are unable to solve the problem.  The first place to look if you are having a problem is the content of the file ~/logs/mongo_log that will be generated when this command runs.\n",
    "\n",
    "### Use outside GeoLab\n",
    "If you are accessing this notebook from github and are not part of the 2025 short course you can still run this tutorial notebook on a desktop system. To do so you will need to do two things:\n",
    "1.  Install the `mspass-desktop` GUI described in __[MsPASS User's Manual here](https://www.mspass.org/getting_started/mspass_desktop.html)__\n",
    "2.  Launch MsPASS as described on that page and run this notebook using the \"run\" button on the GUI. Alternatively, you may run it interactively by pushing the \"jupyter\" button."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f101721-7ea5-42bb-a22b-3c7d7f390ac7",
   "metadata": {},
   "source": [
    "## S3 Objects\n",
    "\n",
    "In the lecture that precedes this hands on exercise, I will have introduced some fundamentals of cloud computing.   A few key points to take from that lecture for this exercise are:\n",
    "\n",
    "- File systems are an abstraction that has proven useful for decades but is problematic for large disk farms\n",
    "- On any computer a \"file\" is a binary blob that is useful only if you can parse the content\n",
    "- On cloud systems and HPC lustre file systems \"files\" are managed by a database engine that \"under the hood\" so you don't see it\n",
    "  * With Lustre the application makes the information act like a unix file system even if it isn't\n",
    "  * On AWS there is no \"file system\" but information is managed as \"object store\" with the acronym S3 (Simple Storate Service).\n",
    "- The standard python package today to work with AWS S3 is called \"boto3\".\n",
    "- SC and NC earthquake centers\n",
    "  *  Distribute their data via S3 in structures that mimic a file system\n",
    "  *  Provide anonymous access \n",
    "- Earthscope plans will be discussed later in this class but include\n",
    "  *  Require credentials to fufill NSF requirements\n",
    "  *  Will use a similar object store structure but with a different virtual directory structure\n",
    "- MsPASS has a prototype set of data retrieval methods that are part of the `Database` class.\n",
    "- For this exercise we are using second generation set of prototypes developed for this class\n",
    "\n",
    "## SCEDC Data\n",
    "### Overview\n",
    "\n",
    "For this exercise we will be manipulating open data from the Southern California Earthquake Data Center (hereafter called SCEDC).  The web site that is the definitive reference for their data is __[here](https://scedc.caltech.edu/data/cloud.html)__.   A feature they have implemented that our earlier prototype did not consider is a set of index files described on __[this page of their documentation](https://scedc.caltech.edu/data/search-index.html)__.   We are using an approach here that utilizes features of MsPASS that is an alternative to the \"Athens\" application that site recommends.  A very important advantage of MsPASS we will see is that queries can be assimilated into a parallel workflow.  \n",
    "\n",
    "### Index data\n",
    "With that background, the first step is to have a look at the index data SCEDC supplies.  There are two of them.  Both are actually small files retrieved from s3 using a function defined in a module we will look at momentarily called \"scedc_s3\" (the first line of this code box imports that file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96a2e8b-1263-4734-9d99-f127824a88df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scedc_s3 as scedc\n",
    "catalog_index=scedc.get_s3_index_keys(\"catalog\")\n",
    "wf_index=scedc.get_s3_index_keys(\"wf\")\n",
    "print(\"Number of keys in catalog index=\",len(catalog_index))\n",
    "print(\"Number of keys in waveform index=\",len(wf_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6ed98e-1c0e-481d-8df4-d854e0cff5c6",
   "metadata": {},
   "source": [
    "As you can see there are only two indices but a lot of items in each of them.   Let's look at what a typical example of both to understand how they can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8f202b-5e44-4854-b4e1-70faefadfad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First entry in catalog index=\",catalog_index[0])\n",
    "print(\"First entry in waveform index=\",wf_index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a34e00c-a69d-4cff-a454-070bad11023b",
   "metadata": {},
   "source": [
    "Aha, you should say.  Each of the entries in the index looks like the path to a unix file.   That reflects a tradition of how earthquake data has been managed in file systems since the first real-time systems using unix were developed in the late 1970s.   That is, a directory (in this case directory like) structure that puts data in some kind of time groupings.  You see here that these indices are two structures:\n",
    "1.  The \"catalog\" index defines a path to the SCEDC catalog, which in this case means a list of all earhquakes recorded for a year (1999 for the example)\n",
    "2.  The \"wf\" index (shorted from \"continous_waveforms\" tag in the dir definition) define directories of continuous day files (day 169 of 1999 for the example)\n",
    "\n",
    "The actual strings that define the \"index\" here is a unique key that defines a specific object in object store.  One assumes, in fact, that it literally is used as a key for a database search to retrieve the piece of information it references.  Both the keys above point to \"csv\" format files.\n",
    "\n",
    "We will be retrieving some of those files momentarily, but now we switch to a MsPASS way of doing something that is new for this course.  We believe it can be a generic solution to handle local data management working seismic data stored on the cloud.  \n",
    "\n",
    "The issue with an obnoxious key string like \"continuous_waveforms/index/csv/year=1999/year_doy=1999_169/1999_169_waveform_index.csv\" is that although it makes sense for a file system it cumbersome to manipulate as an actual entity;  the symbols *catalog_index* and *wf_index* are just lists of similar strings.  The approach we used below is to break the strings up into key-value pairs that make sense and can be pushed to MongoDB.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b844a0-8342-4865-a2cc-406df894714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  mspasspy.util.seismic import print_metadata\n",
    "catidoc=scedc.catkey2doc(catalog_index[0])\n",
    "wfidoc=scedc.wfkey2doc(wf_index[0])\n",
    "print(\"Catalog index key converted to a dictionary for MongoDB\")\n",
    "print_metadata(catidoc)\n",
    "print(\"Waveform index key converted to a dictionary for MongoDB\")\n",
    "print_metadata(wfidoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2f6fdd-9d00-44b0-addc-cffb0049a175",
   "metadata": {},
   "source": [
    "As you should see the python functions `catkey2doc` and `wfkey2doc` parsed that file path string and turned it into key-value pairs (i.e. a python dictionary).  Why that will prove useful is that when we insert documents like that into MongoDB we can query the result with time-based searches using the \"year\", \"yrday\", and/or \"day\" attributes.  \n",
    "\n",
    "With that, run this small code block to save the index keys we retrieved earlier into MongoDB with collection names the code defines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc8305a-3d69-45ac-98a7-6aa678fb0cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.db.database import Database\n",
    "import mspasspy.client as msc\n",
    "mspass_client = msc.Client()\n",
    "dbname=\"scedc_s3\"\n",
    "db = mspass_client.get_database(dbname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a3de1b-1dcf-4ca0-83ed-56812e645cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the two types of keys in two different collections - type example of appropriate grouping in a \"collection\"\n",
    "catindex_collection=\"s3_catalog_index\"\n",
    "wfindex_collection=\"s3_waveform_index\"\n",
    "for k in catalog_index:\n",
    "    doc = scedc.catkey2doc(k)\n",
    "    db[catindex_collection].insert_one(doc)\n",
    "for k in wf_index:\n",
    "    doc = scedc.wfkey2doc(k)\n",
    "    db[wfindex_collection].insert_one(doc)\n",
    "n=db[catindex_collection].count_documents({})\n",
    "print(\"Number of documents stored in {} collection={}\".format(catindex_collection,n))\n",
    "n=db[wfindex_collection].count_documents({})\n",
    "print(\"Number of documents stored i {} collection={}\".format(wfindex_collection,n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c8b97-fdf7-4924-b513-a3ac42cdc990",
   "metadata": {},
   "source": [
    "### Event File Retrieval\n",
    "\n",
    "Now that we have the indexes restructured into two MongoDB collections (\"s3_catalog_index\" and \"s3_waveform_index\") we can do queries.  For this tutorial we will retrieve the catalog data from 2023.  This next step retrieves the entire 2023 catalog from s3 and stored the result in a pandas *DataFrame*.  Notice with this algorithm \n",
    "\n",
    "- The database `find_one` method retrieves the index document we just stored for 2023.\n",
    "- That document is passed to the special function *s3_index_reader* that actually retrieves the data file (We'll look at that function now to see how it works)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af32dfe7-7107-4bca-89df-c18c0c5c65a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses these two temporary variables as shorthand to reduce typing\n",
    "catcol=db[catindex_collection]\n",
    "wficol=db[wfindex_collection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe5d4d1-c2e2-4c12-ae43-8b16b502540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query={\"year\" : 2023,\"format\" : \"parquet\"} \n",
    "n = catcol.count_documents(query)\n",
    "if n!=1:\n",
    "    raise RuntimeError(\"Something is wrong - that should be a unique match\")\n",
    "doc=catcol.find_one(query)\n",
    "df = scedc.s3_index_reader(doc)\n",
    "print(\"Index file retrieved from s3 and converted to pandas DataFrame\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5985c6ee-764e-41c0-b99c-a1d96296b901",
   "metadata": {},
   "source": [
    "If you have used pandas you will recognize that layout.  All are reminded that a pandas DataFrame, which is the symbol \"df\" is referencing is a data object to represent a table.   It is a very powerful data object, however, as the class has a large set of methods and the pandas module has a long list of functions that can be applied to dataframes.   The header lines on the printed output of that catalog data are reasonably self explanatory about the concept each cell represents.  \n",
    "\n",
    "For this exercise, we want to see how you can process an event file from this archive.  To do that we need to use the subsetting capability of the DataFrame object.  The next box is an example.  It inserts all rows of *df* for which the magnitude attribute (MAG) ls larger than 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310ff4ba-94b1-41da-ad64-030a267c7f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big = df.loc[df['MAG'] > 4.5]\n",
    "print(df_big)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9b2b9f-a3fe-4717-97fa-f88a1795af79",
   "metadata": {},
   "source": [
    "Let's look at the largest event from 2023 which the above shows was a M=5.08 event on day 335.   The most important information we need to retrieve the waveform data is the PREFIX and MS_FILENAME attribute.   AS noted earlier S3 requries a \"key\" to define the unique object you want to retrieve.  Why SCEDC didn't chose to build that key into their index directly is beyond me, but it can be constructed from PREFIX and MS_FILENAME as:  \"event_waveforms/\"+PREFIX+\"/\"+MS_FILENAME.  The code box below sets that manually for the the event of interest with the constant string \"event_waveforms/2023/2023_232/39645386.ms\".  \n",
    "\n",
    "With that clarification here is code to retrieve that event file from S3 and use the obspy miniseed reader to crack the miniseed data in memory.  The result is an obspy \"Stream\" object stored in memory.  I show you this in detail to give you a better idea how all this work.  Previously I was intentionally hiding this from in scedc_s3 module so you could \"see the forest for the trees\".   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd70400c-3bef-44b1-8cdf-1cdf8f8c3643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import obspy \n",
    "import io\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "import time\n",
    "t0=time.time()\n",
    "s3client = boto3.client('s3',config=Config(signature_version=UNSIGNED))\n",
    "# can be created in a program by combining PREFIX and MS_FILENAME\n",
    "s3key=\"event_waveforms/2023/2023_232/39645386.ms\"\n",
    "obj = s3client.get_object(Bucket=\"scedc-pds\",Key=s3key)\n",
    "mseed_content=obj[\"Body\"].read()\n",
    "st = obspy.read(io.BytesIO(mseed_content),format=\"mseed\")\n",
    "t=time.time()\n",
    "print(\"Stram object created by obspy.read\")\n",
    "print(st)\n",
    "print(\"Elapsed time to pull and convert data=\",t-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8e039f-c2e3-4927-aef6-029f8d119dd7",
   "metadata": {},
   "source": [
    "Remember the native internal format for MsPASS to match the content of the obspy stream we just read is what we call a `TimeSeriesEnsemble` - a group of single channel signals.   There are a lot here, but the time to convert is tiny compared to what we just did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c8788-0ff0-4bd7-bea6-bc72524d9e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.util.converter import Stream2TimeSeriesEnsemble\n",
    "t0=time.time()\n",
    "ensemble=Stream2TimeSeriesEnsemble(st)\n",
    "t=time.time()\n",
    "print(\"Time to convert=\",t-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b75d42-bfed-47be-a384-19b5b00a039a",
   "metadata": {},
   "source": [
    "Plotting 4450 signals in a notebook like this is problematic.   To plot all the data in a legible way would take at least 200 plot frames.   You are welcome to do that if you are curious, but that is not the purpose of this exercise.   This next code box is a standard QC trick I've used many times to only look at a few of the members of a large ensemble to just verify what I have is reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a09346e-5797-4e03-85a8-6d078a2b238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.graphics import SeismicPlotter\n",
    "from mspasspy.ccore.seismic import TimeSeriesEnsemble\n",
    "plotter = SeismicPlotter(normalize=True)\n",
    "# these are just to improve the plot\n",
    "plotter.style=\"wt\"\n",
    "plotter.title=\"First 15 TimeSeries objects in ensemble\"\n",
    "e2plot=TimeSeriesEnsemble(15)\n",
    "for i in range(15):\n",
    "    e2plot.member.append(ensemble.member[i])\n",
    "e2plot.set_live()\n",
    "plotter.plot(e2plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b14410f-713c-46ce-8836-becbb26660a7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "062e2d01-ff1c-4cd6-afa5-d21e68122995",
   "metadata": {},
   "source": [
    "### Disucssion\n",
    "\n",
    "Before moving on if I'd like to try a brief discussion of a few issues here:\n",
    "\n",
    "- How would you set up a data management scheme for SCEDC event data where you would only pull waveform data for processing as you needed it?\n",
    "- I know from experience (we could test it) that reading similar sized ensembles from a local disk stored as native MsPASS objects takes around one second.  How would that effect your design?\n",
    "- How would you parallelize event processing with this model?   How might that impact your decision for local versus remote storage?\n",
    "text should then move a discussion of what one might do next with that capability?  \n",
    "- The miniseed data has not source and receiver metadata.   What would be the MsPASS way to handle that issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46752f4f-874f-45db-8842-fff66d6aabbe",
   "metadata": {},
   "source": [
    "## Continuous data\n",
    "The final issue I want to address in this tutorial is handling of continuous data.  Note there are two common applications where accessing the continuous data is necessary:\n",
    "1.  Ambient noise processing by cross-correlation.\n",
    "2.  Extracting teleseismic event segments.  No US regional networks routinely process and create event files from teleseismic events so the only way to do so is to carve what you want from continuous data.  Earthscope will have this option as well, but they will likely supply an event extraction service.\n",
    "\n",
    "The requirements of the first are very different from the second.  There all the samples in a day file are required while for the telseismic case, which is what we do here, we only need a small fraction of a day's data.   We will discuss the implications of that at the end of this session.\n",
    "\n",
    "As we saw above SCEDC uses the current standard model of \"day files\" to store one channel of data for each recording day.  As a result, the algorithm below loops over all single channel waveform files carvng out a fix one hour window starting 100 s after the origin time of the Tohoku earthquake.  It then accumulates the windowed waveform data in a `TimeSeriesEnsemble` it then saves at the end of the code box.  \n",
    "\n",
    "First, we need to specify the time period.   For this tutorial we do that as a hand job using an estimate from and outside source.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46c0f98-be62-4f07-bdcd-300308e85c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy import UTCDateTime\n",
    "origin_time=UTCDateTime(\"2011-03-11T05:46:23.200000Z\")\n",
    "print(\"Event origin time=\",origin_time)\n",
    "# Define window range like test data as +100 s to +3700 s\n",
    "tstart = origin_time.timestamp + 100.0\n",
    "tend = origin_time.timestamp + 3700.0\n",
    "print(\"Julian day of event=\",origin_time.julday)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29deef11-85c0-4806-8ed2-da334c5e4ae3",
   "metadata": {},
   "source": [
    "Next we run the fetch and window algorithm.   This box will run for several minutes.  Start this next box and while it is running we will discuss the following issues:\n",
    "\n",
    "-  Do you have a feel for the time required for this trivial processing?  What is your leading hypothesis for the limiting step in this process?\n",
    "-  As a comparison this exact same job ran on my little linux PC development system (a generic 5 year old intel PC running Ubuntu) with a home modest internet connection took 2328 s (about 40 m) to run.  We'll know soon what you get, but we should be able to get a ballpark estimate of GeoLab by comparison.  What is the significance of those numbers?\n",
    "-  If we have time I can show you results form our paper in press at SRL on this topic.   This test reinforces a key conclusion of that paper. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7011a3b7-e8ba-435d-bad6-ab06c85080b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.algorithms.window import WindowData\n",
    "# timing the process time here is an important less\n",
    "import time\n",
    "t0=time.time()\n",
    "# fetch the index for 2011_070\n",
    "collection=\"SCEDC_s3_index\"\n",
    "# type qualifier isn't needed because catalog data do not have year and day attributes\n",
    "query={\"year\" : origin_time.year, \"day\" : origin_time.julday,\"format\" : \"parquet\"}\n",
    "index_doc=db[wfindex_collection].find_one(query)\n",
    "index_docs = scedc.make_day_index(index_doc)\n",
    "print(\"Number of day files in index for this day=\",len(index_docs))\n",
    "# directory and file name where data are stored\n",
    "dir=\"wf_TimeSeries/tohoku\"\n",
    "dfile=\"SCEDCwfdata.binary\"\n",
    "ndoc= 0\n",
    "for doc in index_docs:\n",
    "    ens = scedc.s3_mseed_reader(doc)\n",
    "    # because we set the reader to segment a file if it contains a gap, the return \n",
    "    # of this function is an ensemble.  We have to loop over the ensemble members \n",
    "    # to save all the data if the gap is inside the segment\n",
    "    if ens is not None:\n",
    "        for d in ens.member:\n",
    "            d[\"dir\"]=dir\n",
    "            d[\"dfile\"]=dfile\n",
    "            d[\"storage_mode\"]=\"file\"\n",
    "            d = WindowData(d,tstart,tend)\n",
    "            # note a segment with no data in the tstart to tend range will be marked dead\n",
    "            # by WindowData.  Here we let the database writer bury those segments in \n",
    "            # the cemetery.  We could just drop them but that preserves a record of what \n",
    "            # stations had the gaps of we wanted to pursue that\n",
    "            db.save_data(d,collection=\"wf_TimeSeries\",storage_mode=\"file\",data_tag=\"tohoku\")\n",
    "    ndoc += 1\n",
    "    if ndoc%250 == 0:\n",
    "        print(\"Finished process \",ndoc,\" waveforms\")\n",
    "t=time.time()\n",
    "print(\"Time to process \",len(index_docs),\" day files stored on s3=\",t-t0)\n",
    "print(\"Time per channel processed=\",(t-t0)/len(index_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ed4e5-e2b8-4f69-94d8-afa1129aea4d",
   "metadata": {},
   "source": [
    "Finally, count how many waveforms were saved and plot data from one randomly selected station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6758a8-16eb-4f82-8680-fb3b36d75862",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of TimeSeries data saved=\",db.wf_TimeSeries.count_documents({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd75b0e5-8a14-4f23-bb55-9503b095948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query={\"sta\" : \"BZN\"}  # first station in the file - random choice\n",
    "cursor=db.wf_TimeSeries.find(query)\n",
    "e = db.read_data(cursor,collection=\"wf_TimeSeries\")\n",
    "plotter.title=\"Data from BZN\"\n",
    "plotter.plot(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac3040f-5a9e-4a0a-a72d-e57359735f06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
