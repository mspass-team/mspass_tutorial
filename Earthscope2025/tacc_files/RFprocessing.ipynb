{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dd81a90-b594-45c6-979f-d64497691c6f",
   "metadata": {},
   "source": [
    "# Extended USArray Preprocessing\n",
    "This notebook is designed as a demonstration for the Earthscope 2025 MsPASS course section on HPC computing.   The notebook is designed to run as a batch process.   It assumes:\n",
    "-  The job is being run from a working directory with required files at the or below that work directory in the file system\n",
    "-  A \"master\" database with name \"usarray48\" has been created previously in this that directory. That db is assumed to contain a source, channel, and site collection\n",
    "-  The working (current) directory contains a directory called *./wf/2012* containing miniseed files with names of the form *EventN.mseed\" where N is a numeber (e.g. \"Event4.mseed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf17dfec-3239-4336-9d2f-5d4a1a93a442",
   "metadata": {},
   "source": [
    "# Stage 1:  Build wf_miniseed \n",
    "This notebook is the first step for processing the extended usarray data set in year segments. It runs the indexing program to build a set of index documents in the wf_miniseed collection.  It then runs bulk_normalize to create the channel_id and site_id cross references in the wf_miniseed documents.   That is essential for the second stage of the processing.\n",
    "\n",
    "This is a separate notebook because prototypes demonstrated:\n",
    "1.  There are too many potential issues with miniseed data that can cause problems that is is useful to checkpoint the job at the end of the notebook to verify things are ok. That is particularly  true of normalizations and potential miniseed data problems.\n",
    "2.  This notebook is note efficient to runw with a larger number of workers like the subsequent notebooks.   Running in with only 8 workers or so with a small memory requirement can be it through faster than waiting for a larger job requiring more resources, which is the case for the notebooks run after this one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "712c361f-185a-49b9-a548-4cf55cdca4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change for different calendar year\n",
    "year = 2012\n",
    "dbname = \"usarray{}\".format(year)\n",
    "tsdir=\"./wf_TimeSeries/{}\".format(year)\n",
    "dbmaster=\"usarray48\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "506ac77c-d382-42b5-a3c8-795e400864ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mspasspy.client as msc\n",
    "mspass_client = msc.Client()\n",
    "# waveform data indexed to here\n",
    "db = mspass_client.get_database(dbname)\n",
    "# master database with source and receiver metadata\n",
    "dbmd = mspass_client.get_database(dbmaster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95ba5849-65c4-44f1-bc0e-5cf9b3b4c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out for normal use\n",
    "import pymongo\n",
    "mongoclient=pymongo.MongoClient()\n",
    "mongoclient.drop_database(dbname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "744ff640-a240-49d3-a5e2-ba53c75f8bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time to run index_mseed_file= 17.964977741241455\n"
     ]
    }
   ],
   "source": [
    "# This builds a file list to drive index processing\n",
    "import os\n",
    "import fnmatch\n",
    "import dask.bag as dbg\n",
    "import time\n",
    "topdirectory=\"./wf\"\n",
    "# assume year was defined at the top and data have the structure of wf/year/*.mseed\n",
    "dir=\"{}/{}\".format(topdirectory,year)\n",
    "filelist=fnmatch.filter(os.listdir(dir),'*.mseed')\n",
    "tstart=time.time()\n",
    "data = dbg.from_sequence(filelist)\n",
    "data = data.map(db.index_mseed_file,dir)\n",
    "data=data.compute()\n",
    "tend=time.time()\n",
    "print(\"Elapsed time to run index_mseed_file=\",tend-tstart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff8fabf-11e6-45d6-8d8a-0b3e4b9e5218",
   "metadata": {},
   "source": [
    "Normalization of the mseed records is complicated by the fact we are using two databases here.  The current normalize_mseed will not work because it assumes one db holds wf_miniseed and the channel-site collections.  For that reason I use bulk_normalize which uses a preloaded dataframe as input.  That, of course, is why the first part of this block loads the MiniseedMatcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4988418-28ab-4ea5-9190-9e307e31d3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[178824, 177903, 177903, 178744]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mspasspy.db.normalize import MiniseedMatcher,OriginTimeMatcher,bulk_normalize,normalize\n",
    "# prepend_collection_name defaults to True but best to be clear that is essential here\n",
    "chan_matcher = MiniseedMatcher(dbmd,\n",
    "                               collection=\"channel\",\n",
    "                               prepend_collection_name=True,\n",
    "                              )\n",
    "site_matcher = MiniseedMatcher(dbmd,\n",
    "                               collection=\"site\",\n",
    "                               attributes_to_load=[\"_id\",\"lat\",\"lon\",\"elev\",\"starttime\",\"endtime\"],\n",
    "                               prepend_collection_name=True,\n",
    "                              )\n",
    "source_matcher = OriginTimeMatcher(dbmd,\n",
    "                                   t0offset=0.0,\n",
    "                                   tolerance=100.0,\n",
    "                                   attributes_to_load=['_id','time'])\n",
    "bulk_normalize(db,wf_col=\"wf_miniseed\",matcher_list=[chan_matcher,site_matcher,source_matcher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12ac7e60-434d-4248-b6e2-1c0674185c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear up these large objects \n",
    "del chan_matcher\n",
    "del site_matcher\n",
    "del source_matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dcac34-659a-4261-ab8d-55cce1e5c616",
   "metadata": {},
   "source": [
    "Next we need to define some processing functions for specialized tasks in this workflow.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9cd26c5-cdc6-4d50-aac1-30aad3b2652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.algorithms.window import WindowData\n",
    "from mspasspy.algorithms.signals import detrend\n",
    "from mspasspy.ccore.algorithms.basic import TimeWindow\n",
    "from mspasspy.ccore.utility import ErrorSeverity\n",
    "from mspasspy.algorithms.resample import (ScipyResampler,\n",
    "                                          ScipyDecimator,\n",
    "                                          resample,\n",
    "                                         )\n",
    "from mspasspy.db.normalize import ObjectIdMatcher\n",
    "from mspasspy.algorithms.calib import ApplyCalibEngine\n",
    "from mspasspy.util.seismic import number_live\n",
    "from obspy import UTCDateTime\n",
    "from obspy.geodetics import gps2dist_azimuth,kilometers2degrees\n",
    "from obspy.taup import TauPyModel\n",
    "from dask.distributed import as_completed\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "def set_PStime(d,Ptimekey=\"Ptime\",Stimekey=\"Stime\",model=None):\n",
    "    \"\"\"\n",
    "    Function to calculate P and S wave arrival time and set times \n",
    "    as the header (Metadata) fields defined by Ptimekey and Stimekey.\n",
    "    Tries to handle some complexities of the travel time calculator \n",
    "    returns when one or both P and S aren't calculatable.  That is \n",
    "    the norm in or at the edge of the core shadow.  \n",
    "    \n",
    "    :param d:  input TimeSeries datum.  Assumes datum's Metadata \n",
    "      contains stock source and channel attributes.  \n",
    "    :param Ptimekey:  key used to define the header attribute that \n",
    "      will contain the computed P time.  Default \"Ptime\".\n",
    "    :param model:  instance of obspy TauPyModel travel time engine. \n",
    "      Default is None.   That mode is slow as an new engine will be\n",
    "      constructed on each call to the function.  Normal use should \n",
    "      pass an instance for greater efficiency.  \n",
    "    \"\"\"\n",
    "    if d.live:\n",
    "        if model is None:\n",
    "            model = TauPyModel(model=\"iasp91\") \n",
    "        # extract required source attributes\n",
    "        srclat=d[\"source_lat\"]\n",
    "        srclon=d[\"source_lon\"]\n",
    "        srcz=d[\"source_depth\"]\n",
    "        srct=d[\"source_time\"] \n",
    "        # extract required channel attributes\n",
    "        stalat=d[\"channel_lat\"]\n",
    "        stalon=d[\"channel_lon\"]\n",
    "        staelev=d[\"channel_elev\"]\n",
    "        # set up and run travel time calculator\n",
    "        georesult=gps2dist_azimuth(srclat,srclon,stalat,stalon)\n",
    "        # obspy's function we just called returns distance in m in element 0 of a tuple\n",
    "        # their travel time calculator it is degrees so we need this conversion\n",
    "        dist=kilometers2degrees(georesult[0]/1000.0)\n",
    "        arrivals=model.get_travel_times(source_depth_in_km=srcz,\n",
    "                                            distance_in_degree=dist,\n",
    "                                            phase_list=['P','S'])\n",
    "        # always post this for as it is not cheap to compute\n",
    "        # WARNING:  don't use common abbrevation delta - collides with data dt\n",
    "        d['epicentral_distance']=dist\n",
    "        # these are CSS3.0 shorthands s - station e - event\n",
    "        esaz = georesult[1]\n",
    "        seaz = georesult[2]\n",
    "        # css3.0 names esax = event to source azimuth; seaz = source to event azimuth\n",
    "        d['esaz']=esaz\n",
    "        d['seaz']=seaz\n",
    "        # get_travel_times returns an empty list if a P time cannot be \n",
    "        # calculated.  We trap that condition and kill the output \n",
    "        # with an error message\n",
    "        if len(arrivals)==2:\n",
    "            Ptime=srct+arrivals[0].time\n",
    "            rayp = arrivals[0].ray_param\n",
    "            Stime=srct+arrivals[1].time\n",
    "            rayp_S = arrivals[1].ray_param\n",
    "            d.put(Ptimekey,Ptime)\n",
    "            d.put(Stimekey,Stime)\n",
    "            # These keys are not passed as arguments but could be - a choice\n",
    "            # Ray parameter is needed for free surface transformation operator\n",
    "            # note tau p calculator in obspy returns p=R sin(theta)/V_0\n",
    "            d.put(\"rayp_P\",rayp)\n",
    "            d.put(\"rayp_S\",rayp_S)\n",
    "        elif len(arrivals)==1:\n",
    "            if arrivals[0].name == 'P':\n",
    "                Ptime=srct+arrivals[0].time\n",
    "                rayp = arrivals[0].ray_param\n",
    "                d.put(Ptimekey,Ptime)\n",
    "                d.put(\"rayp_P\",rayp)\n",
    "            else:\n",
    "                # Not sure we can assume name is S\n",
    "                if arrivals[0].name == 'S':\n",
    "                    Stime=srct+arrivals[0].time\n",
    "                    rayp_S = arrivals[0].ray_param\n",
    "                    d.put(Stimekey,Stime)\n",
    "                    d.put(\"rayp_S\",rayp_S)\n",
    "                else:\n",
    "                    message = \"Unexpected single phase name returned by taup calculator\\n\"\n",
    "                    message += \"Expected phase name S but got \" + arrivals[0].name\n",
    "                    d.elog.log_error(\"set_PStime\",\n",
    "                                     message,\n",
    "                                     ErrorSeverity.Invalid)\n",
    "                    d.kill()\n",
    "        else:\n",
    "            # in this context this only happens if no P or S could be calculated\n",
    "            # That shouldn't ever happen but we need this safety in he event it does\n",
    "            message = \"Travel time calculator failed completely\\n\"\n",
    "            message += \"Could not calculator P or S phase time\"\n",
    "            d.elog.log_error(\"set_PStime\",\n",
    "                             message,\n",
    "                             ErrorSeverity.Invalid)\n",
    "            d.kill()\n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def set_file_path(e,dir=\"wf_TimeSeries\",ext=None):\n",
    "    \"\"\"\n",
    "    This function is used to set dir and dfile for this workflow for each \n",
    "    ensemble.  Note these are set in the ensemble's metadata container \n",
    "    not the members.   We don't set them for members as Database.save_data \n",
    "    only uses kwarg dir and dfile with the metadata as a fallback.\n",
    "    \"\"\"\n",
    "    e['dir']=dir\n",
    "    dfile = \"dfile_undefined\"\n",
    "    for d in e.member:\n",
    "        if d.live and 'dfile' in d:\n",
    "            dfile=d['dfile']\n",
    "            sret = dfile.split(\".\")\n",
    "            # split returns a list.  If there is no extension only one is returned \n",
    "            # in 0 but that is all we need here\n",
    "            dfile=sret[0]\n",
    "            break\n",
    "    if ext is not None:\n",
    "        dfile = dfile + \".\" + ext\n",
    "    return e\n",
    "\n",
    "def dbmdquery(year,padsize=86400.0):\n",
    "    \"\"\"\n",
    "    Constructs a MongoDB query dictionary to use as a query argument for normalization matcher classes.\n",
    "    (All standard BasicMatcher children have a query argument in the constructor for this purpose.)\n",
    "    The query is a range spanning specified calendar year.   The interval is extended by padsize \n",
    "    seconds.   (default is one day = 86400.0)\n",
    "\n",
    "    Note this query is appropriate for channel not site.\n",
    "    \"\"\"\n",
    "    # uses obspy's UTCDateTime class to create time range in epoch time using the \n",
    "    # calendar strings for convenience\n",
    "    tstr = \"{}-01-01T00:00:00.0\".format(year)\n",
    "    st = UTCDateTime(tstr)\n",
    "    starttime = st.timestamp - padsize\n",
    "    tstr = \"{}-01-01T00:00:00.0\".format(year+1)\n",
    "    et = UTCDateTime(tstr)\n",
    "    endtime = et.timestamp + padsize\n",
    "    # not sure the and is required but better safe than sorry\n",
    "    query = { \"$and\" :\n",
    "             [\n",
    "                 {\"starttime\" : {\"$lte\" : endtime}},\n",
    "                 {\"endtime\" : {\"$gte\" : starttime}}\n",
    "             ]\n",
    "    }\n",
    "    return query\n",
    "\n",
    "def atomic_ts_processor(d,decimator,resampler,ttmodel,win,calib_engine):\n",
    "    \"\"\"\n",
    "    This function puts all the processing functions for input TimeSeres (d).  It is called in this \n",
    "    workflow in a map operator applied to ensemble members.  \n",
    "    \"\"\"\n",
    "    d = detrend(d,type=\"constant\")\n",
    "    d = resample(d,decimator,resampler)\n",
    "    d = set_PStime(d,model=ttmodel)\n",
    "    if d.live and \"Ptime\" in d:\n",
    "        ptime = d[\"Ptime\"]\n",
    "        d.ator(ptime)\n",
    "        d = WindowData(d,win.start,win.end)\n",
    "        d.rtoa()\n",
    "    d = calib_engine.apply_calib(d)\n",
    "    return d   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5539c45a-6e67-47a6-8aa0-660f750e5791",
   "metadata": {},
   "source": [
    "This is a major run section.  It constructs a parallel container of `TimeSeriesEnsemble` objects from miniseed data in the `read_distributed_data` line.  It then runs a sequence of algorithms on each ensemble aiming mainly to window the data down to shorter time segments focused on the P wave arrival time.  It then saves the preprocessed result as files in wf_TimeSeries.   This is the most time consuming section of this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3464b36a-dbf2-4fdc-ac4d-05f344ea26ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct sources in wf_miniseed=  56\n",
      "Number to process this run= 56\n",
      "working on   67e675c7878ef7c6d337c8bd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/distributed/client.py:3169: UserWarning: Sending large graph of size 12.88 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on   67e675c7878ef7c6d337c8cb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/distributed/client.py:3169: UserWarning: Sending large graph of size 12.87 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on   67e675c7878ef7c6d337c90b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/distributed/client.py:3169: UserWarning: Sending large graph of size 10.10 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on   67e675c7878ef7c6d337c90f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/distributed/client.py:3169: UserWarning: Sending large graph of size 12.86 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on   67e675c7878ef7c6d337c910\n",
      "working on   67e675c7878ef7c6d337c990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/distributed/client.py:3169: UserWarning: Sending large graph of size 12.89 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on   67e675c7878ef7c6d337c991\n",
      "working on   67e675c7878ef7c6d337c992\n",
      "working on   67e675c7878ef7c6d337caca\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/distributed/client.py:3169: UserWarning: Sending large graph of size 12.92 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on   67e675c7878ef7c6d337cafa\n",
      "working on   67e675c7878ef7c6d337cb2e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/distributed/client.py:3169: UserWarning: Sending large graph of size 12.91 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on   67e675c7878ef7c6d337cb50\n",
      "working on   67e675c7878ef7c6d337cb78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/distributed/client.py:3169: UserWarning: Sending large graph of size 12.90 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on   67e675c7878ef7c6d337cb9c\n",
      "working on   67e675c7878ef7c6d337cbab\n",
      "working on   67e675c7878ef7c6d337cc4a\n",
      "working on   67e675c7878ef7c6d337cc6c\n",
      "working on   67e675c7878ef7c6d337cc6d\n",
      "working on   67e675c7878ef7c6d337cce9\n",
      "working on   67e675c8878ef7c6d337ce70\n",
      "working on   67e675c8878ef7c6d337cea6\n",
      "working on   67e675c8878ef7c6d337d048\n",
      "working on   67e675c8878ef7c6d337d13f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/distributed/client.py:3169: UserWarning: Sending large graph of size 12.82 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on   67e675c8878ef7c6d337d17a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/distributed/client.py:3169: UserWarning: Sending large graph of size 12.81 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on   67e675c8878ef7c6d337d17b\n",
      "working on   67e675c8878ef7c6d337d22a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/distributed/client.py:3169: UserWarning: Sending large graph of size 12.83 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on   67e675c8878ef7c6d337d24f\n",
      "working on   67e675c8878ef7c6d337d250\n",
      "working on   67e675c8878ef7c6d337d251\n",
      "working on   67e675c8878ef7c6d337d261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/distributed/client.py:3169: UserWarning: Sending large graph of size 12.84 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on   67e675c9878ef7c6d337d65a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/distributed/client.py:3169: UserWarning: Sending large graph of size 12.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on   67e675c9878ef7c6d337d6c2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/distributed/client.py:3169: UserWarning: Sending large graph of size 12.73 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on   67e675c9878ef7c6d337d702\n",
      "working on   67e675c9878ef7c6d337d72e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/distributed/client.py:3169: UserWarning: Sending large graph of size 12.54 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on   67e675c9878ef7c6d337d74b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/distributed/client.py:3169: UserWarning: Sending large graph of size 12.80 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on   67e675c9878ef7c6d337d74c\n",
      "working on   67e675c9878ef7c6d337d768\n",
      "working on   67e675c9878ef7c6d337d7bf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/distributed/client.py:3169: UserWarning: Sending large graph of size 12.77 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on   67e675c9878ef7c6d337d7f9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/distributed/client.py:3169: UserWarning: Sending large graph of size 12.79 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on   67e675c9878ef7c6d337d813\n",
      "working on   67e675c9878ef7c6d337d858\n",
      "working on   67e675c9878ef7c6d337d859\n",
      "working on   67e675c9878ef7c6d337d85f\n",
      "working on   67e675c9878ef7c6d337d860\n",
      "working on   67e675c9878ef7c6d337d862\n",
      "working on   67e675c9878ef7c6d337d88d\n",
      "working on   67e675c9878ef7c6d337d8c2\n",
      "working on   67e675c9878ef7c6d337dab4\n",
      "working on   67e675c9878ef7c6d337dc3c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/distributed/client.py:3169: UserWarning: Sending large graph of size 12.76 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on   67e675ca878ef7c6d337dd52\n",
      "working on   67e675ca878ef7c6d337dd56\n",
      "working on   67e675ca878ef7c6d337de2b\n",
      "working on   67e675ca878ef7c6d337df11\n",
      "working on   67e675ca878ef7c6d337e19e\n",
      "working on   67e675ca878ef7c6d337e230\n",
      "working on   67e675ca878ef7c6d337e24e\n",
      "Time to process from raw data to TimeSeries objects in db =  2061.5057950019836  seconds\n"
     ]
    }
   ],
   "source": [
    "from mspasspy.ccore.seismic import TimeSeriesEnsemble\n",
    "from mspasspy.io.distributed import read_distributed_data\n",
    "ttmodel = TauPyModel(model=\"iasp91\")\n",
    "t0 = time.time()\n",
    "# important to note these matchers are loaded from dbmd\n",
    "# bulk_normalize above will set the matching ids\n",
    "timequery = dbmdquery(year)\n",
    "chan_matcher = ObjectIdMatcher(dbmd,\n",
    "                               query=timequery,\n",
    "                               collection=\"channel\",\n",
    "                               attributes_to_load=[\"_id\",\"lat\",\"lon\",\"elev\",\"hang\",\"vang\"],\n",
    "                              )\n",
    "tstr1 = \"{}-01-01T00:00:00.0\".format(year)\n",
    "tstr2 = \"{}-01-01T00:00:00.0\".format(year+1)\n",
    "st1=UTCDateTime(tstr1)\n",
    "st2=UTCDateTime(tstr2)\n",
    "srcquery = {\"time\" : {\"$gte\" : st1.timestamp, \"$lt\" : st2.timestamp}}\n",
    "source_matcher = ObjectIdMatcher(dbmd,\n",
    "                                query=srcquery,\n",
    "                                collection=\"source\",\n",
    "                                attributes_to_load=[\"_id\",\"lat\",\"lon\",\"depth\",\"time\"],\n",
    "                                load_if_defined=[\"magnitude\"],\n",
    "                                )\n",
    "target_sample_rate=20.0\n",
    "resampler=ScipyResampler(target_sample_rate)\n",
    "decimator=ScipyDecimator(target_sample_rate)\n",
    "calib_engine = ApplyCalibEngine(dbmd)\n",
    "tswin = TimeWindow(-200.0,500.0)\n",
    "srcidlist_ms=db.wf_miniseed.distinct('source_id')\n",
    "\n",
    "srcidlist_finished = db.wf_TimeSeries.distinct('source_id')\n",
    "if len(srcidlist_finished)>0:\n",
    "    srcidlist=[]\n",
    "    for sid in srcidlist_ms:\n",
    "        if sid not in srcidlist_finished:\n",
    "            srcidlist.append(sid)\n",
    "else:\n",
    "    srcidlist = srcidlist_ms\n",
    "\n",
    "\n",
    "print(\"Number of distinct sources in wf_miniseed= \",len(srcidlist_ms))\n",
    "print(\"Number to process this run=\",len(srcidlist))\n",
    "\n",
    "for sid in srcidlist:\n",
    "    print(\"working on  \",sid)\n",
    "    query = {\"source_id\" : sid}\n",
    "    mydata = read_distributed_data(db,\n",
    "                                   query=query,\n",
    "                                   collection=\"wf_miniseed\",\n",
    "                                   normalize=[chan_matcher,source_matcher],\n",
    "                                   npartitions=60,\n",
    "                                  )\n",
    "    mydata=mydata.map(atomic_ts_processor,\n",
    "                            decimator,\n",
    "                            resampler,\n",
    "                            ttmodel,\n",
    "                            tswin,\n",
    "                            calib_engine)\n",
    "    dlist = mydata.compute()\n",
    "    # package into ensemble for a faster write \n",
    "    ens=TimeSeriesEnsemble(len(dlist))\n",
    "    for d in dlist:\n",
    "        ens.member.append(d)\n",
    "    # needed because we constructed this manual from a list\n",
    "    ens.set_live()\n",
    "    ens = set_file_path(ens,dir=tsdir)\n",
    "    ens = db.save_data(ens,\n",
    "                       return_data=True,\n",
    "                       collection=\"wf_TimeSeries\",\n",
    "                       storage_mode=\"file\",\n",
    "                       dir=tsdir,\n",
    "                       data_tag=\"preprocessed_map\",\n",
    "                       )\n",
    "    del ens\n",
    "    \n",
    "    \n",
    "t = time.time()\n",
    "print(\"Time to process from raw data to TimeSeries objects in db = \",t-t0,\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881d2c3d-df7f-4e4f-b30e-7d537fcb4849",
   "metadata": {},
   "source": [
    "## Create and Store Seismogram objects\n",
    "This next section reads back the data we just created, converts all the data to `Seismogram` objects, runs the free-surface transformation operator, and finally runs an snr based quality control editor that will automatically kill data with no detectable signal.  This section takes a lot of compute time but only about 1/3 as long as the previous major step.\n",
    "\n",
    "First define new functions needed for this processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ead0abb2-1804-4d8d-ad7b-0dd166fdb0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.db.normalize import normalize\n",
    "from mspasspy.algorithms.bundle import bundle_seed_data\n",
    "from mspasspy.ccore.seismic import SlownessVector\n",
    "from mspasspy.algorithms.basic import free_surface_transformation,rotate_to_standard\n",
    "from mspasspy.algorithms.snr import broadband_snr_QC\n",
    "from mspasspy.util.seismic import number_live\n",
    "import dask.bag as dbg\n",
    "import time\n",
    "import math\n",
    "\n",
    "def read_tsensembles(source_id,db,data_tag=None):\n",
    "    \"\"\"\n",
    "    Reader for this script. Could be done with read_distributed_data and a query generator \n",
    "    to create a list of python dictionaries as input to read_distributed_data.   This does the \n",
    "    same thing more or less and is a bit clearer.\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"Debug:  entered read_tsensembles\")\n",
    "    query = {\"source_id\" : source_id}\n",
    "    if data_tag:\n",
    "        query[\"data_tag\"] = data_tag\n",
    "    n = db.wf_TimeSeries.count_documents(query)\n",
    "    print(\"Found \",n,\" documents for nsemble with source_id=\",source_id)\n",
    "    cursor = db.wf_TimeSeries.find(query)\n",
    "    ens = db.read_data(cursor,collection=\"wf_TimeSeries\")\n",
    "    print(\"Number of members in this ensemble=\",len(ens.member))\n",
    "    return ens\n",
    "\n",
    "def normalize_ensembles(ens,matcher):\n",
    "    \"\"\"\n",
    "    Workaround for bug in normalize function where decorators won't work.\n",
    "    This function should be removed when that bug is resolved.\n",
    "    \"\"\"\n",
    "    if ens.live:\n",
    "        for i in range(len(ens.member)):\n",
    "            ens.member[i] = normalize(ens.member[i],matcher)\n",
    "    return ens\n",
    "\n",
    "def apply_FST(d,rayp_key=\"rayp_P\",seaz_key='seaz',vp0=6.0,vs0=3.5):\n",
    "    \"\"\"\n",
    "    Apply free surface transformation operator of Kennett (1983) to an input `Seismogram` \n",
    "    object.   Assumes ray parameter and azimuth data are stored as Metadata in the \n",
    "    input datum.  If the ray parameter or azimuth key are not defined an error \n",
    "    message will be posted and the datum will be killed before returning.  \n",
    "    :param d:  datum to process\n",
    "    :type d:  Seismogram\n",
    "    :param rayp_key:   key to use to extract ray parameter to use to compute the \n",
    "    free surface transformation operator.  Note function assumes the ray parameter is\n",
    "    spherical coordinate form:  R sin(theta)/V.   Default is \"rayp_P\".\n",
    "    :param seaz_key:   key to use to extract station to event azimuth. Default is \"seaz\".\n",
    "    :param vp0:  surface P wave velocity (km/s) to use for free surface transformation \n",
    "    :param vs0:  surface S wave velocity (km/s) to use for free surface transformation.\n",
    "    \"\"\"\n",
    "    if d.is_defined(rayp_key) and d.is_defined(seaz_key):\n",
    "        rayp = d[rayp_key]\n",
    "        seaz = d[seaz_key]\n",
    "        # Some basic seismology here.  rayp is the spherical earth ray parameter\n",
    "        # R sin(theta)/v.  Free surface transformation needs apparent velocity \n",
    "        # at Earth's surface which is sin(theta)/v when R=Re.   Hence the following\n",
    "        # simple convertion to get apparent slowness at surface  sin(theta)/v\n",
    "        Re=6378.1\n",
    "        umag = rayp/Re   # magnitude of slowness vector\n",
    "        # seaz is back azimuth - slowness vector points in direction of propagation\n",
    "        # with is 180 degrees away from back azimuth\n",
    "        az = seaz + 180.0\n",
    "        # component slowness vector components in local coordinates\n",
    "        ux = umag * math.sin(az)\n",
    "        uy = umag * math.cos(az)\n",
    "        # FST implementation requires this special class called a Slowness Vector\n",
    "        u = SlownessVector(ux,uy)\n",
    "        d = free_surface_transformation(d,uvec=u,vp0=vp0,vs0=vs0)\n",
    "    else:\n",
    "        d.kill()\n",
    "        message = \"one of required attributes rayp_P or seaz were not defined for this datum\"\n",
    "        d.elog.log_error(\"apply_FST\",message,ErrorSeverity.Invalid)\n",
    "        \n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "def process2seis(ens,\n",
    "            swin,\n",
    "            nwin,\n",
    "            signal_engine,\n",
    "            noise_engine,\n",
    "                ):\n",
    "    \"\"\"\n",
    "    This is the master processing function for this step.   It assumes an input of ens \n",
    "    that is a common source gather with required Metadata set in the earlier run with the stage 2 \n",
    "    (Preprocess2ts) notebook.  In this workflow ens is read in parallel with this functiond oing \n",
    "    the processing.  The output is a SeismogramEnsemble run through broadband_snr_QC to set \n",
    "    snr metrics.  It also normally kills a large fraction of the data.  \n",
    "\n",
    "    The approach using a loop over the ensemble was designed to reduce the memory footprint as \n",
    "    each member is processed and replaces its parent in the ensemble that is returned.  \n",
    "    \"\"\"\n",
    "    print(\"Processing TimeSeriesEnsemble with \",len(ens.member),\" members\")\n",
    "    ens3c = bundle_seed_data(ens)\n",
    "    del ens\n",
    "    N = len(ens3c.member)\n",
    "    for i in range(N):\n",
    "        ens3c.member[i] = apply_FST(ens3c.member[i])\n",
    "        ens3c.member[i] = broadband_snr_QC(ens3c.member[i],\n",
    "                            component=2,\n",
    "                            signal_window=swin,\n",
    "                            noise_window=nwin,\n",
    "                            use_measured_arrival_time=True,\n",
    "                            measured_arrival_time_key=\"Ptime\",\n",
    "                            noise_spectrum_engine=noise_engine,\n",
    "                            signal_spectrum_engine=signal_engine,\n",
    "                                )\n",
    "    return ens3c\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b64cbab-4965-42e4-b013-0ccf1dd5c332",
   "metadata": {},
   "source": [
    "This block run the functions defined above in combination with several MsPASS auxiliary objects.  It constructs a bag of `TimeSeriesEnsemble` objects  and passes it through the processing functions described above to produce there result.  That is, and edited set of `Seismogram` objects stored as files in \"wf_Seismogram\" with Metadata stored in the database in the collection \"wf_Seismogram\" (Note the confusion that the same name is used for the collection and a directory for holding the sample data in files).  Emphasize the \"edited\" as the output of this sequence is commonly 1/10 the size of the input. It leaves a lot of bodies in the \"cemetery\" collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37fed413-bde2-4b6d-a52a-26b23eff8b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct sources in wf_TimeSeries=  52\n",
      "Number to process this run= 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/distributed/client.py:3169: UserWarning: Sending large graph of size 27.30 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': False}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': False}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': False}, {'is_live': True}, {'is_live': False}, {'is_live': False}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}, {'is_live': True}]\n",
      "Run time =  623.2603845596313  seconds\n"
     ]
    }
   ],
   "source": [
    "from mspasspy.algorithms.MTPowerSpectrumEngine import MTPowerSpectrumEngine\n",
    "wfdir = \"./wf_Seismogram/{}\".format(year)\n",
    "t0 = time.time()\n",
    "#chanqmspasspy.algorithms.MTPowerSpectrumEngine.MTPowerSpectrumEngineuery=dbmdquery(year)\n",
    "chan_matcher = ObjectIdMatcher(dbmd,\n",
    "                               collection=\"channel\",\n",
    "                               attributes_to_load=[\"_id\",\"lat\",\"lon\",\"elev\",\"hang\",\"vang\"],\n",
    "                              )\n",
    "\n",
    "# for usage on IU system each node has 2*64=128 cores so using 128 workers for the run\n",
    "# makes this twice that\n",
    "npartitions=100\n",
    "swin = TimeWindow(-5.0,100.0)\n",
    "nwin = TimeWindow(-195.0,-5.0)\n",
    "# spectrum estimation engines for broadband_snr_QC\n",
    "dt = 0.05\n",
    "nsamp_noise = int((nwin.end-nwin.start)/dt) + 1\n",
    "nsamp_sig = int((swin.end-swin.start)/dt) + 1\n",
    "signal_engine=MTPowerSpectrumEngine(nsamp_sig,5.0,8,2*nsamp_sig,dt)\n",
    "noise_engine=MTPowerSpectrumEngine(nsamp_noise,5.0,10,2*nsamp_noise,dt)\n",
    "srcidlist_ms=db.wf_TimeSeries.distinct('source_id')\n",
    "\n",
    "srcidlist_finished = db.wf_Seismogram.distinct('source_id')\n",
    "if len(srcidlist_finished)>0:\n",
    "    srcidlist=[]\n",
    "    for sid in srcidlist_ms:\n",
    "        if sid not in srcidlist_finished:\n",
    "            srcidlist.append(sid)\n",
    "else:\n",
    "    srcidlist = srcidlist_ms\n",
    "\n",
    "\n",
    "print(\"Number of distinct sources in wf_TimeSeries= \",len(srcidlist_ms))\n",
    "print(\"Number to process this run=\",len(srcidlist))\n",
    "# reduce size for testing \n",
    "#sidtmp=[]\n",
    "#for i in range(4):\n",
    "#    sidtmp.append(srcidlist[i])\n",
    "#srcidlist=sidtmp\n",
    "#print(\"Debug process list:  \",srcidlist)\n",
    "mydata = dbg.from_sequence(srcidlist)\n",
    "mydata = mydata.map(read_tsensembles,db)\n",
    "# note default behavior for normalize is to normalize all members\n",
    "#mydata = mydata.map(normalize,chan_matcher,handles_ensembles=False)\n",
    "#workaround for above until bug is fixed in normalize\n",
    "mydata = mydata.map(normalize_ensembles,chan_matcher)\n",
    "mydata = mydata.map(process2seis,\n",
    "                      swin,\n",
    "                      nwin,\n",
    "                      signal_engine,\n",
    "                      noise_engine,\n",
    "                               )\n",
    "mydata = mydata.map(set_file_path,dir=wfdir,ext=\"3C\")\n",
    "mydata = mydata.map(db.save_data,\n",
    "                    collection=\"wf_Seismogram\",\n",
    "                    storage_mode=\"file\",\n",
    "                    dir=wfdir,\n",
    "                    data_tag=\"FST_and_QCed\",\n",
    "                   )\n",
    "out=mydata.compute()\n",
    "print(out)\n",
    "t = time.time()\n",
    "print(\"Run time = \",t-t0,\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd97a52-2c14-441c-a035-d6a876969422",
   "metadata": {},
   "source": [
    "# Receiver Function Estimation\n",
    "The last step in this workflow is to compute receiver function estimates from all the Seismogram objects that survived winnowing with the broadband_snr_QC function.  The estimation method is a novel method called \"Colored Noise Regularized Deconvolution\" (CNRDecon).   The method can be viewed as form of multitaper deconvolution where the bandwidth of the data is determined dynamically for each Seismogram object.   To validate the data this code saves the \"actual output\" of each deconvolution operator in wf_TimeSeries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05bc4397-08a1-4b02-aee8-aaef3191e658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.ccore.utility import AntelopePf\n",
    "from mspasspy.ccore.seismic import TimeSeries\n",
    "from mspasspy.ccore.algorithms.deconvolution import CNRDeconEngine\n",
    "from mspasspy.ccore.algorithms.amplitudes import MADAmplitude\n",
    "from mspasspy.algorithms.basic import ExtractComponent\n",
    "from mspasspy.algorithms.CNRDecon import CNRRFDecon\n",
    "from mspasspy.util.Undertaker import Undertaker\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12078c1-8467-45d9-bada-f823c223db9f",
   "metadata": {},
   "source": [
    "This box defines a couple composite functions used in the processing sequenc below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "402f62cd-c5ad-424d-9177-9891d8a60ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_incident(d,ao,nw_end=-3.0,P_component=2):\n",
    "    if d.dead():\n",
    "        return d\n",
    "    stime = max(d.t0,ao.t0)\n",
    "    etime = min(d.endtime(),ao.endtime())\n",
    "    x_ao = WindowData(ao,stime,etime)\n",
    "    s_amp = np.zeros(3)\n",
    "    n_amp = np.zeros(3)\n",
    "    snr = np.zeros(3)\n",
    "    for i in range(3):\n",
    "        x = ExtractComponent(d,i)\n",
    "        x_n = WindowData(x,d.t0,nw_end)\n",
    "        n_amp[i] = MADAmplitude(x_n)\n",
    "        x_w = WindowData(x,stime,etime)\n",
    "        amp = np.dot(x_ao.data,x_w.data)\n",
    "        s_amp[i] = amp\n",
    "        if n_amp[i]>0.0:\n",
    "            snr[i] = s_amp[i]/n_amp[i]\n",
    "        else:\n",
    "            if s_amp[i]>0.0:\n",
    "                snr[i]=9999999.9\n",
    "            else:\n",
    "                s_amp[i]=1.0\n",
    "        scaled_ao = TimeSeries(x_ao)\n",
    "        # Note -amp used so when we add scaled it is subtracting incident\n",
    "        scaled_ao *= -amp\n",
    "        # operator += handles irregular start and end time\n",
    "        # taper may be advised but in this context probably necessary\n",
    "        x += scaled_ao\n",
    "        d.data[i,:] = x.data\n",
    "        \n",
    "    # post snr metrics before exiting\n",
    "    snrdoc=dict()\n",
    "    # not sur mongodb will handle a np array corectly so convert this \n",
    "    # to a python list\n",
    "    snrlist=list()\n",
    "    namplist=list()\n",
    "    for i in range(3):\n",
    "        snrlist.append(snr[i])\n",
    "        namplist.append(n_amp[i])\n",
    "    snrdoc['component_snr']=snrlist\n",
    "    snrdoc['component_noise']=namplist\n",
    "    # vector sum of incident horizontal snr a more useful metric than \n",
    "    # components\n",
    "    sig_H = 0.0\n",
    "    noise_H = 0.0\n",
    "    for i in range(3):\n",
    "        if i != P_component:\n",
    "            sig_H += s_amp[i]*s_amp[i]\n",
    "            noise_H += n_amp[i]*n_amp[i]\n",
    "    sig_H = np.sqrt(sig_H)\n",
    "    noise_H = np.sqrt(noise_H)\n",
    "    if noise_H < np.finfo(float).eps:\n",
    "        if sig_H < np.finfo(float).eps:\n",
    "            snr=1.0\n",
    "        else:\n",
    "            snr = sig_H/np.finfo(float).eps\n",
    "    else:\n",
    "        snr = sig_H/noise_H\n",
    "    snrdoc['snr_H'] = snr\n",
    "    d['snr_RF'] = snrdoc\n",
    "    return d\n",
    "\n",
    "def process(sid,db,dbmd,engine,signal_window,noise_window,data_tag=None)->int:\n",
    "    \"\"\"\n",
    "    Process one SeismogramEnsemble for source id sid with CRNDecon algorithm. \n",
    "    Saves output in two forms.   All live outputs from the decon algorithmn \n",
    "    (CNRRFDecon can kill) are saved with data_tag=\"CNRRFDecon_data_raw\".  \n",
    "    The algorithm then calls the function \"remove_incident\" (see above) \n",
    "    to minimize direct P wave energy in all three components.  \n",
    "    (Note with an RF that means the P component will be very small\n",
    "    except at the fringes of the data window)  Finally, it computes the \n",
    "    actual output and ideal output wavelets and stores them in the \n",
    "    wf_TimeSeries with cross-references to the _id of the saved RF estimates.\n",
    "    They are stored with data_tag of \"CNRRFDecon_ao\" and \"CNRRFDecon_io\" \n",
    "    respectively.\n",
    "\n",
    "    :param sid:  source_id to process\n",
    "    :param db:  waveform database managing Seismogram objects to be processed\n",
    "    :param dbmd:  Metadata database - in this implementation not the same as db\n",
    "    :param engine:  Instance of CNRRFDeconEngine.  This algorithm has a \n",
    "      fairly high initialization cost.  The engine avoids that initialziation\n",
    "      cost by encapsulating all the things required to run the algorithm \n",
    "      in this object.\n",
    "    :param signal_window:  TimeWindow object of section of data to treat \n",
    "      as signal\n",
    "    :param noise_window:  TimeWindow object defining section of data to \n",
    "      treat as noise (used for computing regularization so must overlap \n",
    "      data range).\n",
    "    :param data_tag:  data tag to subset INPUT not output.   When set \n",
    "      (default is None) the query by sid will append a data_tag \n",
    "      quality match for the value received.\n",
    "\n",
    "    :return:  tuple that can just be printed.  Content is: \n",
    "      [str(sid), number of ensemble members, number marked live]\n",
    "    \"\"\"\n",
    "    query = {'source_id' : sid}\n",
    "    ntotal = db.wf_Seismogram.count_documents(query)\n",
    "    query['Parrival.median_snr'] = {'$gt' : 2.0}\n",
    "    query['Parrival.snr_filtered_envelope_peak'] = {'$gt' : 3.0}\n",
    "    query['Parrival.bandwidth'] = {'$gt' : 8.0}\n",
    "    if data_tag:\n",
    "        query['data_tag'] = data_tag\n",
    "    cursor=db.wf_Seismogram.find(query)\n",
    "    ens = db.read_data(cursor,collection=\"wf_Seismogram\")\n",
    "    print('source_id=',sid,' Processing ',len(ens.member),' of ',ntotal)\n",
    "    nlive = 0\n",
    "    nmembers = len(ens.member)\n",
    "    for d in ens.member:\n",
    "        if d.live:\n",
    "            Ptime=d['Ptime']\n",
    "            d.ator(Ptime)\n",
    "            decon_output = CNRRFDecon(d,\n",
    "                        engine,\n",
    "                        signal_window=signal_window,\n",
    "                        noise_window=noise_window,\n",
    "                        bandwidth_subdocument_key=\"Parrival\",\n",
    "                        return_wavelet=True,\n",
    "                       )\n",
    "            rf0 = decon_output[0]\n",
    "            if rf0.dead():\n",
    "                # it is a bit inefficient to instantiate an instance of \n",
    "                # an Undertaker for each error but CMRRFDecon exceptions \n",
    "                # are not common unless the engine is badly configured.\n",
    "                stedronsky = Undertaker(dbmd)\n",
    "                stedronsky.bury(rf0)\n",
    "            else:\n",
    "                ao = decon_output[1]\n",
    "                io = decon_output[2]\n",
    "                rf = remove_incident(rf0, ao)\n",
    "                # temp for debug with spyder - no save\n",
    "                #if rf:\n",
    "                    #plotter=SeismicPlotter(normalize=True,style='wt')\n",
    "                    #plotter.plot(rf)\n",
    "                    #plt.show()\n",
    "                    #continue\n",
    "                dir=\"wf_RF\"\n",
    "                s=rf['dfile']\n",
    "                dfile = \"RF\"+s\n",
    "                rf0=dbmd.save_data(rf0,\n",
    "                              collection='wf_Seismogram',\n",
    "                              return_data=False,\n",
    "                              storage_mode='file',\n",
    "                              dir=dir,\n",
    "                              dfile=dfile,\n",
    "                              data_tag='CNRRFDecon_data_raw')\n",
    "                rf=dbmd.save_data(rf,\n",
    "                              collection='wf_Seismogram',\n",
    "                              return_data=True,\n",
    "                              storage_mode='file',\n",
    "                              dir=dir,\n",
    "                              dfile=dfile,\n",
    "                              data_tag='CNRRFDecon_data')\n",
    "            \n",
    "                if rf.live:\n",
    "                    wfid = rf['_id']\n",
    "                    ao['parent_wfid'] = wfid\n",
    "                    io['parent_wfid'] = wfid\n",
    "                    dfile = \"ao_\" + s\n",
    "                    dbmd.save_data(ao,collection='wf_TimeSeries',storage_mode='file',dir=dir,dfile=dfile,data_tag='CNRRFDecon_ao')\n",
    "                    dfile = \"io_\" + s\n",
    "                    dbmd.save_data(io,collection='wf_TimeSeries',\n",
    "                             storage_mode='file',\n",
    "                             dir=dir,dfile=dfile,\n",
    "                             data_tag='CNRRFDecon_io')\n",
    "                    nlive += 1\n",
    "        else:\n",
    "            print(\"WARNING:  ensemble member was marked dead - has to be an abortion\")\n",
    "            nlive = 0\n",
    "        return [str(sid),nmembers,nlive]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541c8cc6-1a0a-4bca-b727-438095c0e377",
   "metadata": {},
   "source": [
    "Now run the decon algorithm.  Note the approach used here is different.  It demonstrates the use of concurrent \"Futures\" in dask.   Note the __[dask Futures API](https://docs.dask.org/en/latest/futures.html)__ is similar to the __[threaded version](https://docs.python.org/3/library/asyncio-future.html)__ that is now standard python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df9989e5-2992-45f3-8182-893ef280b306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on source_id= 67e675c7878ef7c6d337c8bd\n",
      "source_id= 67e675c7878ef7c6d337c8bd  Processing  932  of  1036\n",
      "Working on source_id= 67e675c7878ef7c6d337c8cb\n",
      "source_id= 67e675c7878ef7c6d337c8cb  Processing  321  of  357\n",
      "Working on source_id= 67e675c7878ef7c6d337c90f\n",
      "source_id= 67e675c7878ef7c6d337c90f  Processing  22  of  51\n",
      "Working on source_id= 67e675c7878ef7c6d337c990\n",
      "source_id= 67e675c7878ef7c6d337c990  Processing  363  of  533\n",
      "Working on source_id= 67e675c7878ef7c6d337c992\n",
      "source_id= 67e675c7878ef7c6d337c992  Processing  1208  of  1216\n",
      "Working on source_id= 67e675c7878ef7c6d337caca\n",
      "source_id= 67e675c7878ef7c6d337caca  Processing  1102  of  1153\n",
      "Working on source_id= 67e675c7878ef7c6d337cafa\n",
      "source_id= 67e675c7878ef7c6d337cafa  Processing  1072  of  1143\n",
      "Working on source_id= 67e675c7878ef7c6d337cb2e\n",
      "source_id= 67e675c7878ef7c6d337cb2e  Processing  681  of  813\n",
      "Working on source_id= 67e675c7878ef7c6d337cb50\n",
      "source_id= 67e675c7878ef7c6d337cb50  Processing  602  of  650\n",
      "Working on source_id= 67e675c7878ef7c6d337cb78\n",
      "source_id= 67e675c7878ef7c6d337cb78  Processing  975  of  1095\n",
      "Working on source_id= 67e675c7878ef7c6d337cb9c\n",
      "source_id= 67e675c7878ef7c6d337cb9c  Processing  458  of  514\n",
      "Working on source_id= 67e675c7878ef7c6d337cbab\n",
      "source_id= 67e675c7878ef7c6d337cbab  Processing  710  of  720\n",
      "Working on source_id= 67e675c7878ef7c6d337cc4a\n",
      "source_id= 67e675c7878ef7c6d337cc4a  Processing  322  of  423\n",
      "Working on source_id= 67e675c7878ef7c6d337cc6c\n",
      "source_id= 67e675c7878ef7c6d337cc6c  Processing  0  of  16\n",
      "Working on source_id= 67e675c7878ef7c6d337cce9\n",
      "source_id= 67e675c7878ef7c6d337cce9  Processing  779  of  797\n",
      "Working on source_id= 67e675c8878ef7c6d337ce70\n",
      "source_id= 67e675c8878ef7c6d337ce70  Processing  1223  of  1249\n",
      "Working on source_id= 67e675c8878ef7c6d337cea6\n",
      "source_id= 67e675c8878ef7c6d337cea6  Processing  1149  of  1183\n",
      "Working on source_id= 67e675c8878ef7c6d337d048\n",
      "source_id= 67e675c8878ef7c6d337d048  Processing  862  of  872\n",
      "Working on source_id= 67e675c8878ef7c6d337d13f\n",
      "source_id= 67e675c8878ef7c6d337d13f  Processing  996  of  1082\n",
      "Working on source_id= 67e675c8878ef7c6d337d17a\n",
      "source_id= 67e675c8878ef7c6d337d17a  Processing  3  of  32\n",
      "Working on source_id= 67e675c8878ef7c6d337d22a\n",
      "source_id= 67e675c8878ef7c6d337d22a  Processing  1250  of  1257\n",
      "Working on source_id= 67e675c8878ef7c6d337d24f\n",
      "source_id= 67e675c8878ef7c6d337d24f  Processing  512  of  681\n",
      "Working on source_id= 67e675c8878ef7c6d337d251\n",
      "source_id= 67e675c8878ef7c6d337d251  Processing  457  of  609\n",
      "Working on source_id= 67e675c8878ef7c6d337d261\n",
      "source_id= 67e675c8878ef7c6d337d261  Processing  911  of  1067\n",
      "Working on source_id= 67e675c9878ef7c6d337d65a\n",
      "source_id= 67e675c9878ef7c6d337d65a  Processing  852  of  980\n",
      "Working on source_id= 67e675c9878ef7c6d337d6c2\n",
      "source_id= 67e675c9878ef7c6d337d6c2  Processing  682  of  773\n",
      "Working on source_id= 67e675c9878ef7c6d337d702\n",
      "source_id= 67e675c9878ef7c6d337d702  Processing  1010  of  1087\n",
      "Working on source_id= 67e675c9878ef7c6d337d72e\n",
      "source_id= 67e675c9878ef7c6d337d72e  Processing  735  of  815\n",
      "Working on source_id= 67e675c9878ef7c6d337d74b\n",
      "source_id= 67e675c9878ef7c6d337d74b  Processing  893  of  934\n",
      "Working on source_id= 67e675c9878ef7c6d337d74c\n",
      "source_id= 67e675c9878ef7c6d337d74c  Processing  938  of  994\n",
      "Working on source_id= 67e675c9878ef7c6d337d768\n",
      "source_id= 67e675c9878ef7c6d337d768  Processing  1085  of  1146\n",
      "Working on source_id= 67e675c9878ef7c6d337d7bf\n",
      "source_id= 67e675c9878ef7c6d337d7bf  Processing  1099  of  1121\n",
      "Working on source_id= 67e675c9878ef7c6d337d7f9\n",
      "source_id= 67e675c9878ef7c6d337d7f9  Processing  686  of  778\n",
      "Working on source_id= 67e675c9878ef7c6d337d813\n",
      "source_id= 67e675c9878ef7c6d337d813  Processing  723  of  872\n",
      "Working on source_id= 67e675c9878ef7c6d337d858\n",
      "source_id= 67e675c9878ef7c6d337d858  Processing  411  of  801\n",
      "Working on source_id= 67e675c9878ef7c6d337d85f\n",
      "source_id= 67e675c9878ef7c6d337d85f  Processing  4  of  62\n",
      "Working on source_id= 67e675c9878ef7c6d337d88d\n",
      "source_id= 67e675c9878ef7c6d337d88d  Processing  303  of  529\n",
      "Working on source_id= 67e675c9878ef7c6d337d8c2\n",
      "source_id= 67e675c9878ef7c6d337d8c2  Processing  1164  of  1178\n",
      "Working on source_id= 67e675c9878ef7c6d337dab4\n",
      "source_id= 67e675c9878ef7c6d337dab4  Processing  1075  of  1126\n",
      "Working on source_id= 67e675c9878ef7c6d337dc3c\n",
      "source_id= 67e675c9878ef7c6d337dc3c  Processing  1094  of  1116\n",
      "Working on source_id= 67e675ca878ef7c6d337dd52\n",
      "source_id= 67e675ca878ef7c6d337dd52  Processing  900  of  1007\n",
      "Working on source_id= 67e675ca878ef7c6d337dd56\n",
      "source_id= 67e675ca878ef7c6d337dd56  Processing  1154  of  1170\n",
      "Working on source_id= 67e675ca878ef7c6d337de2b\n",
      "source_id= 67e675ca878ef7c6d337de2b  Processing  1146  of  1165\n",
      "Working on source_id= 67e675ca878ef7c6d337df11\n",
      "source_id= 67e675ca878ef7c6d337df11  Processing  969  of  1049\n",
      "Working on source_id= 67e675ca878ef7c6d337e19e\n",
      "source_id= 67e675ca878ef7c6d337e19e  Processing  1169  of  1186\n",
      "Working on source_id= 67e675ca878ef7c6d337e230\n",
      "source_id= 67e675ca878ef7c6d337e230  Processing  782  of  915\n",
      "Working on source_id= 67e675ca878ef7c6d337e24e\n",
      "source_id= 67e675ca878ef7c6d337e24e  Processing  614  of  648\n",
      "Elapsed time to compute RF estimates= 29.070863723754883\n"
     ]
    }
   ],
   "source": [
    "pf=AntelopePf('CNRDeconEngine.pf')\n",
    "engine = CNRDeconEngine(pf)\n",
    "signal_window=TimeWindow(-10.0,100.0)\n",
    "noise_window=TimeWindow(-200.0,-5.0)\n",
    "\n",
    "sidlist=db.wf_Seismogram.distinct('source_id')\n",
    "t0=time.time()\n",
    "# Serial version - acceptable for tutorial as run time is short\n",
    "nprocessed=0\n",
    "for sid in sidlist:\n",
    "    print(\"Working on source_id=\",sid)\n",
    "    rfens = process(sid,db,dbmd,engine,signal_window,noise_window)\n",
    "    nprocessed+=1\n",
    "\n",
    "# Something sent to process does not serialize so either of the parallelizations\n",
    "# below fail complaining of a serialization error.\n",
    "\n",
    "#import dask.bag as dbg\n",
    "#mydata=dbg.from_sequence(sidlist)\n",
    "#mydata = mydata.map(process,db,dbmd,engine,signal_window,noise_window)\n",
    "#mydata.compute()\n",
    "\n",
    "# this demonstrates parallelization of the loop above using Futures approach\n",
    "#dask_client = mspass_client.get_scheduler()\n",
    "#sidlist=db.wf_Seismogram.distinct(\"source_id\")\n",
    "# this will contain a collection of Futures objects that are data ruuning or \n",
    "# queued for processing with the \"process\" function.  \n",
    "#futures_list=[]\n",
    "#for sid in sidlist:\n",
    "#    f = dask_client.submit(process,sid,db,dbmd,engine,signal_window,noise_window)\n",
    "#    futures_list.append(f)\n",
    "# wait for output and print the return values\n",
    "# this construct is not necessary for this algorithm but demonstrates that one \n",
    "# can run a serial section assynchronously as futures return \n",
    "# i.e. output will not be in the order submitted.\n",
    "#print(\"source_id n_in n_saved\")\n",
    "#for future in as_completed(futures_list):\n",
    "#    out = future.result()\n",
    "#    print(out)\n",
    "    \n",
    "t=time.time()\n",
    "print(\"Elapsed time to compute RF estimates=\",t-t0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
