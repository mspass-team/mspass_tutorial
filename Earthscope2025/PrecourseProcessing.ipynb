{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80b9ae13-225b-416a-91ff-c361c02f77c2",
   "metadata": {},
   "source": [
    "# Earthscope MsPASS Short Course - preprocess\n",
    "\n",
    "## *Gary L. Pavlis, Indiana University and Yinzhi (Ian) Wang, TACC*\n",
    "\n",
    "## Purpose of this Notebook\n",
    "This notebook is intended to be run without explanation by students prior to the start of the course.  The purpose is only to create a working data set that you will work with during the first class session.  You may attempt to grok the code in this notebook but be aware the plan is to visit the code in this notebook near the end of the first class session to discuss what exactly it does.  \n",
    "\n",
    "Readers from github reading this who are not in the 2025 short course must realize this notebook was designed to run on Earthscope's GeoLab jupyter lab gateway on AWS.   It can still work if you are running MsPASS on a local computer.   The main difference is if you run __[mspass-desktop](https://www.mspass.org/getting_started/mspass_desktop.html)__ the step below to launch mongod is not necessary.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d25eb5f-5e71-424c-8940-5d3666c468ec",
   "metadata": {},
   "source": [
    "## Step one:  Terminal preprocess preparation steps\n",
    "### Launch the database server\n",
    "You must run a couple of commands in a jupyter terminal window to allow this notebook to be run.   The first is a necessary evil to avoid authentication complexities in a shared database server.   That is, for this class each student will be running their own instance of the database package called MongoDB.  In normal MsPASS usage mongodb is launched as a service automatically, but we have been unable to devise a comparable setup on GeoLab.  Consequently, you will need to do the following:\n",
    "1.  Launch a jupyter \"Terminal\" tab.  (Push the + icon in the upper left to create  \"Launcher\" tab. Then push the Terminal icon.)\n",
    "2.  Then type this incantation:\n",
    "```\n",
    "mongod --dbpath ./db --logpath ./logs\n",
    "```\n",
    "3.  Verify that worked by typing `ps -A`  You should see a line where the CMD field is \"mongod\".   If not, contact me by email or slack if you are unable to solve the problem.\n",
    "\n",
    "### Fetch waveform files\n",
    "Another current limitation of GeoLab is there is no shared storage area.  You will need to download the waveform data stored on AWS as \"S3\" bucket objects to your GeoLab file work space.  Cut-and-paste this incantation in the terminal window you used above:\n",
    "```\n",
    "!mkdir -p wf && cd wf && for i in {0..19}; do wget https://essc-mspass2024.s3.us-east-2.amazonaws.com/Event_${i}.msd;done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412894cb-726a-4a8a-b293-c8f4610dcfe6",
   "metadata": {},
   "source": [
    "## Step two:  Run the notebook\n",
    "Once the steps above are run successfully, the rest of this notebook can be run sequentially from top to bottom.  When mongod is running and you wget command to fetch the waveform data completes in the terminal window just select Run->Run All Cells.   When the notebook finishes you will need to complete a short quiz in Moodle to verify you completed this preclass exercise successfully.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb9c8904-013c-4e21-b680-992335a4e786",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Event(s) in Catalog:\n",
      "2011-12-14T05:04:57.810000Z |  -7.528, +146.814 | 7.1  MW\n",
      "2011-10-28T18:54:34.750000Z | -14.557,  -76.121 | 7.0  MW\n",
      "2011-10-23T10:41:22.010000Z | +38.729,  +43.447 | 7.1  MW\n",
      "2011-10-21T17:57:17.310000Z | -28.881, -176.033 | 7.4  MW\n",
      "2011-09-15T19:31:03.160000Z | -21.593, -179.324 | 7.3  MW\n",
      "2011-09-03T22:55:35.760000Z | -20.628, +169.778 | 7.0  MW\n",
      "2011-08-24T17:46:11.560000Z |  -7.620,  -74.538 | 7.0  MW\n",
      "2011-08-20T18:19:24.610000Z | -18.331, +168.226 | 7.0  MW\n",
      "2011-08-20T16:55:04.090000Z | -18.277, +168.067 | 7.1  MW\n",
      "2011-07-10T00:57:10.910000Z | +38.055, +143.302 | 7.0  MW\n",
      "2011-07-06T19:03:20.470000Z | -29.307, -176.257 | 7.6  MW\n",
      "2011-06-24T03:09:38.920000Z | +51.980, -171.820 | 7.3  MW\n",
      "2011-04-07T14:32:44.100000Z | +38.251, +141.730 | 7.1  MW\n",
      "2011-03-11T06:25:50.740000Z | +38.051, +144.630 | 7.6  MW\n",
      "2011-03-11T06:15:37.570000Z | +36.227, +141.088 | 7.9  MW\n",
      "2011-03-11T05:46:23.200000Z | +38.296, +142.498 | 9.1  MW\n",
      "2011-03-09T02:45:19.590000Z | +38.441, +142.980 | 7.3  MW\n",
      "2011-01-18T20:23:25.570000Z | +28.683,  +63.995 | 7.2  MW\n",
      "2011-01-02T20:20:18.170000Z | -38.391,  -73.399 | 7.1  MW\n",
      "2011-01-01T09:56:58.460000Z | -26.851,  -63.237 | 7.0  MW\n"
     ]
    }
   ],
   "source": [
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn import Client\n",
    "client=Client(\"IRIS\")\n",
    "ts=UTCDateTime('2011-01-01T00:00:00.0')\n",
    "starttime=ts\n",
    "te=UTCDateTime('2012-01-01T00:00:00.0')\n",
    "endtime=te\n",
    "lat0=38.3\n",
    "lon0=142.5\n",
    "minmag=7.0\n",
    "\n",
    "cat=client.get_events(starttime=starttime,endtime=endtime,\n",
    "        minmagnitude=minmag)\n",
    "# this is a weird incantation suggested by obspy to print a summeary of all the events\n",
    "print(cat.__str__(print_all=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0389c9a0-e0c0-43c1-86b9-57f5aa65381a",
   "metadata": {},
   "source": [
    "This next section creates a client and related database handle to interact with MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc38596a-5c3d-4f1c-8a9a-da340b4c33c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mspasspy.client as msc\n",
    "\n",
    "msc_client=msc.Client()\n",
    "dbclient=msc_client.get_database_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2118762d-8a52-4f4c-96ff-17382b5ead85",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = \"Earthscope2025\"\n",
    "db = dbclient.get_database(dbname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8d2144-d8ad-4aa7-828f-aa778aa55056",
   "metadata": {},
   "source": [
    "As the name suggests this saves the data downloaded by obspy to MongoDB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bad3b1e-2785-451d-85fc-bcbba3d3b038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of event entries saved in source collection= 20\n"
     ]
    }
   ],
   "source": [
    "n=db.save_catalog(cat)\n",
    "print('number of event entries saved in source collection=',n)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ec91519-8dc0-44d9-a980-fba239c68372",
   "metadata": {},
   "source": [
    "import subprocess\n",
    "s3url_base= \"https://essc-mspass2024.s3.us-east-2.amazonaws.com/\"\n",
    "shell_command = \"mkdir -p wf && cd wf && for i in {0..19}; do {}Event_${i}.msd;done\".format(s3url_base)\n",
    "runcout = subprocess.run(shell_command, shell=True, capture_output=True, text=True)\n",
    "print(\"Standard output from wget command:\")\n",
    "print(ruout.stdout)\n",
    "print(\"Standard output from wget commend:\")\n",
    "print(runout.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7028e00-2c7d-4689-b7d4-c4a8a99716cb",
   "metadata": {},
   "source": [
    "The files we just downloaded are raw miniseed data files.   We need to build an index that MsPASS can use to crack these files and load them into our processing workflow later.   We run this step in parallel to improve performance as this process has to pass through every byte of all 20 files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fa90cd0-10aa-4d98-99c4-34cd89422214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Event_7.msd', 'Event_17.msd', 'Event_0.msd', 'Event_11.msd', 'Event_4.msd', 'Event_1.msd', 'Event_14.msd', 'Event_12.msd', 'Event_15.msd', 'Event_6.msd', 'Event_16.msd', 'Event_19.msd', 'Event_13.msd', 'Event_8.msd', 'Event_5.msd', 'Event_9.msd', 'Event_3.msd', 'Event_2.msd', 'Event_10.msd', 'Event_18.msd']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dask.bag as dbg\n",
    "# remove the comment below if you need to restart this workflow \n",
    "# at this point c\n",
    "#db.drop_collection('wf_miniseed')\n",
    "# Note this dir value assumes the wf dir was created with \n",
    "# the previous command that also downloads the data from AWS\n",
    "current_directory = os.getcwd()\n",
    "dir = os.path.join(current_directory, 'wf')\n",
    "dfilelist=[]\n",
    "with os.scandir(dir) as entries:\n",
    "    for entry in entries:\n",
    "        if entry.is_file():\n",
    "            dfilelist.append(entry.name)\n",
    "print(dfilelist)\n",
    "mydata = dbg.from_sequence(dfilelist)\n",
    "mydata = mydata.map(db.index_mseed_file,dir=dir)\n",
    "index_return = mydata.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe533cb0-ab7d-474b-866a-66c333325723",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wf_miniseed indexing documents saved by MongoDB =  26247\n"
     ]
    }
   ],
   "source": [
    "n=db.wf_miniseed.count_documents({})\n",
    "print(\"Number of wf_miniseed indexing documents saved by MongoDB = \",n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0559384-2c1d-49ea-998c-65b65b13ffa9",
   "metadata": {},
   "source": [
    "Next retrieve station metadata with web services using obspy.  Result is loaded as single obspy *Inventory* object.  We then save the data to MongoDB with the MsPASS databse method called *save_inventory*.   The *Inventory* object is disassembled to save the contents as in the form of a python dictionary == a MongoDB document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74c59abc-075b-4d62-ac3c-87e159d46254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stations retrieved= 855\n"
     ]
    }
   ],
   "source": [
    "ts=UTCDateTime('2010-01-01T00:00:00.0')\n",
    "starttime=ts\n",
    "te=UTCDateTime('2013-01-01T00:00:00.0')\n",
    "inv=client.get_stations(network='TA',starttime=starttime,endtime=endtime,\n",
    "                        format='xml',channel='BH?',level='response')\n",
    "net=inv.networks\n",
    "x=net[0]\n",
    "sta=x.stations\n",
    "print(\"Number of stations retrieved=\",len(sta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7625db1b-7e0a-401d-bd50-e52f4dc2fd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database.save_inventory processing summary:\n",
      "Number of site records processed= 857\n",
      "number of site records saved= 857\n",
      "number of channel records processed= 2796\n",
      "number of channel records saved= 2784\n",
      "save_inventory returned value= (857, 2784, 857, 2796)\n"
     ]
    }
   ],
   "source": [
    "ret=db.save_inventory(inv)\n",
    "print('save_inventory returned value=',ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a087a29e-59f0-4ecf-b09e-66a62bfe8bdd",
   "metadata": {},
   "source": [
    "This section creates a normalization cross-reference needed later to connect wf documents to matching documents in the *site*, *channel*, and *source* collections.   The algorithms used for source and receiver data are diferent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "719f8602-d4e8-4ecf-92b5-69558c53914d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents processed in wf_miniseed= 26247\n",
      "Number of documents updated with channel cross reference id= 26247\n",
      "Number of documents updated with site cross reference id= 26247\n",
      "Number of documents updated with source cross reference id= 26232\n"
     ]
    }
   ],
   "source": [
    "from mspasspy.db.normalize import (\n",
    "    bulk_normalize,\n",
    "    MiniseedMatcher,\n",
    "    OriginTimeMatcher,\n",
    ")\n",
    "chan_matcher = MiniseedMatcher(db,\n",
    "        collection=\"channel\",\n",
    "        attributes_to_load=[\"starttime\",\"endtime\",\"lat\",\"lon\",\"elev\",\"hang\",\"vang\",\"_id\"],\n",
    "    )\n",
    "site_matcher = MiniseedMatcher(db,\n",
    "        collection=\"site\",\n",
    "        attributes_to_load=[\"starttime\",\"endtime\",\"lat\",\"lon\",\"elev\",\"_id\"],\n",
    "    )\n",
    "source_matcher = OriginTimeMatcher(db,t0offset=300.0,tolerance=100.0)\n",
    "ret = bulk_normalize(db,matcher_list=[chan_matcher,site_matcher,source_matcher])\n",
    "print(\"Number of documents processed in wf_miniseed=\",ret[0])\n",
    "print(\"Number of documents updated with channel cross reference id=\",ret[1])\n",
    "print(\"Number of documents updated with site cross reference id=\",ret[2])\n",
    "print(\"Number of documents updated with source cross reference id=\",ret[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71bf571f-4c00-4bde-b740-08bcb14b41fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.algorithms.window import WindowData\n",
    "from mspasspy.algorithms.signals import detrend\n",
    "from mspasspy.algorithms.basic import ator\n",
    "from mspasspy.ccore.algorithms.basic import TimeWindow\n",
    "from mspasspy.ccore.utility import ErrorSeverity\n",
    "from mspasspy.db.normalize import (normalize,\n",
    "                                   ObjectIdMatcher,\n",
    "                                   OriginTimeMatcher)\n",
    "from mspasspy.algorithms.window import WindowData\n",
    "from mspasspy.algorithms.resample import (ScipyResampler,\n",
    "                                          ScipyDecimator,\n",
    "                                          resample,\n",
    "                                         )\n",
    "from obspy.geodetics import gps2dist_azimuth,kilometers2degrees\n",
    "from obspy.taup import TauPyModel\n",
    "import time\n",
    "\n",
    "def set_PStime(d,Ptimekey=\"Ptime\",Stimekey=\"Stime\",model=None):\n",
    "    \"\"\"\n",
    "    Function to calculate P and S wave arrival time and set times \n",
    "    as the header (Metadata) fields defined by Ptimekey and Stimekey.\n",
    "    Tries to handle some complexities of the travel time calculator \n",
    "    returns when one or both P and S aren't calculatable.  That is \n",
    "    the norm in or at the edge of the core shadow.  \n",
    "    \n",
    "    :param d:  input TimeSeries datum.  Assumes datum's Metadata \n",
    "      contains stock source and channel attributes.  \n",
    "    :param Ptimekey:  key used to define the header attribute that \n",
    "      will contain the computed P time.  Default \"Ptime\".\n",
    "    :param model:  instance of obspy TauPyModel travel time engine. \n",
    "      Default is None.   That mode is slow as an new engine will be\n",
    "      constructed on each call to the function.  Normal use should \n",
    "      pass an instance for greater efficiency.  \n",
    "    \"\"\"\n",
    "    if d.live:\n",
    "        if model is None:\n",
    "            model = TauPyModel(model=\"iasp91\") \n",
    "        # extract required source attributes\n",
    "        srclat=d[\"source_lat\"]\n",
    "        srclon=d[\"source_lon\"]\n",
    "        srcz=d[\"source_depth\"]\n",
    "        srct=d[\"source_time\"] \n",
    "        # extract required channel attributes\n",
    "        stalat=d[\"channel_lat\"]\n",
    "        stalon=d[\"channel_lon\"]\n",
    "        staelev=d[\"channel_elev\"]\n",
    "        # set up and run travel time calculator\n",
    "        georesult=gps2dist_azimuth(srclat,srclon,stalat,stalon)\n",
    "        # obspy's function we just called returns distance in m in element 0 of a tuple\n",
    "        # their travel time calculator it is degrees so we need this conversion\n",
    "        dist=kilometers2degrees(georesult[0]/1000.0)\n",
    "        arrivals=model.get_travel_times(source_depth_in_km=srcz,\n",
    "                                            distance_in_degree=dist,\n",
    "                                            phase_list=['P','S'])\n",
    "        # always post this for as it is not cheap to compute\n",
    "        # WARNING:  don't use common abbrevation delta - collides with data dt\n",
    "        d['epicentral_distance']=dist\n",
    "        # these are CSS3.0 shorthands s - station e - event\n",
    "        esaz = georesult[1]\n",
    "        seaz = georesult[2]\n",
    "        # css3.0 names esax = event to source azimuth; seaz = source to event azimuth\n",
    "        d['esaz']=esaz\n",
    "        d['seaz']=seaz\n",
    "        # get_travel_times returns an empty list if a P time cannot be \n",
    "        # calculated.  We trap that condition and kill the output \n",
    "        # with an error message\n",
    "        if len(arrivals)==2:\n",
    "            Ptime=srct+arrivals[0].time\n",
    "            rayp = arrivals[0].ray_param\n",
    "            Stime=srct+arrivals[1].time\n",
    "            rayp_S = arrivals[1].ray_param\n",
    "            d.put(Ptimekey,Ptime)\n",
    "            d.put(Stimekey,Stime)\n",
    "            # These keys are not passed as arguments but could be - a choice\n",
    "            # Ray parameter is needed for free surface transformation operator\n",
    "            # note tau p calculator in obspy returns p=R sin(theta)/V_0\n",
    "            d.put(\"rayp_P\",rayp)\n",
    "            d.put(\"rayp_S\",rayp_S)\n",
    "        elif len(arrivals)==1:\n",
    "            if arrivals[0].name == 'P':\n",
    "                Ptime=srct+arrivals[0].time\n",
    "                rayp = arrivals[0].ray_param\n",
    "                d.put(Ptimekey,Ptime)\n",
    "                d.put(\"rayp_P\",rayp)\n",
    "            else:\n",
    "                # Not sure we can assume name is S\n",
    "                if arrivals[0].name == 'S':\n",
    "                    Stime=srct+arrivals[0].time\n",
    "                    rayp_S = arrivals[0].ray_param\n",
    "                    d.put(Stimekey,Stime)\n",
    "                    d.put(\"rayp_S\",rayp_S)\n",
    "                else:\n",
    "                    message = \"Unexpected single phase name returned by taup calculator\\n\"\n",
    "                    message += \"Expected phase name S but got \" + arrivals[0].name\n",
    "                    d.elog.log_error(\"set_PStime\",\n",
    "                                     message,\n",
    "                                     ErrorSeverity.Invalid)\n",
    "                    d.kill()\n",
    "                \n",
    "    # Note python indents mean if an ensemble is marked dead this function just silenetly returns \n",
    "    # what it received doing nothing - correct mspass model\n",
    "    return d\n",
    "def cut_Pwindow(d,stime=-100.0,etime=500.0):\n",
    "    \"\"\"\n",
    "    Window datum relative to P time window.  Time\n",
    "    interval extracted is Ptime+stime to Ptime+etime.\n",
    "    Uses ator,rtoa feature of MsPASS.\n",
    "    \"\"\"\n",
    "    if d.live:\n",
    "        if \"Ptime\" in d:\n",
    "            ptime = d[\"Ptime\"]\n",
    "            d.ator(ptime)\n",
    "            d = WindowData(d,stime,etime)\n",
    "            d.rtoa()\n",
    "    return d\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bacb3a49-8881-42a3-9bc4-f7fecaf43a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/obspy/io/mseed/headers.py:805: InternalMSEEDWarning: readMSEEDBuffer(): Unexpected end of file when parsing record starting at offset 69632. The rest of the file will not be read.\n",
      "  warnings.warn(_w, InternalMSEEDWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time= 408.6127417087555\n",
      "Number of live data saved= 26178\n",
      "number of data killed= 69\n"
     ]
    }
   ],
   "source": [
    "from obspy.taup import TauPyModel\n",
    "from mspasspy.algorithms.signals import filter,detrend\n",
    "from mspasspy.util.Janitor import Janitor\n",
    "\n",
    "ttmodel = TauPyModel(model=\"iasp91\")\n",
    "stime=-100.0\n",
    "etime=500.0\n",
    "\n",
    "janitor = Janitor()\n",
    "# nonstandard keys added for travel times - need to save these to keep them from being thrown out by the janitor\n",
    "for k in [\"seaz\",\"esaz\",\"Ptime\",\"epicentral_distance\",\"rayp_P\",\"rayp_S\"]:\n",
    "    janitor.add2keepers(k)\n",
    "\n",
    "t0 = time.time()\n",
    "nlive=0\n",
    "ndead=0\n",
    "cursor=db.wf_miniseed.find({})\n",
    "for doc in cursor:\n",
    "    # the normalize matchers in this read were defined in the normalize section of this \n",
    "    # notebook.  Could cause problems if this box is run out of order\n",
    "    d = db.read_data(doc,collection='wf_miniseed',normalize=[chan_matcher,source_matcher])\n",
    "    d = detrend(d,type=\"constant\")\n",
    "    d = filter(d,'lowpass',freq=2.0,zerophase=False)\n",
    "    \n",
    "    # this function will run faster if passed an instance the TauP calculator (ttmodel)\n",
    "    # If left as the default an instance is instantiated on each call to the function which is very inefficient.\n",
    "    d = set_PStime(d,model=ttmodel)\n",
    "    d = cut_Pwindow(d,stime,etime)\n",
    "    if d.live:\n",
    "        nlive += 1\n",
    "    else:\n",
    "        ndead += 1\n",
    "    janitor.clean(d)\n",
    "    db.save_data(d,storage_mode='file',dir='./wf_TimeSeries',data_tag='serial_preprocessed')\n",
    "t=time.time()    \n",
    "print(\"Total processing time=\",t-t0)\n",
    "print(\"Number of live data saved=\",nlive)\n",
    "print(\"number of data killed=\",ndead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "330f96f1-561b-46ed-9ac1-0ece0801f013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This run will process  20  common source gathers into Seismogram objects\n",
      "working on ensemble with source_id= 6846c258f5576e7425beca63\n",
      "database contains  1311  documents == channels for this ensemble\n",
      "Number of TimeSeries objects constructed for this source= 1311\n",
      "Number of (3C) Seismogram object created from input ensemble= 437\n",
      "number of bundled Seismogram= 437\n",
      "number of killed Seismogram= 0\n",
      "working on ensemble with source_id= 6846c258f5576e7425beca64\n",
      "database contains  1332  documents == channels for this ensemble\n",
      "Number of TimeSeries objects constructed for this source= 1332\n",
      "Number of (3C) Seismogram object created from input ensemble= 444\n",
      "number of bundled Seismogram= 444\n",
      "number of killed Seismogram= 0\n",
      "working on ensemble with source_id= 6846c258f5576e7425beca65\n",
      "database contains  1323  documents == channels for this ensemble\n",
      "Number of TimeSeries objects constructed for this source= 1323\n",
      "Number of (3C) Seismogram object created from input ensemble= 441\n",
      "number of bundled Seismogram= 441\n",
      "number of killed Seismogram= 0\n",
      "working on ensemble with source_id= 6846c258f5576e7425beca66\n",
      "database contains  1311  documents == channels for this ensemble\n",
      "Number of TimeSeries objects constructed for this source= 1311\n",
      "Number of (3C) Seismogram object created from input ensemble= 437\n",
      "number of bundled Seismogram= 437\n",
      "number of killed Seismogram= 0\n",
      "working on ensemble with source_id= 6846c258f5576e7425beca67\n",
      "database contains  1296  documents == channels for this ensemble\n",
      "Number of TimeSeries objects constructed for this source= 1296\n",
      "Number of (3C) Seismogram object created from input ensemble= 432\n",
      "number of bundled Seismogram= 432\n",
      "number of killed Seismogram= 0\n",
      "working on ensemble with source_id= 6846c258f5576e7425beca68\n",
      "database contains  1308  documents == channels for this ensemble\n",
      "Number of TimeSeries objects constructed for this source= 1308\n",
      "Number of (3C) Seismogram object created from input ensemble= 436\n",
      "number of bundled Seismogram= 436\n",
      "number of killed Seismogram= 0\n",
      "working on ensemble with source_id= 6846c258f5576e7425beca69\n",
      "database contains  1317  documents == channels for this ensemble\n",
      "Number of TimeSeries objects constructed for this source= 1317\n",
      "Number of (3C) Seismogram object created from input ensemble= 439\n",
      "number of bundled Seismogram= 439\n",
      "number of killed Seismogram= 0\n",
      "working on ensemble with source_id= 6846c258f5576e7425beca6a\n",
      "database contains  1335  documents == channels for this ensemble\n",
      "Number of TimeSeries objects constructed for this source= 1335\n",
      "Number of (3C) Seismogram object created from input ensemble= 445\n",
      "number of bundled Seismogram= 445\n",
      "number of killed Seismogram= 0\n",
      "working on ensemble with source_id= 6846c258f5576e7425beca6b\n",
      "database contains  1335  documents == channels for this ensemble\n",
      "Number of TimeSeries objects constructed for this source= 1335\n",
      "Number of (3C) Seismogram object created from input ensemble= 445\n",
      "number of bundled Seismogram= 445\n",
      "number of killed Seismogram= 0\n",
      "working on ensemble with source_id= 6846c258f5576e7425beca6c\n",
      "database contains  1311  documents == channels for this ensemble\n",
      "Number of TimeSeries objects constructed for this source= 1311\n",
      "Number of (3C) Seismogram object created from input ensemble= 437\n",
      "number of bundled Seismogram= 437\n",
      "number of killed Seismogram= 0\n",
      "working on ensemble with source_id= 6846c258f5576e7425beca6d\n",
      "database contains  1311  documents == channels for this ensemble\n",
      "Number of TimeSeries objects constructed for this source= 1311\n",
      "Number of (3C) Seismogram object created from input ensemble= 437\n",
      "number of bundled Seismogram= 437\n",
      "number of killed Seismogram= 0\n",
      "working on ensemble with source_id= 6846c258f5576e7425beca6e\n",
      "database contains  1284  documents == channels for this ensemble\n",
      "Number of TimeSeries objects constructed for this source= 1284\n",
      "Number of (3C) Seismogram object created from input ensemble= 428\n",
      "number of bundled Seismogram= 428\n",
      "number of killed Seismogram= 0\n",
      "working on ensemble with source_id= 6846c258f5576e7425beca6f\n",
      "database contains  1287  documents == channels for this ensemble\n",
      "Number of TimeSeries objects constructed for this source= 1287\n",
      "Number of (3C) Seismogram object created from input ensemble= 429\n",
      "number of bundled Seismogram= 429\n",
      "number of killed Seismogram= 0\n",
      "working on ensemble with source_id= 6846c258f5576e7425beca70\n",
      "database contains  1311  documents == channels for this ensemble\n",
      "Number of TimeSeries objects constructed for this source= 1311\n",
      "Number of (3C) Seismogram object created from input ensemble= 437\n",
      "number of bundled Seismogram= 437\n",
      "number of killed Seismogram= 0\n",
      "working on ensemble with source_id= 6846c258f5576e7425beca71\n",
      "database contains  1311  documents == channels for this ensemble\n",
      "Number of TimeSeries objects constructed for this source= 1311\n",
      "Number of (3C) Seismogram object created from input ensemble= 437\n",
      "number of bundled Seismogram= 437\n",
      "number of killed Seismogram= 0\n",
      "working on ensemble with source_id= 6846c258f5576e7425beca72\n",
      "database contains  1311  documents == channels for this ensemble\n",
      "Number of TimeSeries objects constructed for this source= 1311\n",
      "Number of (3C) Seismogram object created from input ensemble= 437\n",
      "number of bundled Seismogram= 437\n",
      "number of killed Seismogram= 0\n",
      "working on ensemble with source_id= 6846c258f5576e7425beca73\n",
      "database contains  1305  documents == channels for this ensemble\n",
      "Number of TimeSeries objects constructed for this source= 1305\n",
      "Number of (3C) Seismogram object created from input ensemble= 435\n",
      "number of bundled Seismogram= 435\n",
      "number of killed Seismogram= 0\n",
      "working on ensemble with source_id= 6846c258f5576e7425beca74\n",
      "database contains  1293  documents == channels for this ensemble\n",
      "Number of TimeSeries objects constructed for this source= 1293\n",
      "Number of (3C) Seismogram object created from input ensemble= 431\n",
      "number of bundled Seismogram= 431\n",
      "number of killed Seismogram= 0\n",
      "working on ensemble with source_id= 6846c258f5576e7425beca75\n",
      "database contains  1293  documents == channels for this ensemble\n",
      "Number of TimeSeries objects constructed for this source= 1293\n",
      "Number of (3C) Seismogram object created from input ensemble= 431\n",
      "number of bundled Seismogram= 431\n",
      "number of killed Seismogram= 0\n",
      "working on ensemble with source_id= 6846c258f5576e7425beca76\n",
      "database contains  1293  documents == channels for this ensemble\n",
      "Number of TimeSeries objects constructed for this source= 1293\n",
      "Number of (3C) Seismogram object created from input ensemble= 431\n",
      "number of bundled Seismogram= 431\n",
      "number of killed Seismogram= 0\n",
      "Time to convert all data to Seismogram objects= 66.97771906852722\n"
     ]
    }
   ],
   "source": [
    "from mspasspy.algorithms.bundle import bundle_seed_data\n",
    "from mspasspy.util.Undertaker import Undertaker\n",
    "\n",
    "stedronsky=Undertaker(db)\n",
    "t0=time.time()\n",
    "srcids=db.wf_TimeSeries.distinct('source_id')\n",
    "nsrc=len(srcids)\n",
    "print(\"This run will process \",nsrc,\n",
    "      \" common source gathers into Seismogram objects\")\n",
    "for sid in srcids:\n",
    "    query={'source_id' : sid,\n",
    "           'data_tag' : 'serial_preprocessed'}\n",
    "    nd=db.wf_TimeSeries.count_documents(query)\n",
    "    print('working on ensemble with source_id=',sid)\n",
    "    print('database contains ',nd,' documents == channels for this ensemble')\n",
    "    cursor=db.wf_TimeSeries.find(query)\n",
    "    # For this operation we only need channel metadata loaded by normalization\n",
    "    # orientation data is critical (hang and vang attributes)\n",
    "    ensemble = db.read_data(cursor,\n",
    "                            normalize=[chan_matcher],\n",
    "                           )\n",
    "    print('Number of TimeSeries objects constructed for this source=',len(ensemble.member))\n",
    "    ensemble=bundle_seed_data(ensemble)\n",
    "    # The reader would do the following handling of dead data automatically\n",
    "    # it is included here for demonstration purposes only\n",
    "    # part of the lesson is handling of dead data\n",
    "    print('Number of (3C) Seismogram object created from input ensemble=',len(ensemble.member))\n",
    "    [living,bodies]=stedronsky.bring_out_your_dead(ensemble)\n",
    "    print('number of bundled Seismogram=',len(living.member))\n",
    "    print('number of killed Seismogram=',len(bodies.member))\n",
    "    # bury the dead if necessary\n",
    "    if len(bodies.member)>0:\n",
    "        stedronsky.bury(bodies)\n",
    "    janitor.clean(ensemble)\n",
    "    db.save_data(ensemble,storage_mode='file',dir='./wf_Seismogram',collection='wf_Seismogram',data_tag='serial_preprocessed')\n",
    "\n",
    "t = time.time()\n",
    "print(\"Time to convert all data to Seismogram objects=\",t-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f9007e-d452-4565-981c-bd749ed116dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
