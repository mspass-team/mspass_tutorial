{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80b9ae13-225b-416a-91ff-c361c02f77c2",
   "metadata": {},
   "source": [
    "# Earthscope MsPASS Short Course - preprocess\n",
    "\n",
    "## *Gary L. Pavlis, Indiana University and Yinzhi (Ian) Wang, TACC*\n",
    "\n",
    "## Purpose of this Notebook\n",
    "This notebook is intended to be run without explanation by students prior to the start of the course.  The purpose is only to create a working data set that you will work with during the first class session.  You may attempt to grok the code in this notebook but be aware the plan is to visit the code in this notebook near the end of the first class session to discuss what exactly it does.  \n",
    "\n",
    "Readers from github reading this who are not in the 2025 short course must realize this notebook was designed to run on Earthscope's GeoLab jupyter lab gateway on AWS.   It can still work if you are running MsPASS on a local computer.   The main difference is if you run __[mspass-desktop](https://www.mspass.org/getting_started/mspass_desktop.html)__ the step below to launch mongod is not necessary.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d25eb5f-5e71-424c-8940-5d3666c468ec",
   "metadata": {},
   "source": [
    "## Step one:  Terminal preprocess preparation steps\n",
    "### Launch the database server\n",
    "You will need to run a command in a jupyter terminal window to allow this notebook to be run.   This is a necessary evil to avoid authentication complexities in a shared database server that would could have used to solve this problem otherwise.   As a result, for this class each student will be running their own instance of the database package called MongoDB.  That is actually the mode of running MsPASS for most research applications anyway.  On the other hand, in normal MsPASS usage mongodb is launched as a service automatically, but we have been unable to devise a comparable setup on GeoLab.  Consequently, you will need to do the following:\n",
    "1.  Launch a jupyter \"Terminal\":\n",
    "    a.  Push the + icon in the upper left corner if you aren't seeing a Launcher tab in your browser\n",
    "    b.  In the Launcher window select \"Terminal\".\n",
    "2.  Create two workng directories you will need for this exercise in your home directory.  You can copy and paste the following into the terminal you just launched:\n",
    "```\n",
    "mkdir db\n",
    "mkdir logs\n",
    "```\n",
    "3.  Then type this incantation to launch a MongoDB server.\n",
    "```\n",
    "mongod --dbpath ./db --logpath ./logs/mongo_log\n",
    "```\n",
    "Note each time you reconnect to GeoLab you will need to relaunch the MonogDB server with that last incantation. \n",
    "4.  Verify that worked by typing `ps -A`  You should see a line where the CMD field is \"mongod\".   If not, contact me by email or slack if you are unable to solve the problem.\n",
    "\n",
    "### Use outside GeoLab\n",
    "If you are accessing this notebook from github and are not part of the 2025 short course you can still run this tutorial notebook on a desktop system. To do so you will need to do two things:\n",
    "1.  Install the `mspass-desktop` GUI described in __[MsPASS User's Manual here](https://www.mspass.org/getting_started/mspass_desktop.html)__\n",
    "2.  Launch MsPASS as described on that page and run this notebook using the \"run\" button on the GUI. Alternatively, you may run it interactively by pushing the \"jupyter\" button."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412894cb-726a-4a8a-b293-c8f4610dcfe6",
   "metadata": {},
   "source": [
    "## Step two:  Run the notebook\n",
    "Once the steps above are run successfully, the rest of this notebook can be run sequentially from top to bottom.  Just select *Run->Run All Cells* or *Kernel-> Restart Kernal and Run All Cells ... I\n",
    "\n",
    "Note running this entire notebook take on the order of 10 minutes to run.   If you chose to run it box by box be prepared to wait several minutes for some of the code boxes to run.  When the notebook finishes you will need to complete a short quiz in Moodle to verify you completed this preclass exercise successfully.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9c8904-013c-4e21-b680-992335a4e786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn import Client\n",
    "client=Client(\"IRIS\")\n",
    "ts=UTCDateTime('2011-01-01T00:00:00.0')\n",
    "starttime=ts\n",
    "te=UTCDateTime('2012-01-01T00:00:00.0')\n",
    "endtime=te\n",
    "lat0=38.3\n",
    "lon0=142.5\n",
    "minmag=7.0\n",
    "\n",
    "cat=client.get_events(starttime=starttime,endtime=endtime,\n",
    "        minmagnitude=minmag)\n",
    "# this is a weird incantation suggested by obspy to print a summeary of all the events\n",
    "print(cat.__str__(print_all=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0389c9a0-e0c0-43c1-86b9-57f5aa65381a",
   "metadata": {},
   "source": [
    "This next section creates a client and related database handle to interact with MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc38596a-5c3d-4f1c-8a9a-da340b4c33c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mspasspy.client as msc\n",
    "\n",
    "msc_client=msc.Client()\n",
    "dbclient=msc_client.get_database_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2118762d-8a52-4f4c-96ff-17382b5ead85",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = \"Earthscope2025\"\n",
    "db = dbclient.get_database(dbname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8d2144-d8ad-4aa7-828f-aa778aa55056",
   "metadata": {},
   "source": [
    "As the name suggests this saves the data downloaded by obspy to MongoDB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bad3b1e-2785-451d-85fc-bcbba3d3b038",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=db.save_catalog(cat)\n",
    "print('number of event entries saved in source collection=',n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb63dc6-c849-4afe-a268-d2d5a393853e",
   "metadata": {},
   "source": [
    "Next we fetch a set of waveform files from AWS S3.  We will talk about the incantations below in the third class on cloud computing.  For now you should realize that the incantations below download a set of 20 files form AWS.  Those 20 files are miniseed format event files that were obtained using web services to extract the data from the Earthscope archives. On the cloud session we will refer to the approach here as the \"download\" model where you save an image of what is transferred from AWS to local storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade700ae-ff00-49de-be39-3e08870365e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this may fail if this notebook is rerun - mkdir may need a conditional\n",
    "import os\n",
    "wfdir=\"./wf\"\n",
    "if os.path.isdir(wfdir):\n",
    "    print(\"Warning:  \",wfdir,\" already exists.  miniseed files in that directory will be overwritten\")\n",
    "else:\n",
    "    os.mkdir(wfdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb4ed95-9691-4fe6-97fe-93dfb8204f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the AWS python package for accessing \"s3 data'\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "s3=boto3.resource(\"s3\",config=Config(signature_version=UNSIGNED))  # the config arg seems necessary for anonymous access\n",
    "bucket=\"essc-mspass2024\"\n",
    "for evid in range(20):\n",
    "    # for this bucket the file name is the key required by boto3\n",
    "    fname = \"Event_{}.msd\".format(evid)\n",
    "    key = fname\n",
    "    # this is a file path passed to download_file as the output file name\n",
    "    path = wfdir+\"/\"+fname\n",
    "    s3.Bucket(bucket).download_file(key,path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7028e00-2c7d-4689-b7d4-c4a8a99716cb",
   "metadata": {},
   "source": [
    "The files we just downloaded are raw miniseed data files.   We need to build an index that MsPASS can use to crack these files and load them into our processing workflow later.   We run this step in parallel to improve performance as this process has to pass through every byte of all 20 files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa90cd0-10aa-4d98-99c4-34cd89422214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dask.bag as dbg\n",
    "# remove the comment below if you need to restart this workflow \n",
    "# at this point c\n",
    "#db.drop_collection('wf_miniseed')\n",
    "# Note this dir value assumes the wf dir was created with \n",
    "# the previous command that also downloads the data from AWS\n",
    "current_directory = os.getcwd()\n",
    "dir = os.path.join(current_directory, 'wf')\n",
    "dfilelist=[]\n",
    "with os.scandir(dir) as entries:\n",
    "    for entry in entries:\n",
    "        if entry.is_file():\n",
    "            dfilelist.append(entry.name)\n",
    "print(dfilelist)\n",
    "mydata = dbg.from_sequence(dfilelist)\n",
    "mydata = mydata.map(db.index_mseed_file,dir=dir)\n",
    "index_return = mydata.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe533cb0-ab7d-474b-866a-66c333325723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n=db.wf_miniseed.count_documents({})\n",
    "print(\"Number of wf_miniseed indexing documents saved by MongoDB = \",n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0559384-2c1d-49ea-998c-65b65b13ffa9",
   "metadata": {},
   "source": [
    "Next retrieve station metadata with web services using obspy.  Result is loaded as single obspy *Inventory* object.  We then save the data to MongoDB with the MsPASS databse method called *save_inventory*.   The *Inventory* object is disassembled to save the contents as in the form of a python dictionary == a MongoDB document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c59abc-075b-4d62-ac3c-87e159d46254",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts=UTCDateTime('2010-01-01T00:00:00.0')\n",
    "starttime=ts\n",
    "te=UTCDateTime('2013-01-01T00:00:00.0')\n",
    "inv=client.get_stations(network='TA',starttime=starttime,endtime=endtime,\n",
    "                        format='xml',channel='BH?',level='response')\n",
    "net=inv.networks\n",
    "x=net[0]\n",
    "sta=x.stations\n",
    "print(\"Number of stations retrieved=\",len(sta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7625db1b-7e0a-401d-bd50-e52f4dc2fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret=db.save_inventory(inv)\n",
    "print('save_inventory returned value=',ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a087a29e-59f0-4ecf-b09e-66a62bfe8bdd",
   "metadata": {},
   "source": [
    "This section creates a normalization cross-reference needed later to connect wf documents to matching documents in the *site*, *channel*, and *source* collections.   The algorithms used for source and receiver data are diferent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719f8602-d4e8-4ecf-92b5-69558c53914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.db.normalize import (\n",
    "    bulk_normalize,\n",
    "    MiniseedMatcher,\n",
    "    OriginTimeMatcher,\n",
    ")\n",
    "chan_matcher = MiniseedMatcher(db,\n",
    "        collection=\"channel\",\n",
    "        attributes_to_load=[\"starttime\",\"endtime\",\"lat\",\"lon\",\"elev\",\"hang\",\"vang\",\"_id\"],\n",
    "    )\n",
    "site_matcher = MiniseedMatcher(db,\n",
    "        collection=\"site\",\n",
    "        attributes_to_load=[\"starttime\",\"endtime\",\"lat\",\"lon\",\"elev\",\"_id\"],\n",
    "    )\n",
    "source_matcher = OriginTimeMatcher(db,t0offset=300.0,tolerance=100.0)\n",
    "ret = bulk_normalize(db,matcher_list=[chan_matcher,site_matcher,source_matcher])\n",
    "print(\"Number of documents processed in wf_miniseed=\",ret[0])\n",
    "print(\"Number of documents updated with channel cross reference id=\",ret[1])\n",
    "print(\"Number of documents updated with site cross reference id=\",ret[2])\n",
    "print(\"Number of documents updated with source cross reference id=\",ret[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b46d6d1-bfbb-400b-af06-a47c31ae0c29",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36ce71a3-d680-4478-8c7e-c65178d6823f",
   "metadata": {},
   "source": [
    "Next we define a set of specialized functions for completing the processing of this notebook.  The docstring with each function describes what it does.  Note that all use components of MsPASS to form a composite function that does something useful from the pieces of the framework.  That is a general theme for developing a new processing workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bf571f-4c00-4bde-b740-08bcb14b41fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.algorithms.window import WindowData\n",
    "from mspasspy.algorithms.signals import detrend\n",
    "from mspasspy.algorithms.basic import ator\n",
    "from mspasspy.ccore.algorithms.basic import TimeWindow\n",
    "from mspasspy.ccore.utility import ErrorSeverity\n",
    "from mspasspy.db.normalize import (normalize,\n",
    "                                   ObjectIdMatcher,\n",
    "                                   OriginTimeMatcher)\n",
    "from mspasspy.algorithms.window import WindowData\n",
    "from mspasspy.algorithms.resample import (ScipyResampler,\n",
    "                                          ScipyDecimator,\n",
    "                                          resample,\n",
    "                                         )\n",
    "from obspy.geodetics import gps2dist_azimuth,kilometers2degrees\n",
    "from obspy.taup import TauPyModel\n",
    "import time\n",
    "\n",
    "def set_PStime(d,Ptimekey=\"Ptime\",Stimekey=\"Stime\",model=None):\n",
    "    \"\"\"\n",
    "    Function to calculate P and S wave arrival time and set times \n",
    "    as the header (Metadata) fields defined by Ptimekey and Stimekey.\n",
    "    Tries to handle some complexities of the travel time calculator \n",
    "    returns when one or both P and S aren't calculatable.  That is \n",
    "    the norm in or at the edge of the core shadow.  \n",
    "    \n",
    "    :param d:  input TimeSeries datum.  Assumes datum's Metadata \n",
    "      contains stock source and channel attributes.  \n",
    "    :param Ptimekey:  key used to define the header attribute that \n",
    "      will contain the computed P time.  Default \"Ptime\".\n",
    "    :param model:  instance of obspy TauPyModel travel time engine. \n",
    "      Default is None.   That mode is slow as an new engine will be\n",
    "      constructed on each call to the function.  Normal use should \n",
    "      pass an instance for greater efficiency.  \n",
    "    \"\"\"\n",
    "    if d.live:\n",
    "        if model is None:\n",
    "            model = TauPyModel(model=\"iasp91\") \n",
    "        # extract required source attributes\n",
    "        srclat=d[\"source_lat\"]\n",
    "        srclon=d[\"source_lon\"]\n",
    "        srcz=d[\"source_depth\"]\n",
    "        srct=d[\"source_time\"] \n",
    "        # extract required channel attributes\n",
    "        stalat=d[\"channel_lat\"]\n",
    "        stalon=d[\"channel_lon\"]\n",
    "        staelev=d[\"channel_elev\"]\n",
    "        # set up and run travel time calculator\n",
    "        georesult=gps2dist_azimuth(srclat,srclon,stalat,stalon)\n",
    "        # obspy's function we just called returns distance in m in element 0 of a tuple\n",
    "        # their travel time calculator it is degrees so we need this conversion\n",
    "        dist=kilometers2degrees(georesult[0]/1000.0)\n",
    "        arrivals=model.get_travel_times(source_depth_in_km=srcz,\n",
    "                                            distance_in_degree=dist,\n",
    "                                            phase_list=['P','S'])\n",
    "        # always post this for as it is not cheap to compute\n",
    "        # WARNING:  don't use common abbrevation delta - collides with data dt\n",
    "        d['epicentral_distance']=dist\n",
    "        # these are CSS3.0 shorthands s - station e - event\n",
    "        esaz = georesult[1]\n",
    "        seaz = georesult[2]\n",
    "        # css3.0 names esax = event to source azimuth; seaz = source to event azimuth\n",
    "        d['esaz']=esaz\n",
    "        d['seaz']=seaz\n",
    "        # get_travel_times returns an empty list if a P time cannot be \n",
    "        # calculated.  We trap that condition and kill the output \n",
    "        # with an error message\n",
    "        if len(arrivals)==2:\n",
    "            Ptime=srct+arrivals[0].time\n",
    "            rayp = arrivals[0].ray_param\n",
    "            Stime=srct+arrivals[1].time\n",
    "            rayp_S = arrivals[1].ray_param\n",
    "            d.put(Ptimekey,Ptime)\n",
    "            d.put(Stimekey,Stime)\n",
    "            # These keys are not passed as arguments but could be - a choice\n",
    "            # Ray parameter is needed for free surface transformation operator\n",
    "            # note tau p calculator in obspy returns p=R sin(theta)/V_0\n",
    "            d.put(\"rayp_P\",rayp)\n",
    "            d.put(\"rayp_S\",rayp_S)\n",
    "        elif len(arrivals)==1:\n",
    "            if arrivals[0].name == 'P':\n",
    "                Ptime=srct+arrivals[0].time\n",
    "                rayp = arrivals[0].ray_param\n",
    "                d.put(Ptimekey,Ptime)\n",
    "                d.put(\"rayp_P\",rayp)\n",
    "            else:\n",
    "                # Not sure we can assume name is S\n",
    "                if arrivals[0].name == 'S':\n",
    "                    Stime=srct+arrivals[0].time\n",
    "                    rayp_S = arrivals[0].ray_param\n",
    "                    d.put(Stimekey,Stime)\n",
    "                    d.put(\"rayp_S\",rayp_S)\n",
    "                else:\n",
    "                    message = \"Unexpected single phase name returned by taup calculator\\n\"\n",
    "                    message += \"Expected phase name S but got \" + arrivals[0].name\n",
    "                    d.elog.log_error(\"set_PStime\",\n",
    "                                     message,\n",
    "                                     ErrorSeverity.Invalid)\n",
    "                    d.kill()\n",
    "                \n",
    "    # Note python indents mean if an ensemble is marked dead this function just silenetly returns \n",
    "    # what it received doing nothing - correct mspass model\n",
    "    return d\n",
    "def cut_Pwindow(d,stime=-100.0,etime=500.0):\n",
    "    \"\"\"\n",
    "    Window datum relative to P time window.  Time\n",
    "    interval extracted is Ptime+stime to Ptime+etime.\n",
    "    Uses ator,rtoa feature of MsPASS.\n",
    "    \"\"\"\n",
    "    if d.live:\n",
    "        if \"Ptime\" in d:\n",
    "            ptime = d[\"Ptime\"]\n",
    "            d.ator(ptime)\n",
    "            d = WindowData(d,stime,etime)\n",
    "            d.rtoa()\n",
    "    return d\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e085b2-fca0-473d-b345-b038d4a90d95",
   "metadata": {},
   "source": [
    "This next box does the first order processing for this analysis.   It reads raw data from miniseed files we downloaded above, does a couple of simple processing steps, and saves the data as MsPASS native format that is much faster to read back it.  \n",
    "\n",
    "This code illustrates a standard serial processing algorithm for an entire data set. The for loop iterates through all the TimeSeries data that can be constructed from the miniseed data files and handles them one at a time.  Later in the course we will see how to parallelize this loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacb3a49-8881-42a3-9bc4-f7fecaf43a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.taup import TauPyModel\n",
    "from mspasspy.algorithms.signals import filter,detrend\n",
    "from mspasspy.util.Janitor import Janitor\n",
    "\n",
    "ttmodel = TauPyModel(model=\"iasp91\")\n",
    "stime=-100.0\n",
    "etime=500.0\n",
    "\n",
    "janitor = Janitor()\n",
    "# nonstandard keys added for travel times - need to save these to keep them from being thrown out by the janitor\n",
    "for k in [\"seaz\",\"esaz\",\"Ptime\",\"epicentral_distance\",\"rayp_P\",\"rayp_S\"]:\n",
    "    janitor.add2keepers(k)\n",
    "\n",
    "t0 = time.time()\n",
    "nlive=0\n",
    "ndead=0\n",
    "cursor=db.wf_miniseed.find({})\n",
    "for doc in cursor:\n",
    "    # the normalize matchers in this read were defined in the normalize section of this \n",
    "    # notebook.  Could cause problems if this box is run out of order\n",
    "    d = db.read_data(doc,collection='wf_miniseed',normalize=[chan_matcher,source_matcher])\n",
    "    d = detrend(d,type=\"constant\")\n",
    "    d = filter(d,'lowpass',freq=2.0,zerophase=False)\n",
    "    \n",
    "    # this function will run faster if passed an instance the TauP calculator (ttmodel)\n",
    "    # If left as the default an instance is instantiated on each call to the function which is very inefficient.\n",
    "    d = set_PStime(d,model=ttmodel)\n",
    "    d = cut_Pwindow(d,stime,etime)\n",
    "    if d.live:\n",
    "        nlive += 1\n",
    "    else:\n",
    "        ndead += 1\n",
    "    janitor.clean(d)\n",
    "    db.save_data(d,storage_mode='file',dir='./wf_TimeSeries',data_tag='serial_preprocessed')\n",
    "t=time.time()    \n",
    "print(\"Total processing time=\",t-t0)\n",
    "print(\"Number of live data saved=\",nlive)\n",
    "print(\"number of data killed=\",ndead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75787f02-2aa2-4110-9de6-dc33cc3e4390",
   "metadata": {},
   "source": [
    "This final code box does one simple task:  it assembles all possible three-component sets of TimeSeries objects and \"bundles\" them into what we call Seismogram object.  We will review that concept in detail in the first class session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f96f1-561b-46ed-9ac1-0ece0801f013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.algorithms.bundle import bundle_seed_data\n",
    "from mspasspy.util.Undertaker import Undertaker\n",
    "\n",
    "stedronsky=Undertaker(db)\n",
    "t0=time.time()\n",
    "srcids=db.wf_TimeSeries.distinct('source_id')\n",
    "nsrc=len(srcids)\n",
    "print(\"This run will process \",nsrc,\n",
    "      \" common source gathers into Seismogram objects\")\n",
    "for sid in srcids:\n",
    "    query={'source_id' : sid,\n",
    "           'data_tag' : 'serial_preprocessed'}\n",
    "    nd=db.wf_TimeSeries.count_documents(query)\n",
    "    print('working on ensemble with source_id=',sid)\n",
    "    print('database contains ',nd,' documents == channels for this ensemble')\n",
    "    cursor=db.wf_TimeSeries.find(query)\n",
    "    # For this operation we only need channel metadata loaded by normalization\n",
    "    # orientation data is critical (hang and vang attributes)\n",
    "    ensemble = db.read_data(cursor,\n",
    "                            normalize=[chan_matcher],\n",
    "                           )\n",
    "    print('Number of TimeSeries objects constructed for this source=',len(ensemble.member))\n",
    "    ensemble=bundle_seed_data(ensemble)\n",
    "    # The reader would do the following handling of dead data automatically\n",
    "    # it is included here for demonstration purposes only\n",
    "    # part of the lesson is handling of dead data\n",
    "    print('Number of (3C) Seismogram object created from input ensemble=',len(ensemble.member))\n",
    "    [living,bodies]=stedronsky.bring_out_your_dead(ensemble)\n",
    "    print('number of bundled Seismogram=',len(living.member))\n",
    "    print('number of killed Seismogram=',len(bodies.member))\n",
    "    # bury the dead if necessary\n",
    "    if len(bodies.member)>0:\n",
    "        stedronsky.bury(bodies)\n",
    "    janitor.clean(ensemble)\n",
    "    db.save_data(ensemble,storage_mode='file',dir='./wf_Seismogram',collection='wf_Seismogram',data_tag='serial_preprocessed')\n",
    "\n",
    "t = time.time()\n",
    "print(\"Time to convert all data to Seismogram objects=\",t-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f9007e-d452-4565-981c-bd749ed116dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
