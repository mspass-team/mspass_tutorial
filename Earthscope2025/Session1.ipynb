{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5416b55-bf0c-45b9-b657-3ddd96fdb387",
   "metadata": {},
   "source": [
    "# Earthscope MsPASS Short Course 2025:  Session 1\n",
    "## *Prof. Gary L. Pavlis*\n",
    "## Data and Metadata\n",
    "The lecture I just presented covered two topics:\n",
    "1.  A high level introduction to MsPASS\n",
    "2.  A brief overview of the concept of \"data\" and \"metadata\" and how the ideas are encapsulated in core data objects used in MsPASS.\n",
    "\n",
    "This notebook is a hands on exercise to strengthen your understanding of item 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42169a35-45ac-487c-baa7-fe364664b760",
   "metadata": {},
   "source": [
    "## Before Running this Notebook\n",
    "### Launch the database server\n",
    "\n",
    "As noted in the PrecourseProcessing notebook, an issue we haven't been able to resolve is how to have a private instance of MongoDB launch for each instance of a geolab login.  For that reason you will need to do a shortened version of the commands you used in the PrecourseProcessing notebook.  \n",
    "\n",
    "Launch a terminal window in GeoLab and enter these commands:\n",
    "```\n",
    "cd   #  makes sure you are running this in your home directory\n",
    "mongod --dbpath ./db --logpath ./logs/mongo_log\n",
    "```\n",
    "Note a couple things that can go wrong here:\n",
    "-  The command will block.  If you want to reuse that terminal window put an & at the end of the mongod launch line.\n",
    "-  DO NOT exit the terminal window from which you run the mongod command or you will kill the database server.  \n",
    "-  Each time you reconnect to GeoLab you will need to relaunch the MonogDB server with that  incantation. Be careful you always run the command from the same directory as that way the database files will be written to ~/db and the mongodb log will appear in ~/logs.\n",
    "-  In all cases it is wise to launch a second Terminal window and erify that worked by typing `ps -A`  You should see a line where the CMD field is \"mongod\".   If not, contact me by email or slack if you are unable to solve the problem.  The first place to look if you are having a problem is the content of the file ~/logs/mongo_log that will be generated when this command runs.\n",
    "\n",
    "### Use outside GeoLab\n",
    "If you are accessing this notebook from github and are not part of the 2025 short course you can still run this tutorial notebook on a desktop system. To do so you will need to do two things:\n",
    "1.  Install the `mspass-desktop` GUI described in __[MsPASS User's Manual here](https://www.mspass.org/getting_started/mspass_desktop.html)__\n",
    "2.  Launch MsPASS as described on that page and run this notebook using the \"run\" button on the GUI. Alternatively, you may run it interactively by pushing the \"jupyter\" button."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a37823-c7e9-429a-a5eb-9e711efc535c",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "Later in this session we will dig deeper into the database system used in MsPASS called MongoDB.   When you entered the GeoLab gateway, the gateway launched an instance of the \"MongoDB server\".   MongoDB, like most modern dbms systems, uses a client-server model.   The server is the gatekeeper and all interactions with the database happen through that program.  It might also be helpful to think of the MongoDB server as providing a \"service\" - a modern IT buzz word.   In this case, the \"service\" is pulling and pushing data to from and to storage managed by the database server.   We will be interacting in this session that server through a \"client\".  There a special object in MsPASS that is an subclass of the MongoDB client.   We can create an instance to connect to the database you created earlier with the following incantation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0e7e31-1d1b-4c32-9f79-996f244069b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.client import Client\n",
    "dbclient=Client().get_database_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84c371e-c1b9-4947-941a-92d6f4141ab4",
   "metadata": {},
   "source": [
    "The symbol *dbclient* now contains a running instance of the \"client\" that is set up for interaction with the MongoDB \"server\".   The client itself is useful only as a mid-level manager that handles data flow with the server.  What we need to actually do something useful is create an instance of a handle to a particular database managed by the database server.   This incantation creates an instance of a MsPASS `Database` object, which we will assign the symbol *db*, that we will use for interactions with the database you created earlier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ed7178-8836-4c1d-ac5a-0ac036d7635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = dbclient.get_database(\"Earthscope2025\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98c9f9d-8f42-4e7b-b7aa-50ba10fb9dc1",
   "metadata": {},
   "source": [
    "We will explore the API of the object stored in *db* later, but for now think of it as simply a handle to interact with the entire data set you prepared by running the workflow prior to the classs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97370c84-4c99-4615-a543-3c6af8b125a7",
   "metadata": {},
   "source": [
    "### TimeSeries Objects\n",
    "The workflow you ran before the start of this class created a complete, working data set of over 26,000 \"TimeSeries objects\".   A `TimeSeries` in MsPASS is an abstraction of the single channel of data in a particular time interval.   Let's just load up the first one in the data set and play with it a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a5eaf3-e97a-4d0e-81ae-4cc17dd3f631",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = db.wf_TimeSeries.find_one()\n",
    "d = db.read_data(doc)\n",
    "type(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbbb7a1-5f70-47d3-9f97-0e33664d0b64",
   "metadata": {},
   "source": [
    "First, let's do a quick visualization of the content of that object with matplotlib, which I suspect many of you are are already familiar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166fc4a2-72f0-4a70-9383-3f5c583144a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(d.time_axis(),d.data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7861a0-c0b1-4f7f-bdc9-48faa87f4711",
   "metadata": {},
   "source": [
    "Two details about code segment that are informative:\n",
    "1.  The `d.data` symbol is a reference to the `data` attribute of the (data) object `d`.  The visualization show the content of `data` is a vector of numbers that define this particular channel of data recording a particular teleseismic earthquake.  In MsPASS \"the data\" means mostly the sample data.\n",
    "2.  For the plot we used a \"method\" of the data object, `d`, called `time_axis`.   It is a convenience method of matplotlib plots that creates a vector of the time of each sample. Note that time is displayed as the x axis rounded to the nearest second.  The time standard by default in MsPASS is \"unix epoch time\", which is the number of seconds elapsed since the beginning of 1970.  The issue of time in MsPASS is an important issue, but best put aside for now. __[See this section of the MsPASS User's Manual ](https://www.mspass.org/user_manual/time_standard_constraints.html)__ for details.\n",
    "\n",
    "A `TimeSeries` object has a lot of complexity to provide the functionality needed for MsPASS, but the following code blocks shows what auxiliary data (normally called \"attributes\") are essential to define a particular waveform that is \"data\" in MsPASS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833a8d72-6034-4efa-b4ce-49e23c37c044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy import UTCDateTime\n",
    "print(\"Number of points in the data vector is d.npts=\",d.npts)\n",
    "print(\"The sample interval is d.dt=\",d.dt)\n",
    "print(\"The data start time in epoch seconds is d.t0=\",d.t0)\n",
    "print(\"The UTC time of d.t0=\",UTCDateTime(d.t0))\n",
    "print(\"The special variable d.tref={} defines the time standard as UTC\".format(d.tref))\n",
    "print(\"It can also be set to what we call 'relative time' for active source data where t0 is normally shot time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c6273-07c2-4ed4-ad7d-035fcef5a271",
   "metadata": {},
   "source": [
    "Everything else in a `TimeSeries` object is defined as \"Metadata\", which we will discuss at length below. \n",
    "\n",
    "### Seismogram Objects\n",
    "A `Seismogram` object encapsulates the core concept of seismology of a three-component seismogram.  The concepts a `Seismogram` shares the following concepts with a `TimeSeries`:\n",
    "1.  Uniform sample interval - stored with the same attribute name of *dt*.\n",
    "2.  Start time is normally specified as a UTC epoch time and stored as the attribute *t0*.\n",
    "3.  The number of samples is fixed for an instance of the object and is defined with the attribute *npts*.\n",
    "\n",
    "The differences in concept are:\n",
    "1.  The samples are three-component vectors defining amplitudes along three internally defined basis vectors.\n",
    "2.  The object needs to store a transformation matrix to define the basis vectors each component scales.\n",
    "\n",
    "The later is admittedly abstract, but necessary to define the generic concept.  The abstraction is needed because some data is \"cardinal\", meaning orieted so that the components are x1= +East, x2= + North, and x3 = +Up, but during processing that is not necessarily true.  e.g. in the receiver function processing we will do in session 2 we will apply Kennett's free surface tranformation operator that transforms the data into a non-orthonal system of approximate P, SV, and SH components.  \n",
    "\n",
    "To make that clearer let's look at one of these and inspect some of the data attributes like we did with a `TimeSeries` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14db4a53-3a2f-4ec7-ab9c-d467a8eab01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need this MsPASS module to make plotting a Seismogram easier\n",
    "from mspasspy.graphics import SeismicPlotter\n",
    "plotter = SeismicPlotter(normalize=True)   # instantiates an instance of a SeismicPlotter object\n",
    "doc = db.wf_Seismogram.find_one()\n",
    "d = db.read_data(doc,collection=\"wf_Seismogram\")\n",
    "plotter.plot(d)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f30260-7400-439a-b1c2-2fca7e0aab75",
   "metadata": {},
   "source": [
    "This set of print statements are mostly copied from the comparable box above for `TimeSeries` with a couple changes for differences in \"concept\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d034c46-f76a-4c81-9ae9-9c073d364496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from above - attribute names are the same \n",
    "print(\"Number of points in the data vector is d.npts=\",d.npts)\n",
    "print(\"The sample interval is d.dt=\",d.dt)\n",
    "print(\"The data start time in epoch seconds is d.t0=\",d.t0)\n",
    "print(\"The UTC time of d.t0=\",UTCDateTime(d.t0))\n",
    "print(\"The special variable d.tref={} defines the time standard as UTC\".format(d.tref))\n",
    "# these is the key difference for a Seismogram\n",
    "print(\"The type of d.data=\",type(d.data))\n",
    "print(\"The tranformation matrix for this object is d.tmatrix=\",d.tmatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb61970c-9d5a-484f-bfe3-345526a6b530",
   "metadata": {},
   "source": [
    "A few things to note:\n",
    "1.  The *data* attribute is now a matrix instead of a vector.   An \"implementation detail\" is that the thing storing the data is a lightweight C++ class called a \"dmatrix\" and the \"shape\" of the matrix is 3Xnpts.\n",
    "2.  The transformation matrix for this datum is NOT cardinal.   The reason is this is TA data where the sensor orientation was measured to high precision and the processing you did to create this thing defined this matrix correctly to reflect a small rotation of the horizontal components.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf80e85-4566-4a8a-af9e-d0bdb4cd60d3",
   "metadata": {},
   "source": [
    "### Ensembles\n",
    "A final important core data type of MsPASS is the concept of what we call an *Ensemble*.   A synonymn for ensemble that some students may recognize from seismic reflection processing is a *gather*.   That is, the key concept is that an *Enemble/gather* is a set of objects that have some generic relationship.   e.g. in seismic reflection processing common jargon terms are:  \"shot gather\" for data one active source, \"CMP gather\" for data assembled in midpoint coordinates, \"common receiver gather\" for a collection of data from a common receiver position.  The point is an ensemble/gather is a subset of a larger data set defined by some subset selection.  If you are familiar with database concepts, \"subset\" is a key function of any database.  Hence, a key thing to realize is that in MsPASS creating an ensemble usually involves a database query to define the subset to be bundled together into and ensemble object.   The following code block illustrates a type example for event processing (appropriate of this tutorial since the data are looking at are teleseismic events):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2ff491-44d6-488e-879b-e5b9126321e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the first source record for this demo - not a generic alorithm\n",
    "doc = db.source.find_one()\n",
    "# extract the source_id that is a unique key we can use for a query\n",
    "srcid = doc[\"_id\"]\n",
    "query = {\"source_id\" : srcid}\n",
    "n=db.wf_TimeSeries.count_documents(query)\n",
    "print(\"The number of TimeSeries data in this data set with source_id=\",srcid,\" is \", n)\n",
    "cursor = db.wf_TimeSeries.find(query).limit(10)     # limit to first 10 to make plot legible in the notebook\n",
    "e = db.read_data(cursor,collection=\"wf_TimeSeries\")\n",
    "# this sets the plot title\n",
    "plotter.title=\"TimeSeriesEnsemble first 10 members\"\n",
    "plotter.plot(e)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37612a1-4b26-4e33-bcad-6fde590388a3",
   "metadata": {},
   "source": [
    "We will look at some of the database constructs used in that code block later, but for now the key thing to recognize is that ensemble is just a collection of several \"atomic objects (in MsPASS that means `TimeSeries` or `Seismogram` objects).   A few key details are best seen with some print statements comparable to what we did for atomic data above: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3164e584-ef82-4c02-a49e-9a06cdbf5ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The type of the ensemble we created is \",type(e))\n",
    "print(\"The atomic components are stored in the member attribute which for this case has type=\",type(e.member))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64bd8b3-7581-4047-b030-c10a3662499e",
   "metadata": {},
   "source": [
    "To get the size of an ensemble used the python len function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eae61e-8104-43fa-ace6-11e572f4a91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The size of this ensemble=\",len(e.member))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107b9b55-327d-45c4-b6b8-4eaaaf8419c5",
   "metadata": {},
   "source": [
    "The member attribute is \"iterable\" and can be subscripted.  e.g. here we extract and plot member 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5927875-489b-497b-adb9-40986bed714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = e.member[3]\n",
    "print(\"The type of d=\",type(d))\n",
    "plt.plot(d.time_axis(),d.data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6248b33-6a33-44e0-8239-8ed1e4c1fc2e",
   "metadata": {},
   "source": [
    "Note a `SeismogramEnemble` has exactly the same attribute names as a `TimeSeriesEnsemble` with only one significant difference:  the member attribute is an iterable container with `Seismogram` objects inside instead of `TimeSeries` objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b9692f-ee08-4513-ac32-520d7a08e3be",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "All seismic data objects in MsPASS (i.e. the four things just covered above) in OOP jargon are \"subclasses of Metadata\".   That means that each data object \"is a Metadata container\" as well as a seismic data container.  From the precourse work I can assume you all are familiar with python dictionaries.  The MsPASS `Metadata` container is mostly but not exactly synonmyous with a python dictionary.   The reasons are technical and irrelevant.  The key thing to realize is all seismic data objects act like python dictionaries.  This small code block that reloads the `TimeSeries` object we started with illustrates the key ideas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf91938-b05b-4557-a255-aaf7c43bd522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.util.seismic import print_metadata\n",
    "doc = db.wf_TimeSeries.find_one()\n",
    "print(\"////////////////////////////////////////////////////////////////\")\n",
    "print(\"This is the python dictionary that will be used to construct d\")\n",
    "print(\"////////////////////////////////////////////////////////////////\")\n",
    "print_metadata(doc)\n",
    "d = db.read_data(doc)\n",
    "print(\"////////////////////////////////////////////////////////////////\")\n",
    "print(\"This is the Metadata container content of d after it was constructed\")\n",
    "print(\"////////////////////////////////////////////////////////////////\")\n",
    "print_metadata(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3c0cd8-1c8d-4845-b894-bb51814ae862",
   "metadata": {},
   "source": [
    "The key point is except for the order the output of the json display of the python dictionary (doc) and the content of the Metdata container of d are identical.\n",
    "\n",
    "This illustrates a few features of Metadata that are typical python dictionary constructs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd52d202-00d0-43e2-9dac-b30cc9b0d911",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Station code=\",d[\"sta\"])\n",
    "print(\"The number of attributes stored in the Metadata contaienr of d=\",len(d))\n",
    "for k in d.keys():\n",
    "    print(\"the value associated with key=\",k,\" is of type \",type(d[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3707b436-eb7b-438d-9b02-06fc5a719ff1",
   "metadata": {},
   "source": [
    "### Break and First Assignment\n",
    "At this point we will take a short break.   If you have time and care to do so you may start on the first quiz for this session in Moodle on the topics I covered above.  That quiz is also homework so you may want to wait until later to complete it.   \n",
    "\n",
    "When we reconvene there will be a short lecture on MongoDB that builds on the precourse lecture on the general database concepts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5320f364-18a1-4c24-baec-9190bd73ffed",
   "metadata": {},
   "source": [
    "## MongoDB Fundamentals\n",
    "*Learning goals of this section*:\n",
    "\n",
    "- Students will learn the following generic concepts of MongoDB\n",
    "  * What a document database is\n",
    "  * Create Read Update and Delete (CRUD) database concepts\n",
    "  * The pymongo query language \n",
    "  * Why an index is essential for performance in a document database\n",
    "- Students will learn the following concepts about how MsPASS utilizes MongoDB\n",
    "  * Waveform collections and contents\n",
    "  * The MsPASS abstraction of data object construction from a MongoDB document\n",
    "    - File-based constructors\n",
    "    - URL-based constructors (important for session 3)\n",
    "    - Handling formats\n",
    "  * The “source” collection for managing event data\n",
    "  * The “channel” and “site” collections for managing receiver data\n",
    "  * Normalization – what it is, why it is necessary, how to define the operations, and how to use it in waveform processing\n",
    "\n",
    "### The Database Object\n",
    "We have been using an instance of the MsPASS database handle.  In this notebook the working instance has the symbolic name *db*.  Consider the following output noting it only partially captures the complexity of *db*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29bb440-40f0-4292-827e-cb5a9c00fc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(db))\n",
    "dir(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa328409-5d14-4351-aec3-70c40587ac65",
   "metadata": {},
   "source": [
    "The main lesson you should take from the above is that an instance of the class type `mspasspy.db.database.Database` is complex because it bundles together a lot of functionality.  That functionality, however, is central to using MsPASS because a handle like db is central to data management with the package.   \n",
    "\n",
    "### Collections and Documents\n",
    "As I noted in the lecture MongoDB is a \"document\" database and it groups \"documents\" into collections.   For students familiar with relational databases I reiterate here that a \"collection\" in MongoDB plays the same role as a \"relation\" (table) in a relational db and a \"document\" plays the same role as a \"tuple\" (row of a table).  \n",
    "\n",
    "This code box will show us the collections that exist in the database we have been working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf30478-5bde-40fe-9aad-7c7afe4cc965",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = db.list_collections()\n",
    "for doc in cursor:\n",
    "    print(doc['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dace54-9aa5-49d9-aa93-3bbbd658e22a",
   "metadata": {},
   "source": [
    "The key thing to understand about collections at this point is they are used to organize information that has a common structure.   Each collection is made up of one or more \"documents\" that contain data with a (mostly) common structure.   We saw examples of \"documents stored in wf_TimeSeries earlier.   Here, for example, is a similar box showing the first document in the wf_TimeSeries collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eb807e-a551-44e8-af0f-a7a97b6bcde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=db.wf_TimeSeries.find_one()\n",
    "print(\"The type of the thing returned by find_one  (symbol doc) = \",type(doc))\n",
    "print(\"Content of this document\")\n",
    "print_metadata(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd1222c-caff-4139-a64a-0e3c857a8f80",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "- a \"document\" is translated by pymongo (the package we are using to communicate with the MongoDB server) into a python dictionary\n",
    "- A document is defined by key-value pairs.  In MsPSS the keys are assumed to have a common meaning inside that collection and the values for each key are assumed to be the same same type.  Note that is not something MongoDB enforces but should normally be done to avoid mysterious errors when handling millions of documents.\n",
    "- The print_metadata function we used translates the dictionary into \"json-format\" text output\n",
    "\n",
    "What is not shown by that example is that unlike a table, key-value pairs can be missing from a document and it makes no difference to the MongoDB server.   \n",
    "\n",
    "Finally, on this point here is a document from a completely different collection we call \"site* that is used largely to hold station location information.  I show two documents to illustrate the common set of keys that make the collection useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f3252-7c6a-419b-b501-dbdf3fd87c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor=db.site.find({}).limit(2)\n",
    "for doc in cursor:\n",
    "    print_metadata(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c285fe1-6e5c-4b0e-883f-04da5547438c",
   "metadata": {},
   "source": [
    "### CRUD\n",
    "A near universal mnemonic found in books on database theory and online tutorials is the acronymn CRUD.  CRUD is short for Create-Read-Update-Delete.  It is used to as a mnenomic to remember those four primry functions any operational database must be capable of doing.   In this class we will focus mainly on  the C (Create==writers) and R (readers).   Reading and writing are pretty much essential for all MsPASS workflows.  Updates and deletes, in contrast, are rarely needed and, in fact, are usually ill advised and best done not within a larger workflow but as a sidebar to fix some problem.  A notable exception is [normalization](https://www.mspass.org/user_manual/normalization.html) we will discuss later.  In any case, to reduce information overload this class will focus on read and write operations.  When you use MsPASS if you understand the syntax for the C and R functions of MongoDB the forms for the U and D functions are completely logical.   You can also consult the sections in the [User Manual on CRUD operations](http://www.mspass.org/user_manual/CRUD_operations.html) and the section titled [\"Using MongoDB with MsPASS\"](http://www.mspass.org/user_manual/mongodb_and_mspass.html).\n",
    "\n",
    "#### Create\n",
    "The first letter in the CRUD acronynm is \"Create\".  For all applications some form of \"create\" is an essential first step to put some kind of data into your database. The main purpose of the workflow you ran before the start of this class was to drive a string of create operations.   Let's collectively look at the notebook \"PrecourseProcessing.ipynb\".   Double click that notebook with the file browser in jupyter lab.  We will look at the context there of the following sequence of \"Create\" steps in that notebook (you can search the highlighted keyword if you have trouble finding any of these):\n",
    "\n",
    "1.  The *save_catalog* is a method of the MsPASS extension of `Database`.  It takes apart an obspy [Catalog](https://docs.obspy.org/packages/autogen/obspy.core.event.Catalog.html) object, converts it into a python dictionary, and saves one document in the \"source\" collection (see list above) for each event stored in the \"Catalog\".  That \"Catalog\" object is created by running an obspy web service method called [get_events](https://docs.obspy.org/packages/autogen/obspy.clients.fdsn.client.Client.get_events.html).  \n",
    "2.  The *index_miniseed_file\" method is another MsPASS extension.  It reads through a miniseed file to build an index of the file.   We'll look at what that index looks like momentarily when we consider the \"R\" of CRUD below.\n",
    "3.  The *save_inventory* method is another extension.  It performs a function similar to *save_catalog* in that it builds on an obspy capability.  That is, the workflow uses the obspy web services function, [get_stations](https://docs.obspy.org/packages/autogen/obspy.clients.fdsn.client.Client.get_stations.html), to fetch requested station Metadata from FDSN data centers.  The retrieval stores that data in memory as an obspy [inventory](https://docs.obspy.org/packages/autogen/obspy.core.inventory.inventory.Inventory.html) object.  As the name *save_inventory* implies it takes apart the Inventory object and reorganizes the data into two MongoDB collections called \"channel\" and \"site\". Those two are the MsPASS collections used to store station Metadata.\n",
    "4.  Box 12 does some low-level processing.  It does an R of CRUD operation using the *read_data* method we used above, applies several processing functions (we will discuss them later), and then calls the *save_data* method of the MsPASS `Database` object (*db* symbol).  That saves processed `TimeSeries` objects (the content of the data stored with the symbol *d*) as files with the Metadata content of each datum stored in a collection called *wf_TimeSeries*.  That box is a typical serial processing loop in MsPASS over \"atomic\" data.  \n",
    "5.  The algorithm in Box 13 takes the content of the entire *wf_TimeSeries* collection and converts all possible combinations to `Seismogram` objects.   It does that by running the [bundle_seed_data](https://www.mspass.org/python_api/mspasspy.algorithms.html#module-mspasspy.algorithms.bundle) function.   The algorithm is another serial loop processing sequence but differs from the previous by looping over ensemble objects instead of atomic data.   The loop again terminates with a call to the `Database.save_data` method, but in this case `save_data` is handling `SeismogramEnsemble` objects instead of `TimeSeries` objects.\n",
    "\n",
    "A summary of what the above produced is the report produced by this small code segment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2cb840-2266-4b49-a05e-5042e92b74e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for collection in [\"source\",\"wf_miniseed\",\"channel\",\"site\",\"wf_TimeSeries\",\"wf_Seismogram\"]:\n",
    "    n = db[collection].count_documents({})\n",
    "    print(\"The {} collection contains {} documents\".format(collection,n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c329f5-fdaf-4f14-b9ed-2ee10627ce1c",
   "metadata": {},
   "source": [
    "With those counts and the processing that workflow did you should not be surprised that notebook took several minutes to run.  It is also important to note that ALL of these C operations used in that workflow use MsPASS extensions.   Online tutorials you may read on MongoDB will not discuss any of those for C operation, but instead will show examples for the low level `insert_one` and `insert_many` described __[here in the pymongo documentation](https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html)__.  Those two methods are used to insert plain documents (i.e. data stored in a python dictionary) into a MongoDB collection.   If you examine the python code of the MsPASS extensions you will see that all of them have calls within them to one of the pymongo insert methods.  You should think of the MsPASS extensions as way to automate saving MsPASS data objects. The readers we discuss below do the inverse constructing an object from a MongoDB document.  \n",
    "\n",
    "There is an important detail in running the `save_data` method we need to emphasize before moving to \"R\" of CRUD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137a7fa3-8584-45be-b057-5a3c0ca075a4",
   "metadata": {},
   "source": [
    "##### Storage Mode Options\n",
    "The [save_data](http://www.mspass.org/python_api/mspasspy.db.html#mspasspy.db.database.Database.save_data) method of `Database` has an argument called \"storage_mode\".   It determines how the sample data are saved.   The options are string keywords that must be one of the following:\n",
    "1.  \"gridfs\" (the default) saves the sample data internally in the MongoDB data area.  In this mode the sample data are stored in file-like objects managed by MongoDB and with MongoDB generated documents needed to define them in the two collections called \"fs.files\" and \"fs.chunks\".  Gridfs is convenient storage because it is easier to manage as an integrated and bombproof storage area managed completely by MongoDB.   The dark side is we know from experience writing data to gridfs can cause an io bottleneck since the more voluminous sample data have to pass through the same io channel as the Metadata write operations (the actual calls to `insert` to \"wf_TimeSeries` or `wf_Seismogram`.  Furthermore, it puts the voluminoous sample data on the same file system where the database is stored.  That is universally a bad idea for large data sets.\n",
    "2.  \"files\", as the name suggests, writes data to conventional computer files.   When using the option `storage_mode=\"file\"` in a call to `save_data` by definition the writer has to know what file it should open and use to save the requested data.  The best way to do that is to set an explicit value for `dir` and/or `dfile` in the line where you call `save_data`.  If the arguments are not defined, `save_data` attempts to extract values from each atomic datum's Metadata container using the same keywords. (i.e. attempts to retrieve two string values with the keys \"dir\" and \"dfile\".  If that fails, it falls to the last resort;  \"dir\" will be set as to the run directory and \"dfile\" will be defined by a unique, random string. In all cases the file name is then generated by using the stock python `join` method of the `os.path`.   That is, the file name is generated as `fname=os.path.join(dir,dfile)`.  `save_data` then attempts to open the file.  If successful it seeks to the end of the file, posts the byte offset as the \"foff\" attribute, and writes the sample data.   The default is a raw binary dump, but as described in the User Manual and docstring for [save_data](http://www.mspass.org/python_api/mspasspy.db.html#mspasspy.db.database.Database.save_data) the output can be written in any format supported by obspy's writer (subject to major issues of rigid namespace requirements for many formats.)  All should recognize that using files requires some thought beforehand about how the files should be named and organized.   The model used is heavily project dependent and outside the scope of this course.\n",
    "\n",
    "There is one more detail about writing data that is an important performance issue.  That is, when working with atomic data MsPASS will always open a file, write data, and then close the file.  All three operations take nontrivial amounts of time to complete.  The excessive open-close commands are intrinsic bottlenecks on any system.   The fastest write model is possible with ensembles written in the default binary mode as done here in box 13.   In that mode, a file is opened only once for each ensemble and the data are dumped sequentially to the same file using a the low-level C fwrite function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247f578a-4416-43e4-9951-8a5de797ae30",
   "metadata": {},
   "source": [
    "#### Read\n",
    "The single biggest advantage of using a database for managing a large seismic data set is the query capability that are central to the R of CRUD.   If all seismic data used the same data that was always in the same order a database would not be necessary.  There is, however, a wide variation of what a given research project needs for input data and how that data needs to be grouped and processed.  Hence, selection of data and grouping them into logical chunks is always required.  Databases are a scalable way to do that and MsPASS is designed to perform those operations as simply as possible. \n",
    "\n",
    "You should first realize that there are two fundamentally different types of read operations you will need to use to process your data with MsPASS:\n",
    "1.  All waveform processing should be driven by documents retrieved from MongoDB.  In MsPASS the model is that there is a one-to-one mapping between documents stored in one of the \"wf\" collections (i.e. the \"wf_miniseed\", \"wf_TimeSeries\", or \"wf_Seismogram\" collections we discussed above).  Unless something goes wrong (always possible - something called an \"abortion\" during read) your workflow should assume each document can be used to construct one atomic data object (i.e. `TimeSeries` or `Seismogram`). The standard tool to do that is the __[Database.read_data](https://www.mspass.org/python_api/mspasspy.db.html#mspasspy.db.database.Database.read_data)__ method we used several times above and just discussed with the precourse workflow disucssion.\n",
    "2.  The core pymongo read methods are two \"collection\" methods called:  `find` and `find_one` __[discussed here](https://www.mongodb.com/docs/languages/python/pymongo-driver/current/crud/query/find/)__ and in numerous online sources.\n",
    "\n",
    "It is important to understand that the MsPASS `read_data` method is a higher level operator that expects to use the output of a `find` or `find_one` method as it's input.   In particular, if you refer back to examples earlier you will see:\n",
    "- The pymongo `find_one` method returns a document/dictionary.   When `read_data` detects a document as arg0 it assumes that document is a template to create an atomic seismic data object.  It then calls the constructor for that object and creates one.  Here is an example that is similar to one we did earlier but using wf_miniseed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f875da23-6707-4f42-b3d2-6eca35d36b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = db.wf_miniseed.find_one()\n",
    "d = db.read_data(doc,collection=\"wf_miniseed\")\n",
    "print(\"The data type of d is \",type(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276665cf-6381-47d0-b851-8adbe66aa72c",
   "metadata": {},
   "source": [
    "- The pymongo `find` operation returns a special concept called a \"command cursor\".  When `read_data detects a command cursor is assumes it has the output of a find operatoin applied to a wf collection.  It will then attempt to construct an ensemble object from that \"cursor\".  Here is a variant of such a construct of one we used earlier that illustrates what I meand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9580f356-92f2-4273-9213-960a933bd9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = db.source.find_one()\n",
    "# extract the source_id that is a unique key we can use for a query\n",
    "srcid = doc[\"_id\"]\n",
    "query = {\"source_id\" : srcid}\n",
    "n=db.wf_Seismogram.count_documents(query)\n",
    "print(\"The number of Seismogram data in this data set with source_id=\",srcid,\" is \", n)\n",
    "cursor = db.wf_Seismogram.find(query)\n",
    "e = db.read_data(cursor,collection=\"wf_Seismogram\")\n",
    "print(\"The data type of e returned by read_data is \",type(e))\n",
    "print(\"The number of members in e=\",len(e.member))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e2938f-76a4-4821-af29-635324b88906",
   "metadata": {},
   "source": [
    "Every database system I know of implements some version of the concept encapsulated by the pymongo class called a [CommandCursor](https://pymongo.readthedocs.io/en/stable/api/pymongo/command_cursor.html).   A \"cursor\" is a standard return from any query like operation in any database system.  A MongoDB `CommandCursor` is technically a __[forward iterator](https://www.boost.org/sgi/stl/ForwardIterator.html)__.   That means it acts like a list that can only be traversed \"forward\".  In python that means it is done as a `for x in y:`  construct.  For instance, the cursor in the above could be tranversed with this construct:\n",
    "```\n",
    "   for doc in cursor:\n",
    "      d = db.read_data(doc,collection=\"wf_Seismogram\")\n",
    "```\n",
    "to handle the ensemble members at the atomic level rather than grouping them into an ensmeble. \n",
    "\n",
    "A command cursor, however, is not at all the same thing as a python list. It just acts like one.  It is a handle that interacts with the database to sequentially return documents.  It is fundamentall different, however, in two ways:\n",
    "1. The content of what it references often do not fit in memory.  It should be thought of as like a file handle being read sequentially. The client-server pair manage the grungy work of trying to keep the memory buffer full and assuring the client does no have to wait for data to arrive.  The important thing that means is that when reading a very large amount of data (e.g. processing millions of TimeSeries objects driven by wf_miniseed records) sequential reads with a cursor almost never have to wait for data. That is in contrast to atomic queries where a there is a large lag between the time a client issues a query and the server can response with a single document.\n",
    "2. A cursor is not subscriptable.  That is a consequence of the definition as  forward iterator.  It can only be traversed sequentially like reading data sequentially from a file.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a4abca-ccf3-4b09-b67d-c8e36a280ee0",
   "metadata": {},
   "source": [
    "#### Mongo Query Language (MQL)\n",
    "The standard language for interaction with relational database system is a language called \"Structured Query Language\" (SQL).   MongoDB uses a completely different language.  MongoDB suffers, in my opinion, from a lack of useful online resources that provide any kind of overview.  MongoDB is utilized enough that there are textbooks on the package.  If the resources I point you to here are not sufficient you might consult your local campus library for books on MongoDB as a learning aid.   In previous versions of this class I spent a lot of time on MQL.  However, MQL is like a language best learned by being immersed in it.   Hence, in this version of this class I will only highlight a few things that are needed to get you started. As you use MsPASS you will quickly learn the language as it is actually simpler than SQL. \n",
    "\n",
    "A good referencde for this section is the MsPASS User Manual section titled [\"Using MongoDB with MsPASS\"](http://www.mspass.org/user_manual/mongodb_and_mspass.html).  A starting point is synax rules defining MQL.  The manual page above has this definition for MQL in pymongo:\n",
    "\n",
    "1.  All queries use a python dictionary to contain the instructions.\n",
    "2.  The key of a dictionary used for a query normally refers to an attribute\n",
    "    in documents of the collection being queried.  There is an exception to this rule\n",
    "    for the logical OR and logical AND operators (discussed below).\n",
    "3.  The \"value\" of each key-value pair is normally itself a python\n",
    "    dictionary.   The contents of the dictionary define a simple\n",
    "    language (Mongo Query Language) that resolves True for a match\n",
    "    and False if there is no match.  The key point is the overall\n",
    "    expression the query dictionary has to resolve to a boolean condition.\n",
    "4.  The keys of the dict containers that are on the value side of\n",
    "    a query dict are normally operators.  Operators are defined with\n",
    "    strings that begin with the \"\\$\" symbol.\n",
    "5.  Simple queries are a single key-value pair with the value either\n",
    "    a constant or a dictionary with a single operator key.  e.g.\n",
    "    to a test for the \"sta\" attribute being the constant \"AAK\" the\n",
    "    query could be either `{\"sta\" : \"AAK\"}` or `{\"sta\" : {\"$eq\" : \"AAK\"}}`.\n",
    "    The form with constant value only works for \"$eq\".\n",
    "6.  Compound queries (e.g. time interval expressions) have a value\n",
    "    with multiple operator keys.\n",
    "7.  There is an implied logical AND operation\n",
    "    between multiple key operations.  An OR must be specified differently\n",
    "    (see below).\n",
    "\n",
    "That set of rule is a case in point of why we are only going to do some simple example here.   Those are like foreign language grammar rules. Use them for generic guidance and learn the details by apply examples in sources give here and online. \n",
    "\n",
    "For reference the examples below are all derived from __[this notebook of the 2024 version of this course](https://github.com/mspass-team/mspass_tutorial/blob/master/Earthscope2024/Session2.ipynb)__.   You can use that as a reference to understand each of these examples.  There are more detailed explanations of each example there.    \n",
    "\n",
    "First, a unique match query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28062587-1e57-4d34-8432-f9fa3123cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all documents in site for station 035A\n",
    "query = {\"sta\" : \"034A\"}\n",
    "cursor = db.site.find(query)\n",
    "for doc in cursor:\n",
    "    print_metadata(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c3a283-96e9-4492-83a4-4853c035b453",
   "metadata": {},
   "source": [
    "This example uses a range query in combination with what MongoDB calls a \"projection\".  A projection is equivalent to \"SELECT\" in SQL. It also illustrates use of pandas DataFrame objects that are useful containers or some types of database operations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e49956c-1838-4d35-8d6f-0b6e4515c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query={\n",
    "    'lat' : {'$gte' : 30.0,'$lte' : 35.0},\n",
    "    'lon' : {'$gte' : -110.0, '$lte' : -100},\n",
    "}\n",
    "projection={\n",
    "   'net':1,\n",
    "    'sta':1,\n",
    "    'chan':1,\n",
    "    'lat':1,\n",
    "    'lon':1,\n",
    "    'elev':1,\n",
    "    '_id':0, \n",
    "}\n",
    "\n",
    "cursor=db.site.find(query,projection)\n",
    "doclist=[]\n",
    "for doc in cursor:\n",
    "    doclist.append(doc)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(doclist)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c05aa59-e007-44bd-b45b-99570f5b093e",
   "metadata": {},
   "source": [
    "Finally, an example that combines sorting with find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b12fa28-6b8e-4c5d-86c7-6202e04005ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = {\n",
    "    \"_id\":0,\n",
    "    \"net\":1,\n",
    "    \"sta\":1,\n",
    "    \"chan\":1,\n",
    "    \"starttime\":1,\n",
    "    \"endtime\":1,\n",
    "}\n",
    "sort_clause = [\n",
    "    (\"net\",1),\n",
    "    (\"sta\",1),\n",
    "    (\"chan\",1),\n",
    "    (\"starttime\",1),\n",
    "  ]\n",
    "# obscure syntax to find all documents with station name starting with Z\n",
    "query={\n",
    "    'sta' : {'$regex' : '^Z.*'},\n",
    "}\n",
    "cursor=db.channel.find(query,projection).sort(sort_clause)\n",
    "doclist=[]\n",
    "for doc in cursor:\n",
    "    doclist.append(doc)\n",
    "df = pd.DataFrame.from_dict(doclist)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e081c2b-d3df-4a92-ae19-1ae6425cd9e4",
   "metadata": {},
   "source": [
    "#### Indexing\n",
    "A find/subset/select (different words for the same concept used in different dbms software) operation in any database is very very slow if the selection criteria is some arbitrary attribute stored in each document.   The reason is that without an index a find operation has to do a linear search of the entire collection to fufill the request.   An index on one or more keys produces a fast index to speed up that process.   \n",
    "\n",
    "To demonstrate how much of a difference we query the *channel* collection for a station \"034A\" with an exact match query similar to above but this time we time how it takes to load the data a command cursor references:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb21f56-01ca-4185-a85a-22097554df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "query = {\"sta\" : \"034A\"}\n",
    "t0 = time.time()\n",
    "cursor = db.channel.find(query)\n",
    "doclist=[]\n",
    "for doc in cursor:\n",
    "    doclist.append(doc)\n",
    "t=time.time()\n",
    "print(\"Elapsed time to read \",len(doclist),\" documents=\",t-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0f2a98-6a7a-4c03-be56-3c148dcc883b",
   "metadata": {},
   "source": [
    "Now let's build a common index for channel for the seed channel codes which in MsPASS are called *net, sta*, and *chan*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94279422-ad74-484f-a784-ed9c55dffd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import ASCENDING\n",
    "index_response = db.channel.create_index([\n",
    "    (\"net\", ASCENDING),\n",
    "    (\"sta\", ASCENDING),\n",
    "    (\"chan\",ASCENDING),  \n",
    "])\n",
    "print(index_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9b0b3a-6ca2-44e8-b1a5-5ca2e9bde72b",
   "metadata": {},
   "source": [
    "This is a duplicate of the box above, but channel now has an index that refernces sta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c129398-2f46-478a-a541-881728aaba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "query = {\"sta\" : \"034A\"}\n",
    "t0 = time.time()\n",
    "cursor = db.channel.find(query)\n",
    "doclist=[]\n",
    "for doc in cursor:\n",
    "    doclist.append(doc)\n",
    "t=time.time()\n",
    "print(\"Elapsed time to read \",len(doclist),\" documents=\",t-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82df4e19-3670-43e2-bea0-294b80135e33",
   "metadata": {},
   "source": [
    "The times are not hugely different in this case because the channel collection is relatively small.   In contrast I've seen examples for a collection with millions of documents where the times differ by 100 or more.  From the introduction of indexes above you should be able to figure why an index becomes progressively critical as the size of a collection increases.  The following is a more typical application for waveform data to speed access to common source gathers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac637104-9661-4540-a8b0-f86a03677cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.wf_Seismogram.create_index(\"source_id\")   # note simpler syntax for a single key index\n",
    "db.wf_TimeSeries.create_index(\"source_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d396f4f-f9ce-4495-91b4-a609472fecab",
   "metadata": {},
   "source": [
    "### Waveform read details\n",
    "We will finish this section to consider some more details of the `Database.read_data` method.   To appreciate this section your need a sound understanding of two things we covered above:\n",
    "1.  The concepts of what defines a MsPASS, atomic, seismic data object - `TimeSeries` or `Seismogram`.\n",
    "2.  The idea of what a \"document\" stored in MongoDB is and how to fetch one or more of them.\n",
    "\n",
    "Because `read_data` is such a central player in MsPASS workflows it will help your understanding to know some details about the algorithm inside that method.  I will limit this discusssion to how `read_data` works for atomic reads.  You can and should think of normal ensemble read operations as a loop over a command cursor with the method calling itself for each document the command cursor returns in a `foreach doc in cursor:` loop.\n",
    "\n",
    "Atomic reads are driven by a document passed as arg0 to the method.  Let's look at a wf_miniseed example and dig into the contents a bit this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0366a16-a59f-4be1-87b5-1d7452aeb6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = db.wf_TimeSeries.find_one()\n",
    "print_metadata(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0327bdf0-416c-463d-a335-69ff4bbbbb26",
   "metadata": {},
   "source": [
    "The first thing the reader does is call a constructor for a `TimeSeries` object that loads the entire document into the object's `Metadata` container and creates the `data` vector of size defined by the attribute definition `\"npts\" : 768000\"`.  A key point here is that the wf_TimeSeries document serves as only a template to create a partially completed `TimeSeries` object.  To build a valid object the sample data have to then be loaded.\n",
    "\n",
    "A critical attribute for the reader is this one:  `\"storage_mode\": \"file\"`.   Other valid values are \"gridfs\" and \"url\".   \"gridfs\" uses an internal storage system managed by MongoDB.   It is the default and has the advantage of being easy to use.  It is not recommended for large data sets, however, for a variety of reason discussed in detail in __[this section of the MsPASS User Manual](https://www.mspass.org/user_manual/io.html)__.  \"url\" is used to abstract the process of reading form a network connection via web services or, as we will see in the 3rd session of this class, data stored in a cloud service like Amazon Web Services S3 system.  \n",
    "\n",
    "For this example, file IO demands the following attributes:\n",
    "1. *dir* and *dfile* define the path to the file to be read.\n",
    "2. *foff* is the number of bytes to __[seek](https://www.geeksforgeeks.org/python-seek-function/)__ in the file before reading.\n",
    "3. *format* is an optional attribute defining a data format (e.g. miniseed or SAC).  The default is the same as this example (\"binary\") which means the reader loads the data with the C *fread* function for speed.   For other formats, the data have to be converted.  \n",
    "4. *nbytes* defines the number of bytes to be loaded into memory.  With binary data *npts*, the number of data points, is used as *nbytes* can be inferred as 8*npts.\n",
    "5. When format is anything but \"binary\" the reader attempts to call an obspy reader for that format passing the *nbytes* buffer to the reader.  Any of the formats defined supported by obspy described __[here](https://docs.obspy.org/packages/autogen/obspy.core.stream.read.html#supported-formats)__ should technically work, but we have limited experience with anything other than miniseed.\n",
    "\n",
    "It may be informative to compare the document we just examined to it's parent.  You need to recognize the the wf_TimeSeries object related to that document came by saving a partially processed waveform created by cracking a miniseed file.   This small code segments finds that parent and then prints a few key attributes that show the differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8212e2f7-361d-4991-a64f-f9d86c04f493",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\"net\" : doc[\"net\"],\n",
    "          \"sta\" : doc[\"sta\"],\n",
    "          \"chan\" : doc[\"chan\"],\n",
    "         \"source_id\" : doc[\"source_id\"],\n",
    "        }\n",
    "# verify that is a unique match\n",
    "n = db.wf_miniseed.count_documents(query)\n",
    "print(\"Number of documents matching query=\",n)\n",
    "# since 1 we can use find_one for simplicity\n",
    "doc = db.wf_miniseed.find_one(query)\n",
    "for k in [\"format\",\"nbytes\",\"npts\",\"dir\",\"dfile\",\"foff\"]:\n",
    "    print(\"{} : {}\".format(k,doc[k]))\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbefdcd1-a4e0-425f-80d6-39db3421655b",
   "metadata": {},
   "source": [
    "At your leisure you should study that output and try to understand the differences.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f0823-947c-4698-bb89-3f8808c8c6b0",
   "metadata": {},
   "source": [
    "### Other Standard Collections\n",
    "It is worth repeating the code box from above that lists all the collections in our demonstration database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad6dbbb-deab-43cd-92ec-1f406ec20087",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = db.list_collections()\n",
    "for doc in cursor:\n",
    "    print(doc['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4f34b8-a38f-4bb9-b35e-3a1652a8a4e8",
   "metadata": {},
   "source": [
    "We have used the three \"wf\" collections extensively in our discussions of waveform data management.  There are two other groups of collections you need to understand:\n",
    "1.  We have poked into the *source*, *site*, and *channel* collections.  We have looked at them a bit earlier in different contexts.  Briefly note\n",
    "    - *source* contains information about the source to which a waveform is connected.\n",
    "    - *site* and *channel* contain station related metadata.   *site* documents are briefer and contain only location information.   *channel* documents have the metadata connected to a particular \"channel\" of recorded data.   In the SEED world that means a station code defined by the four magic keys \"net\", \"sta\", \"chan\", and \"loc\" along with a time period for which the metdata are valid.   The documents in this database also contain orientation information and response data created from the obspy *Inventory* object created using web services - we will review that a bit before ending today.\n",
    "2.  The three collection *elog*, *cemetery*, and *abortions* are related to a feature of MsPASS that is the topic of the next section of this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282df59b-937e-44e0-8282-061e6912020d",
   "metadata": {},
   "source": [
    "#### kill concept\n",
    "MsPASS makes extensive use of an approach to data editing that has been used in seismic reflection processing since the earliers processing systems of the 1960s.  The basic idea is that all seismic data objects in MsPASS have a `live` attribute.   When the `live` is set False it tell all MsPASS processing functions to treat that datum as bad.  Processing algorithms do nothing to dead (live==False) data and simply pass them along.   For more on the ideas of why and this is done and how to use this feature for data editing see two sections of the MsPASS User's Manual:   (1) __[Handling Errors](https://www.mspass.org/user_manual/handling_errors.html)__ give a complete concept of trace editing and how it is implemented in MsPASS, and (2) __[Data Editing](https://www.mspass.org/user_manual/data_editing.html)__ describes tools for algorithms that can be used for killing data with certain problems within a workflow.  \n",
    "\n",
    "For today's topic focused on data management and MongoDB the thing you need to understand is what the three collections called *elog*, *cemetery*, and *abortions* are and what they are used for.  \n",
    "\n",
    "First, consider *elog*. Let's start with this little code segment to explore what is in our demo database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b54bc7-d94c-44eb-a2ad-5bec5d9af816",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_elog = db.elog.count_documents({})\n",
    "print(\"The elog collection of our data base has \",n_elog,\" documents\")\n",
    "print(\"This is the first one in that collection\")\n",
    "doc=db.elog.find_one()\n",
    "print_metadata(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f892580-8a0d-447d-9835-0e6b5abc08da",
   "metadata": {},
   "source": [
    "This data set is very clean and as you can see elog is very small.  Tne only error logged is one related to a problem in one of the miniseed files.  It is relatively harmless.   Any time any algorithm posts an error of any kind the message is saved in the database in this *elog* collection with a document like that above.   Queries of elog are necessary if you are having a problem with a lot of data being unexpectedly killed.   \n",
    "\n",
    "The *cemetery* and *abortions* collections provide a record of two different kinds of \"dead data\".   *cemetery* contains metadata describing a datum that was killed during processing. There are documents in *abortions* only if there are read problems in constructing one or more data objects.   (An \"abortion\" is a data object that was never born (constructed successfully)).   Let's explore their contents for this demon database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6288b37c-9315-4d5c-9400-831557f6e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cemetery=db.cemetery.count_documents({})\n",
    "print(\"Number of documents in the cemetery=\",n_cemetery)\n",
    "n_abortions=db.abortions.count_documents({})\n",
    "print(\"Number of documents in the abortions collection=\",n_abortions)\n",
    "print(\"First cemetery document\")\n",
    "doc = db.cemetery.find_one()\n",
    "print_metadata(doc)\n",
    "print(\"First abortions document\")\n",
    "doc=db.abortions.find_one()\n",
    "print_metadata(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e519f2-3bde-4895-8618-cc73bee96bc9",
   "metadata": {},
   "source": [
    "Note:\n",
    "- Both example documents have a \"tombstone\" attribute.   The \"tombstone\" is a dump of the Metadata container of the problem datum.\n",
    "- The *cemetery* example has an embedded \"logdata\" attribute containing a record similar to elog.  In this example we see the datum was killed because the data had a gap and when the processing windowed the data around P it killed that datum.  The *abortions* example does not have a \"logdata\" entry.  Sometime they do, but in this case where this came from can be gleaned from errors you can see that were posted when you created this database.  That is, one of the miniseed files has a corrupted section that generates read errors.   The example abortions document can be linked back to that file (\"Event_12.msd\").  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d949bec-f31f-4a26-96d4-236b5b86d994",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "The final topic for today's discussion of data management is what MongoDB calls \"normalization\".   For students familiar with relational databases MongoDB \"normalization\" is conceptually similar to a relational database \"join\".  A more extensive reference on this topic is the section of the MsPASS User's Manual titled __[Normalization](https://www.mspass.org/user_manual/normalization.html)__.   Before attempting the homework for this session, students should read that section.\n",
    "\n",
    "Normalization in seismic processing with MsPASS most useful for joining small tables to waveform documents.   We have found that with MongoDB normalization makes sense if the collection to be matched against a waveform collection is small compared to the number of documents in the waveform collection.  Source and receiver Metadata are a case in point.   For a larger wf collection there are many wf documents for each source and wf documents with a common match in a site/channel collection.  \n",
    "\n",
    "Normalization is implemented in MsPASS in a novel way with a concept we call a \"Matcher\".  The most useful matchers are those that load an entire collection and have a fast algorithm to match records in a cache to wf documents.  We will look at a full implementation before closing today when we review the processing workflow next.  For now, however, here is an example matcher that is used to match SEED waveform data using net:sta:chan:loc and a time range: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee01dc99-9e57-4aed-b574-be8325fe9ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.db.normalize import MiniseedMatcher\n",
    "matcher=MiniseedMatcher(db)\n",
    "print(matcher.attributes_to_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aa04e7-71c6-4093-addf-14aad361f377",
   "metadata": {},
   "source": [
    "The print statement shows one attribute of this object called *attributes_to_load*.   It defines what key-value pairs will be loaded from the matchers internal cache that that match that datum.   This short example compares the content of a `TimeSeries` loaded from a doc with and without normalization using the normalizer in *matcher*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a20b88-812b-4d3b-a357-bb83350f5f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=db.wf_TimeSeries.find_one()\n",
    "d = db.read_data(doc)\n",
    "print(\"///////////////////////////////////////////////////////////\")\n",
    "print(\"Metadata without normalization\")\n",
    "print(\"///////////////////////////////////////////////////////////\")\n",
    "print_metadata(d)\n",
    "d = db.read_data(doc,normalize=[matcher])\n",
    "print(\"///////////////////////////////////////////////////////////\")\n",
    "print(\"Metadata with normalization using MiniseedMatcher class\")\n",
    "print(\"///////////////////////////////////////////////////////////\")\n",
    "print_metadata(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d55d08-541c-47f5-8698-60fc35dc8625",
   "metadata": {},
   "source": [
    "Notice that all the key names listed above for \"attributes_to_load\" appear in the second print preceded by \"channel_\".   e.g \"channel_lat\" or \"channel_elev\".   They were added by the *matcher* passed to `read_data` with the *normalize* argument.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c135dd8-574b-42e4-aaaf-c7c2b9f34a6a",
   "metadata": {},
   "source": [
    "## Discuss Processing Workflow\n",
    "Now open a tab in jupyter lab for the \"PrecourseProcessing.ipynb\" notebook.   It is the workflow you ran to create the database we have been exploring.  We will close with me telling you how that workflow works and linking back to things we have just discussed.  In the next session we will talk at length about processing functions and how to build a workflow like that. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654aff3c-a8ff-4c0d-b8c2-eb8bcf2e4022",
   "metadata": {},
   "source": [
    "## Quiz and Homework\n",
    "A quiz on topics we covered since the break is the first section of homework you should plan to complete before the next class session.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392b2b2a-94a8-45b1-957d-24ae1dddb699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
