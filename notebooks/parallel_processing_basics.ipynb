{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "477df7d5-d793-4dfe-a868-b45f53309f6c",
   "metadata": {},
   "source": [
    "# Parallel Processing Basics\n",
    "Developing this as a followup to getting started.   Shows how to run a parallel workflow.  Builds on data saved in getting started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db5c73ed-5e94-4c65-9936-67cd86c717cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.db.database import Database\n",
    "import mspasspy.client as msc\n",
    "dbclient=msc.Client()\n",
    "db = dbclient.get_database('getting_started')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078947e1-f131-4f80-8124-40768fa7fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.algorithms.window import WindowData\n",
    "from mspasspy.algorithms.basic import ator\n",
    "from mspasspy.ccore.algorithms.basic import TimeWindow\n",
    "from mspasspy.db.normalize import (normalize,\n",
    "                                   ObjectIdMatcher,\n",
    "                                )\n",
    "from mspasspy.algorithms.window import WindowData\n",
    "from obspy.geodetics import gps2dist_azimuth,kilometers2degrees\n",
    "from obspy.taup import TauPyModel\n",
    "from obspy import UTCDateTime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcd55b9-4ec3-408e-a87f-6e3e5c68f278",
   "metadata": {},
   "source": [
    "### Parallel Workflow with dask distributed\n",
    "We next introduce an important variant of the above when running a job on a large cluster with multiple nodes.  The method above is appropriate for testing a workflow on a desktop before running the job on a bigger scale data set on a large cluster with multiple nodes.   In that case the authors of dask recommend the use of the newer scheduler they call [dask distributed](https://distributed.dask.org/en/stable/). Besides better performance in a cluster dask distributed adds the capability of monitoring a job in real time and profiling a job through the dask [diagnostic monitor](https://distributed.dask.org/en/stable/diagnosing-performance.html).   The next block enables this capability with this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ef3f8-b0f2-4ab8-9b40-23cd5bb96d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "scheduler_client=Client()\n",
    "scheduler_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553f9346-132c-4edc-b92e-478b1a8c6c10",
   "metadata": {},
   "source": [
    "The status page for this notebook is now available to you an can be accessed via port 8787.   It is because of that requirement that you may have had to restart this container with the `-p 8787:8787` incantation.   Without that port mapping you would not be able to connect to the diagnostics page.   We note that in our experience using the hyperlink above will no work either.   You instead will probably need to use the link via the default localhost of `127.0.0.1:8787/status`.  You might be able to click on [this link](http://127.0.0.1:8787/status).   If that doesn't work resort to a cut and paste of the above url. \n",
    "\n",
    "Now that we have dask diagnostics running let's run a variation of the above workflow that will allow you to watch dask work.  Note this workflow differs from the above in three ways:\n",
    "1.  It doesn't repeat the initializations.  Note that was done here only because of the structure of this notebook and would not be normal.\n",
    "2.  We use a different approach to launch the computations.  We link the bag (`bg`) to the dask distributed scheduler we just created.   Without that line the diagnostics monitor will not display.\n",
    "3.  We intentionally commented out the line to save the data.  We did that to allow you to run this next box repeatedly and not produce duplicate data.  \n",
    "\n",
    "As item 3 says run the next code box and watch the real time display.   Experiment with the different menu options as described in the dask documentation link above.   When the job completes you might also want to look at the profiling output.   We won't dwell on the details of dask diagnostics, but refer you to documentation.  The main point here is that those tools can be useful to improve performance on a workflow you need to run on a large amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae88748f-2e2e-4a2d-890d-e14e6565eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_by_Ptime(d):\n",
    "    if d.live:\n",
    "        if d.is_defined('Ptime'):\n",
    "            tshift = d['Ptime']\n",
    "            d = ator(d,tshift)\n",
    "        else:\n",
    "            d.kill()\n",
    "            d.elog.log_error(\"shift_by_Ptime\",\n",
    "                             \"Required key Ptime is was not defined\",\n",
    "                             ErrorSeverity.Invalid)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9061b76f-2699-4844-a98b-e49a39a51abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttmodel = TauPyModel(model=\"iasp91\")\n",
    "station_normalizer = MiniseedMatcher(db)\n",
    "# we loaded data with this offset relative to origin time\n",
    "# TODO:  This matcher is currently broken. Workaround below\n",
    "t0offset=763.0 - 4.0*60.0  \n",
    "source_normalizer = OriginTimeMatcher(db,\n",
    "                                      t0offset=t0offset,\n",
    "                                      source_time_key='time',\n",
    "                                     )\n",
    "resampler=ScipyResampler(20.0)\n",
    "decimator=ScipyDecimator(20.0)\n",
    "stime=-100.0\n",
    "etime=600.0    \n",
    "cursor = db.wf_miniseed.find({})  # this is a loop over the full dataset\n",
    "t0 = time.time()\n",
    "dataset = read_distributed_data(db,\n",
    "                                collection='wf_TimeSeries',\n",
    "                                normalize=[station_normalizer],\n",
    "                               )\n",
    "#dataset = dataset.map(load_source_data,db)\n",
    "#dataset = dataset.map(detrend,type=\"constant\")\n",
    "#dataset = dataset.map(resample,decimator,resampler)\n",
    "#dataset = dataset.map(set_Ptime,model=ttmodel)\n",
    "#dataset = dataset.map(shift_by_Ptime)\n",
    "#dataset = dataset.map(WindowData,stime,etime)\n",
    "dataset = dataset.map(terminator)\n",
    "result=dataset.compute()\n",
    "t=time.time()\n",
    "nlive=0\n",
    "ndead=0\n",
    "#for x in result:\n",
    " #   if x:\n",
    "  #      nlive += 1\n",
    "   # else:\n",
    "    #    ndead += 1\n",
    "print(\"Total processing time=\",t-t0)\n",
    "print(\"Number of live data saved=\",nlive)\n",
    "print(\"number of data killed=\",ndead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef8bb7-dba4-4c65-8353-17aa10170bdd",
   "metadata": {},
   "source": [
    "Compare the above with the earlier serial job.  The for loop command and read_data line are replaced by the following line:\n",
    "```\n",
    "bg = read_distributed_data(db,cursor,normalize=['source'])\n",
    "```\n",
    "The `read_distributed_data` function creates a container called a dask \"bag\".  A convenient way to view a bag is list of things that doesn't need to fit in memory.   The \"things\", in our case, are mspass TimeSeries objects.   The `read_distributed_data` line is followed by a series of lines that in python jargon apply the \"map method\" of the \"bag\" object/container. The concept of a \"map\" operator is one of the two keywords in the modern concept of the \"map-reduce\" model of big data science.  You can find many web pages and turorials discussing map-reduce in general and map-reduce for dask in particular.   For now, we emphasize that arg0 of the map method is a function name.  Each call to map applies a named function to data that it assumes emits another datum that is always the same type.   All the processing functions in the loop above use that model.  For example, the `resample` function takes an input TimeSeries of any sample rate and returns a resampled representation of that datum at 20 sps.  \n",
    "\n",
    "With that background, note the workflow runs a sequence of algorithms through the map method driven by the same function names as above in the same order. For example, consider this line in the serial job that runs the normalize function we used earlier:\n",
    "```\n",
    "d = normalize(d,station_matcher)\n",
    "```\n",
    "The comparable operator above is this:\n",
    "```\n",
    "bg = bg.map(normalize,station_matcher)\n",
    "```\n",
    "The key point we want to make here is that it is straightforward to convert any loop like the serial job to a parallel version using dask.  There are three deviation:\n",
    "1.  The call to the ator function required us to use a python `lambda` function.  That is often a useful trick to handle variable arguments padded through header values.   If you are unfamiliar with lambda function there are numerous articles on this topic on the web.\n",
    "2. We added a call to `db.save_data` so we an work on these data further befow.   \n",
    "3.  We use a terminator lambda function to return only the value of the boolean \"live\" attribute.    \n",
    "\n",
    "A feature of dask potentially confusing to newcomers is all the calls the the bag \"map method\" are \"lazy\".   What that means is nothing is actually computed until we call the bag's \"compute method\".   A simple way to understand the call to compute is that it converts a bag to a python list and returns the result.  We store that list here as `res`.  This entire data set may not fit in your local machine.  That is why we used the last lambda function. It reduces the bag to a list of booleans that are unlikely to cause a memory problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb0bf32-145e-4d40-bf5d-0e77483f47f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor=db.wf_TimeSeries.find(query)\n",
    "t0 = time.time()\n",
    "bg = read_distributed_data(db,cursor,normalize=['source'])\n",
    "bg = bg.map(normalize,station_matcher)\n",
    "bg = bg.map(detrend,type=\"constant\")\n",
    "bg = bg.map(resample,decimator,resampler)\n",
    "bg = bg.map(set_Ptime,model=ttmodel)\n",
    "bg = bg.map(lambda d : ator(d,d[\"Ptime\"]))\n",
    "bg = bg.map(WindowData,stime,etime)\n",
    "bg = bg.map(lambda d : d.live)\n",
    "#bg = bg.map(db.save_data,data_tag=\"Pwave_windowed_data\")\n",
    "scheduler_client.persist(bg)\n",
    "res=bg.compute()\n",
    "t=time.time()    \n",
    "print(\"Parallel job processing time with dask distributed=\",t-t0)\n",
    "print(\"Time per waveform=\",(t-t0)/n)\n",
    "nlive=0\n",
    "n=len(res)\n",
    "for x in res:\n",
    "    if x:\n",
    "        nlive += 1\n",
    "print(\"Processing completed \",nlive,\" of \",n,\" waveforms handled\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
