{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f8408e2-8694-4d3f-8437-fdba5fabc518",
   "metadata": {},
   "source": [
    "# MongoDB Tutorial\n",
    "## *Prof. Gary L. Pavlis*\n",
    "\n",
    "## Overview\n",
    "This notebook is designed as a teaching tutorial on use of the MongoDB database used in MsPASS.  Numerous pedagogic materials exist online for learning MongoDB, but this notebook focuses on key features the author has found useful in seismology research.  It is best used in conjunction with two other sources:\n",
    "1.  The section of the User's Manual titled \"Using MongoDB with MsPASS\".\n",
    "2.  As with most modern IT topics a web search for details of some topics addressed in this tutorial may be helpful if the MsPASS User's Manual doesn't address the topic.\n",
    "\n",
    "The bulk of this notebook is organized by the keywords of the standard CRUD acronymn of database theory.  CRUD is an abbreviation of Create (save), Read, Update, and Delete.  Section are titled with those keywords and covered in the order defined by CRUD.  Before that, however, it is necessary to review a few basic concepts covered in the section immediately below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8c262b-4e39-44cb-8c76-9f02ab571088",
   "metadata": {},
   "source": [
    "## MongoDB Core Concepts\n",
    "### Client-server model\n",
    "MongoDB is a client-server system.  That bit of jargon \n",
    "has some important implications:\n",
    "\n",
    "1.  All database commands issued from python are not executed directly by the python interpreter.  Instead instructions are sent to the MongoDB server.   In MsPASS the server is launched inside a container.   Unless you are running this notebook on a cluster with multiple nodes, you can verify the server is running by launching a terminal in the jupyterlab interface and running the command `ps -A`.  You should get output similar to the following that shows the server as the CMD with the name `mongod`:\n",
    "```\n",
    "root@b0d79c4cc440:/home/scoped# ps -A\n",
    "  PID TTY          TIME CMD\n",
    "    1 ?        00:00:00 tini\n",
    "    8 ?        00:00:00 start-mspass.sh\n",
    "   15 ?        00:07:27 dask-scheduler\n",
    "   21 ?        00:06:47 dask-worker\n",
    "   22 ?        00:01:44 mongod\n",
    "   23 ?        00:00:44 jupyter-lab\n",
    "   34 ?        00:00:00 python3.10\n",
    "   37 ?        00:10:43 python3.10\n",
    "  154 ?        00:00:20 python\n",
    "  364 ?        00:00:01 python\n",
    " 1010 pts/0    00:00:00 bash\n",
    " 1036 pts/0    00:00:00 ps\n",
    "```\n",
    "2.  All database IO passes through a network data connection on network \"port number\" 27017.   That is important to know as a fundamental issue because a network communication channel is not the fastest data pipe on most computers.\n",
    "3.  To communicate with MongoDB, your program must create a connection to the \"server\".  In the jargon of modern computing you have to create a \"client\" that will act as your agent to talk to the arrogant MongoDB \"server\" (the mongod program running in the background).  \n",
    "\n",
    "With that background, the first thing you will need to do, since mongod is already running in this environment, is to create the \"client\".    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b56e3e23-3392-4a8b-af67-0df9c3de33bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.db import DBClient\n",
    "dbclient=DBClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8490264-3aaa-4130-84cd-4ddcb874ce36",
   "metadata": {},
   "source": [
    "A geeky detail worth noting here is that we are using a python class (object) called `DBClient` that is a \"subclass\" of `pymongo.MongoClient`.  I point that out because all internet sources that are MongoDB introductions will create an instance of `pymongo.MongoClient` instead of the MsPASS extension used above.  An important \"extension\" DBClient adds is illustrated by the next code box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09f3f532-0e7f-41a0-8f89-031e42a217a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = dbclient.get_database(\"dbtutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878cd8f2-7e47-45c5-8e5b-1bdcdebc1c90",
   "metadata": {},
   "source": [
    "This incantation runs the `get_database` \"method\" of the class called `DBClient`.   It returns what we call a \"database handle\" in the User's Manual.   The MsPASS \"database handle\" is a python class that is itself a subclass  of another pymongo class.  Both have the name `Database`, but the MsPASS version adds a number of extensions for handling of seismic data.   The main ones of interest are readers and writers for seismic data objects, station metadata, and source metadata.  A key point is almost all MsPASS workflows begin with a variation of the combination of the two python code boxes above.   \n",
    "\n",
    "When you call the `get_database` method as shown above the \"handle\" is created/constructed and can be accessed for the rest of your python workflow with the symbol you put on the left hand side of the expression (`db` in this example).  That name, of course, can be anything you want it to be, but For all examples in the MsPASS documentation we used `db` as a standard symbol to reduce confusion, but that should be viewed as simply a notation convention not a rule.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2894ea2b-a29a-44ee-bf69-94de61092783",
   "metadata": {},
   "source": [
    "### Documents and Collections\n",
    "The User's Manual section companion to this tutorial discusses the MongoDB jargon terms `document` and `collection` at length.   I will not repeat that material here, but note from here on I assume you know what those two terms mean.   If you don't know what these terms mean consult the \"Using MongoDB with MsPASS\" section of the User's Manual or some other source before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6b1788-2105-4c8d-a116-d0ce94d5ddad",
   "metadata": {},
   "source": [
    "## Create\n",
    "The first letter in the CRUD acronynm is \"Create\".  For this tutorial some form of \"create\" is an essential first step to put some kind of data into our tutorial database.   Most tutorials will begin inserting some largely arbitrary data.  Since this tutorial is designed for seismologists it seems more appropriate to work with seismology data.   The box below is a variant of one in the \"getting_started\" tutorial. It uses obspy's web service module to fetch station metadata for all \"B-channels\" defined for Earthscope TA stations that operated during the calendar year 2011. Obspy creates a python image of the stationxml downloaded from IRIS they call an `Inventory`.  In this code we use the MsPASS \"create\" method `save_inventory` to save a version of `Inventory` repackaged to mesh with MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f6594d-7e96-456e-bd7d-8e027777b04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database.save_inventory processing summary:\n",
      "Number of site records processed= 653\n",
      "number of site records saved= 653\n",
      "number of channel records processed= 2091\n",
      "number of channel records saved= 2079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(653, 2079, 653, 2091)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn import Client\n",
    "client=Client(\"IRIS\")\n",
    "starttime=UTCDateTime('2011-01-01T00:00:00.0')\n",
    "endtime=UTCDateTime('2012-01-01T00:00:00.0')\n",
    "inv=client.get_stations(network='TA',starttime=starttime,endtime=endtime,\n",
    "                        format='xml',channel='BH?',level='response')\n",
    "db.save_inventory(inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eecefc-0f72-490c-818a-52ef44fd51ba",
   "metadata": {},
   "source": [
    "## Read\n",
    "The R of CRUD is \"Read\" and is more-or-less the inverse of \"create\".   The keyword used for pulling \"documents\" from a MongoDB database, however, is `find`.  There are two basic methods in the core MongoDB API for fetching documents:  `find_one` and `find`.  They behave completely differently.\n",
    "\n",
    "### find_one\n",
    "\n",
    "Let's begin with a simple application of `find_one`.  As the name implies it always returns one and only one document.  Here is a default application to the \"site\" collection that was created under the hood when we ran `save_inventory` above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c9a86d7-21ac-4478-97ac-909d8ec349fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of a document =  <class 'dict'>\n",
      "This is the content of that document\n",
      "{'_id': ObjectId('65e322fb5cd7e1e9490a5904'), 'loc': '', 'net': 'TA', 'sta': '034A', 'lat': 27.064699, 'lon': -98.683296, 'coords': [-98.683296, 27.064699], 'location': {'type': 'Point', 'coordinates': [-98.683296, 27.064699]}, 'elev': 0.155, 'edepth': 0.0, 'starttime': 1262908800.0, 'endtime': 1321574399.0, 'site_id': ObjectId('65e322fb5cd7e1e9490a5904')}\n"
     ]
    }
   ],
   "source": [
    "doc = db.site.find_one()\n",
    "print(\"The type of a document = \",type(doc))\n",
    "print(\"This is the content of that document\")\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca751521-c2a9-4f1b-b7b1-0d95782ac56c",
   "metadata": {},
   "source": [
    "As the output demonstrates a `find_one` returns data in a python dictionary.   You might also note the raw `print(doc)` output is a bit challenging to read.   For the rest of this tutorial we will use a construct I've used a lot that makes the output a bit easier to read.   I'll define a small little function we will use elsewhere in this tutorial to make output more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc855df9-552a-4ee4-a4d1-092ad9e47462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"65e322fb5cd7e1e9490a5904\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"034A\",\n",
      "  \"lat\": 27.064699,\n",
      "  \"lon\": -98.683296,\n",
      "  \"coords\": [\n",
      "    -98.683296,\n",
      "    27.064699\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -98.683296,\n",
      "      27.064699\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 0.155,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1262908800.0,\n",
      "  \"endtime\": 1321574399.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"65e322fb5cd7e1e9490a5904\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from bson import json_util\n",
    "def pretty_print(doc,indent=2):\n",
    "    print(json_util.dumps(doc,indent=indent))\n",
    "doc=db['site'].find_one()\n",
    "pretty_print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c88284-19d6-4a94-8db2-121941c65074",
   "metadata": {},
   "source": [
    "Things of note in that box are:\n",
    "1.  The `pretty_print` function definition is a bit trivial, which is why it isn't a standard MsPASS function.   It uses the `json_util.dumps` function to create the curly bracket formatted print that is a lot easier to understand than the raw dump of the python dictionary.   It shows more clearly that a document is always made of up of one or more key-value pairs.\n",
    "2.  This example intentionally uses a variant of the syntax for interacting with the database handle.   Note in the first box I used `db.site` while in the second I used `db['site']`.   A powerful but confusing, in my opinion, feature of python is its capability to create that type of syntactic alternative incantation.   Technically, what it does is specify a \"collection\", which in this case is named \"site\".  In the jargon of MongoDB the `find` and `find_one` methods, which are the core MongoDB \"read\" methods, are \"collection operation\".   You should realize that `db` is the top-level symbol that refers to the \"whole\" database that is assumed to contain one more \"collection\"s.  The two incantations used above are alternative ways to get a handle to a specific \"collection\".   To clarify that point the following box illustrates a useful way to find the set of collections defined in our tutorial database at this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "358b57a0-cbe7-4748-90bd-fbf67f430976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current collections in tutorial database:\n",
      "site\n",
      "channel\n"
     ]
    }
   ],
   "source": [
    "cursor=db.list_collections()\n",
    "print(\"Current collections in tutorial database:\")\n",
    "for doc in cursor:\n",
    "    print(doc['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52b0c09-a5ea-4a26-ab87-04bf4da7c2b6",
   "metadata": {},
   "source": [
    "### find\n",
    "The above is also a good segway to the second standard MongoDB read method called `find`.  We defined the return of the `list_collection` function with the symbol \"cursor\".   That was a choice for the name, but consider this output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62544a35-1bec-414b-a3c1-33373dc511a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type type of the symbol cursor is  <class 'pymongo.command_cursor.CommandCursor'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Type type of the symbol cursor is \",type(cursor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e525f-5c77-480a-8a7d-134872781b0e",
   "metadata": {},
   "source": [
    "A MongoDB `CommandCursor` is technically a __[forward iterator](https://www.boost.org/sgi/stl/ForwardIterator.html)__.   That means it acts like a list that can only be traversed \"forward\" with a construct like that above.   It is not at all the same thing, however, as a python list.   It is a handle that interacts with the database to sequentially return documents.   The following example with the `find` method illustrates the more common usage of a cursor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d014ad89-0f0b-4afe-9100-9d89344fb3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"65e322fb5cd7e1e9490a5904\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"034A\",\n",
      "  \"lat\": 27.064699,\n",
      "  \"lon\": -98.683296,\n",
      "  \"coords\": [\n",
      "    -98.683296,\n",
      "    27.064699\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -98.683296,\n",
      "      27.064699\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 0.155,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1262908800.0,\n",
      "  \"endtime\": 1321574399.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"65e322fb5cd7e1e9490a5904\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"65e322fb5cd7e1e9490a5907\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"035A\",\n",
      "  \"lat\": 26.937901,\n",
      "  \"lon\": -98.102303,\n",
      "  \"coords\": [\n",
      "    -98.102303,\n",
      "    26.937901\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -98.102303,\n",
      "      26.937901\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 0.029,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1263254400.0,\n",
      "  \"endtime\": 1321315199.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"65e322fb5cd7e1e9490a5907\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"65e322fb5cd7e1e9490a590a\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"035Z\",\n",
      "  \"lat\": 26.462999,\n",
      "  \"lon\": -98.068298,\n",
      "  \"coords\": [\n",
      "    -98.068298,\n",
      "    26.462999\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -98.068298,\n",
      "      26.462999\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 0.019,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1262995200.0,\n",
      "  \"endtime\": 1321833599.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"65e322fb5cd7e1e9490a590a\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "cursor=db.site.find()\n",
    "cursor.limit(3)\n",
    "for doc in cursor:\n",
    "    pretty_print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1b6b01-9933-457e-bba5-c2c450c97fdd",
   "metadata": {},
   "source": [
    "A few points of note about that simple 3 line code box:\n",
    "1.  I used the default return for `find`.   The default returns \"all\", which in this would mean several hundred documents. For a large waveform data set it can easily be millions.\n",
    "2.  To limit the output for this notebook I used a \"method\" of the `CommandCursor` class called \"limit\".  Here I did that with a separate line, but most python programmers would write the same expression as `cursor=db.site.find().limit(3)`.\n",
    "3.  The output shows iterating through that (modified) cursor retrieves 3 documents from site.\n",
    "\n",
    "Returning \"all\" is rarely what you want.  The more common use is to run `find` with a query as arg0 to the function.  The next subsection illustrates that use along with basics of the query language discussed in numerous printed sources, online sources, and the MsPASS User's Manual.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd55e96-3d6f-442f-a5b5-a7139dbdd6fd",
   "metadata": {},
   "source": [
    "### Mongo Query Language (MQL)\n",
    "#### Single key match and basics\n",
    "I will run a set of examples of increasing levels of complexity.   This particular section of this tutorial is intended as a hands on supplement to the section of the User's Manual titled \"Using MongoDB with MsPASS\" describing MQL.  \n",
    "\n",
    "First, a unique match query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81e37a54-a0d6-4012-a015-96548eccc7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of site documents for station 134A= 1\n",
      "Number of channel documents for station 134A= 3\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"65e322fb5cd7e1e9490a591c\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"134A\",\n",
      "  \"lat\": 32.572899,\n",
      "  \"lon\": -98.079498,\n",
      "  \"coords\": [\n",
      "    -98.079498,\n",
      "    32.572899\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -98.079498,\n",
      "      32.572899\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 0.297,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1258329600.0,\n",
      "  \"endtime\": 1315526399.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"65e322fb5cd7e1e9490a591c\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "query={'sta' : '134A'}\n",
    "nsite=db.site.count_documents(query)\n",
    "print(\"Number of site documents for station 134A=\",nsite)\n",
    "nchannel=db.channel.count_documents(query)\n",
    "print(\"Number of channel documents for station 134A=\",nchannel)\n",
    "cursor=db.site.find(query)\n",
    "for doc in cursor:\n",
    "    pretty_print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c6507-78b9-4e23-885f-54aa885f9594",
   "metadata": {},
   "source": [
    "Notice:\n",
    "1.  I used another important collection method called `count_documents` to fetch the expected number of documents the query would yield.  Standard practice in working through many queries is to do a check that the number it returns makes sense.\n",
    "2.  We see there is one and only one station matching query is site and three in channel.  The reason channel has three, of course, is that there is a three-component sensor at that station that defines the recording channels.  To see why I didn't run the for loop over a cursor created from channel consider this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7998ec7e-e73d-42c9-88fd-0e6d8eb64804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"65e322fb5cd7e1e9490a591c\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"134A\",\n",
      "  \"lat\": 32.572899,\n",
      "  \"lon\": -98.079498,\n",
      "  \"coords\": [\n",
      "    -98.079498,\n",
      "    32.572899\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -98.079498,\n",
      "      32.572899\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 0.297,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1258329600.0,\n",
      "  \"endtime\": 1315487100.0,\n",
      "  \"chan\": \"BHE\",\n",
      "  \"vang\": 90.0,\n",
      "  \"hang\": 90.7,\n",
      "  \"serialized_channel_data\": {\n",
      "    \"$binary\": {\n",
      "      \"base64\": \"gASVoBoAAAAAAACMHG9ic3B5LmNvcmUuaW52ZW50b3J5LmNoYW5uZWyUjAdDaGFubmVslJOUKYGUfZQojA5fbG9jYXRpb25fY29kZZSMAJSMCV9sYXRpdHVkZZSMGW9ic3B5LmNvcmUuaW52ZW50b3J5LnV0aWyUjAhMYXRpdHVkZZSTlEdAQElUwSJ0n4WUgZR9lCiMBWRhdHVtlE6MEWxvd2VyX3VuY2VydGFpbnR5lE6MEXVwcGVyX3VuY2VydGFpbnR5lE6MEm1lYXN1cmVtZW50X21ldGhvZJROdWKMCl9sb25naXR1ZGWUaAiMCUxvbmdpdHVkZZSTlEfAWIUWfseGPIWUgZR9lChoDk5oD05oEE5oEU51YowKX2VsZXZhdGlvbpRoCIwIRGlzdGFuY2WUk5RHQHKQAAAAAACFlIGUfZQoaA9OaBBOaBFOjAVfdW5pdJROdWKMBl9kZXB0aJRoGkcAAAAAAAAAAIWUgZR9lChoD05oEE5oEU5oHk51YowIX2F6aW11dGiUaAiMB0F6aW11dGiUk5RHQFaszMzMzM2FlIGUfZQoaA9OaBBOaBFOdWKMBF9kaXCUaAiMA0RpcJSTlEcAAAAAAAAAAIWUgZR9lChoD05oEE5oEU51YowMX3dhdGVyX2xldmVslE6MBXR5cGVzlF2UjAtHRU9QSFlTSUNBTJRhjBNleHRlcm5hbF9yZWZlcmVuY2VzlF2UjAxfc2FtcGxlX3JhdGWUaAiMClNhbXBsZVJhdGWUk5RHQEQAAAAAAACFlIGUfZQoaA9OaBBOaBFOdWKMIHNhbXBsZV9yYXRlX3JhdGlvX251bWJlcl9zYW1wbGVzlE6MIHNhbXBsZV9yYXRlX3JhdGlvX251bWJlcl9zZWNvbmRzlE6MIl9jbG9ja19kcmlmdF9pbl9zZWNvbmRzX3Blcl9zYW1wbGWUaAiMCkNsb2NrRHJpZnSUk5RHPzBiTdLxqfyFlIGUfZQoaA9OaBBOaBFOdWKMEWNhbGlicmF0aW9uX3VuaXRzlIwBVpSMHWNhbGlicmF0aW9uX3VuaXRzX2Rlc2NyaXB0aW9ulIwMZW1mIGluIHZvbHRzlIwGc2Vuc29ylGgIjAlFcXVpcG1lbnSUk5QpgZR9lCiMBHR5cGWUTowLZGVzY3JpcHRpb26UjDJTdHJlY2tlaXNlbiBTVFMtMiBHMy9RdWFudGVycmEgMzMwIExpbmVhciBQaGFzZSBDb5SMDG1hbnVmYWN0dXJlcpROjAZ2ZW5kb3KUTowFbW9kZWyUTowNc2VyaWFsX251bWJlcpROjBFpbnN0YWxsYXRpb25fZGF0ZZROjAxyZW1vdmFsX2RhdGWUTowRY2FsaWJyYXRpb25fZGF0ZXOUXZSMC3Jlc291cmNlX2lklE51YowNcHJlX2FtcGxpZmllcpROjAtkYXRhX2xvZ2dlcpROjAtfZXF1aXBtZW50c5RdlIwIcmVzcG9uc2WUjB1vYnNweS5jb3JlLmludmVudG9yeS5yZXNwb25zZZSMCFJlc3BvbnNllJOUKYGUfZQoaFdOjBZpbnN0cnVtZW50X3NlbnNpdGl2aXR5lGhdjBVJbnN0cnVtZW50U2Vuc2l0aXZpdHmUk5QpgZR9lCiMBXZhbHVllEdBwrEYYAAAAIwJZnJlcXVlbmN5lEc/yZmZmZmZmowLaW5wdXRfdW5pdHOUjANNL1OUjBdpbnB1dF91bml0c19kZXNjcmlwdGlvbpSMHXZlbG9jaXR5IGluIG1ldGVycyBwZXIgc2Vjb25klIwMb3V0cHV0X3VuaXRzlIwGQ09VTlRTlIwYb3V0cHV0X3VuaXRzX2Rlc2NyaXB0aW9ulIwOZGlnaXRhbCBjb3VudHOUjBVmcmVxdWVuY3lfcmFuZ2Vfc3RhcnSUTowTZnJlcXVlbmN5X3JhbmdlX2VuZJROjBxmcmVxdWVuY3lfcmFuZ2VfZGJfdmFyaWF0aW9ulE51YowVaW5zdHJ1bWVudF9wb2x5bm9taWFslE6MD3Jlc3BvbnNlX3N0YWdlc5RdlChoXYwXUG9sZXNaZXJvc1Jlc3BvbnNlU3RhZ2WUk5QpgZR9lCiMGl9wel90cmFuc2Zlcl9mdW5jdGlvbl90eXBllIwYTEFQTEFDRSAoUkFESUFOUy9TRUNPTkQplIwYX25vcm1hbGl6YXRpb25fZnJlcXVlbmN5lGgIjAlGcmVxdWVuY3mUk5RHP8mZmZmZmZqFlIGUfZQoaA9OaBBOaBFOdWKMFG5vcm1hbGl6YXRpb25fZmFjdG9ylEdDk1fxXv7jgIwGX3plcm9zlF2UKIwbb2JzcHkuY29yZS51dGlsLm9ic3B5X3R5cGVzlIwYQ29tcGxleFdpdGhVbmNlcnRhaW50aWVzlJOURwAAAAAAAAAARwAAAAAAAAAAhpSBlH2UKIwSX3VwcGVyX3VuY2VydGFpbnR5lGiGjBNfQ29tcGxleFVuY2VydGFpbnR5lJOURwAAAAAAAAAARwAAAAAAAAAAhpSBlIwSX2xvd2VyX3VuY2VydGFpbnR5lGiORwAAAAAAAAAARwAAAAAAAAAAhpSBlIwXbWVhc3VyZW1lbnRfbWV0aG9kX3JlYWyUTowXbWVhc3VyZW1lbnRfbWV0aG9kX2ltYWeUTowGbnVtYmVylEsAdWJoiEcAAAAAAAAAAEcAAAAAAAAAAIaUgZR9lChojGiORwAAAAAAAAAARwAAAAAAAAAAhpSBlGiRaI5HAAAAAAAAAABHAAAAAAAAAACGlIGUaJROaJVOaJZLAXViaIhHwHzxmZmZmZpHQHroAAAAAACGlIGUfZQoaIxojkcAAAAAAAAAAEcAAAAAAAAAAIaUgZRokWiORwAAAAAAAAAARwAAAAAAAAAAhpSBlGiUTmiVTmiWSwJ1YmiIR8B88ZmZmZmaR8B66AAAAAAAhpSBlH2UKGiMaI5HAAAAAAAAAABHAAAAAAAAAACGlIGUaJFojkcAAAAAAAAAAEcAAAAAAAAAAIaUgZRolE5olU5olksDdWJoiEfAZhMzMzMzM0cAAAAAAAAAAIaUgZR9lChojGiORwAAAAAAAAAARwAAAAAAAAAAhpSBlGiRaI5HAAAAAAAAAABHAAAAAAAAAACGlIGUaJROaJVOaJZLBHViaIhHwC5MzMzMzM1HAAAAAAAAAACGlIGUfZQoaIxojkcAAAAAAAAAAEcAAAAAAAAAAIaUgZRokWiORwAAAAAAAAAARwAAAAAAAAAAhpSBlGiUTmiVTmiWSwV1YmWMBl9wb2xlc5RdlChoiEfAyfoAAAAAAEcAAAAAAAAAAIaUgZR9lChojGiORwAAAAAAAAAARwAAAAAAAAAAhpSBlGiRaI5HAAAAAAAAAABHAAAAAAAAAACGlIGUaJROaJVOaJZLAHViaIhHwMSRAAAAAABHQMOhAAAAAACGlIGUfZQoaIxojkcAAAAAAAAAAEcAAAAAAAAAAIaUgZRokWiORwAAAAAAAAAARwAAAAAAAAAAhpSBlGiUTmiVTmiWSwF1YmiIR8DEkQAAAAAAR8DDoQAAAAAAhpSBlH2UKGiMaI5HAAAAAAAAAABHAAAAAAAAAACGlIGUaJFojkcAAAAAAAAAAEcAAAAAAAAAAIaUgZRolE5olU5olksCdWJoiEfAgEJmZmZmZkcAAAAAAAAAAIaUgZR9lChojGiORwAAAAAAAAAARwAAAAAAAAAAhpSBlGiRaI5HAAAAAAAAAABHAAAAAAAAAACGlIGUaJROaJVOaJZLA3ViaIhHwHdszMzMzM1HAAAAAAAAAACGlIGUfZQoaIxojkcAAAAAAAAAAEcAAAAAAAAAAIaUgZRokWiORwAAAAAAAAAARwAAAAAAAAAAhpSBlGiUTmiVTmiWSwR1YmiIR8BYVcKPXCj2R0B5CzMzMzMzhpSBlH2UKGiMaI5HAAAAAAAAAABHAAAAAAAAAACGlIGUaJFojkcAAAAAAAAAAEcAAAAAAAAAAIaUgZRolE5olU5olksFdWJoiEfAWFXCj1wo9kfAeQszMzMzM4aUgZR9lChojGiORwAAAAAAAAAARwAAAAAAAAAAhpSBlGiRaI5HAAAAAAAAAABHAAAAAAAAAACGlIGUaJROaJVOaJZLBnViaIhHwC9HrhR64UhHAAAAAAAAAACGlIGUfZQoaIxojkcAAAAAAAAAAEcAAAAAAAAAAIaUgZRokWiORwAAAAAAAAAARwAAAAAAAAAAhpSBlGiUTmiVTmiWSwd1YmiIR7+i8an752yLRz+i8an752yLhpSBlH2UKGiMaI5HAAAAAAAAAABHAAAAAAAAAACGlIGUaJFojkcAAAAAAAAAAEcAAAAAAAAAAIaUgZRolE5olU5olksIdWJoiEe/ovGp++dsi0e/ovGp++dsi4aUgZR9lChojGiORwAAAAAAAAAARwAAAAAAAAAAhpSBlGiRaI5HAAAAAAAAAABHAAAAAAAAAACGlIGUaJROaJVOaJZLCXViaIhHwG/jMzMzMzNHAAAAAAAAAACGlIGUfZQoaIxojkcAAAAAAAAAAEcAAAAAAAAAAIaUgZRokWiORwAAAAAAAAAARwAAAAAAAAAAhpSBlGiUTmiVTmiWSwp1YmWMFXN0YWdlX3NlcXVlbmNlX251bWJlcpRLAWhpjANNL1OUaG1oRGhrjB12ZWxvY2l0eSBpbiBtZXRlcnMgcGVyIHNlY29uZJRob4wMZW1mIGluIHZvbHRzlGhXTowMcmVzb3VyY2VfaWQylE6MCnN0YWdlX2dhaW6UR0CXXVwo9cKPjBRzdGFnZV9nYWluX2ZyZXF1ZW5jeZRHP8mZmZmZmZqMBG5hbWWUTmhNTowcZGVjaW1hdGlvbl9pbnB1dF9zYW1wbGVfcmF0ZZROjBFkZWNpbWF0aW9uX2ZhY3RvcpROjBFkZWNpbWF0aW9uX29mZnNldJROjBBkZWNpbWF0aW9uX2RlbGF5lE6MFWRlY2ltYXRpb25fY29ycmVjdGlvbpROdWJoXYwdQ29lZmZpY2llbnRzVHlwZVJlc3BvbnNlU3RhZ2WUk5QpgZR9lCiMGl9jZl90cmFuc2Zlcl9mdW5jdGlvbl90eXBllIwHRElHSVRBTJSMCl9udW1lcmF0b3KUXZSMDF9kZW5vbWluYXRvcpRdlGoJAQAASwJoaWhEaG2MBkNPVU5UU5Roa4wMZW1mIGluIHZvbHRzlGhvjA5kaWdpdGFsIGNvdW50c5RoV05qDQEAAE5qDgEAAEdBGZmYAAAAAGoPAQAARz/JmZmZmZmaahABAABOaE1OahEBAABof0dARAAAAAAAAIWUgZR9lChoD05oEE5oEU51YmoSAQAASwFqEwEAAEsAahQBAABohowdRmxvYXRXaXRoVW5jZXJ0YWludGllc0FuZFVuaXSUk5RHAAAAAAAAAACFlIGUfZQoaA9OaBBOaBFOaB5OdWJqFQEAAGonAQAARwAAAAAAAAAAhZSBlH2UKGgPTmgQTmgRTmgeTnVidWJqFwEAACmBlH2UKGoaAQAAahsBAABqHAEAAF2UKGhdjBxDb2VmZmljaWVudFdpdGhVbmNlcnRhaW50aWVzlJOURz1HhtyoGqtOhZSBlH2UKGgPjAMwLjCUaBCMAzAuMJRoEU6MB19udW1iZXKUTnViajIBAABHPqFz4fdF5RmFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHvtNMnnZFqiCFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHPt0GedhlXKiFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHPz3cZ30aZZCFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHP0sboqZmOU6FlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHv2zg9tBWbyKFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHP1UMwgVIlEOFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHPxGqikg8BZ+FlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHv3PPrdK2m7OFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHP4TAS7UKOGiFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHv46al8ZawsSFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHP5AQfSXFjGeFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHv4N9VD6bnCiFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHv3gsz5bWm66FlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHP59eIEbHZK6FlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHv6/ByxEK6j+FlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHP7hXE+ML03OFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHv8CfWcz67/aFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHP8JyZKFqSHOFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHP+tGXHBDXvqFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHP8SSvC/GlymFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHv8FOdoWYWtWFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHP7iTjebd8AuFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHv69JpxL+4EWFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHP52Vf1ils4OFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHv3EJL1OqogCFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHv4YmLLpzLfWFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHP5DLARqyMu6FlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHv48AW2+SXxOFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHP4R33C+WRaeFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHv3Kv6xS2cGqFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHvygoYkE1IOuFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHP1fZ5mXQE36FlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHv21K2W0ZMwKFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHP0nHhxKuWG2FlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHPz9LY5py9zKFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHPuDV4w5pN5qFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViajIBAABHvtLtHtIqOzGFlIGUfZQoaA+MAzAuMJRoEIwDMC4wlGgRTmo4AQAATnViZWoeAQAAXZRqCQEAAEsDaGmMBkNPVU5UU5RobYwGQ09VTlRTlGhrjA5kaWdpdGFsIGNvdW50c5Rob4wOZGlnaXRhbCBjb3VudHOUaFdOag0BAABOag4BAABHP/AAAAAAAABqDwEAAEc/yZmZmZmZmmoQAQAATmhNTmoRAQAAaH9HQEQAAAAAAACFlIGUfZQoaA9OaBBOaBFOdWJqEgEAAEsBahMBAABLAGoUAQAAaicBAABHP+AAAAAAAACFlIGUfZQoaA9OaBBOaBFOaB5OdWJqFQEAAGonAQAARz/gAAAAAAAAhZSBlH2UKGgPTmgQTmgRTmgeTnVidWJldWKMBV9jb2RllIwDQkhFlIwIY29tbWVudHOUXZRoCIwHQ29tbWVudJSTlCmBlH2UKIwGX3ZhbHVllIwWOTAzNDUgMDEwMDAwMERCMEJDQzhFQ5SMFV9iZWdpbl9lZmZlY3RpdmVfdGltZZROjBNfZW5kX2VmZmVjdGl2ZV90aW1llE6MCF9hdXRob3JzlF2UjANfaWSUTowHc3ViamVjdJROdWJhaE1OjApzdGFydF9kYXRllIwWb2JzcHkuY29yZS51dGNkYXRldGltZZSMC1VUQ0RhdGVUaW1llJOUKYGUfZQojBdfVVRDRGF0ZVRpbWVfX3ByZWNpc2lvbpRLBowQX1VUQ0RhdGVUaW1lX19uc5SKCAAAXLIafHYRjAxfaW5pdGlhbGl6ZWSUiHVijAhlbmRfZGF0ZZRqGAIAACmBlH2UKGobAgAASwZqHAIAAIoIANgbNoyMQRJqHQIAAIh1YowRcmVzdHJpY3RlZF9zdGF0dXOUjARvcGVulIwPX2FsdGVybmF0ZV9jb2RllE6MEF9oaXN0b3JpY2FsX2NvZGWUTowRZGF0YV9hdmFpbGFiaWxpdHmUTowMX2lkZW50aWZpZXJzlF2UjApfc291cmNlX2lklE51Yi4=\",\n",
      "      \"subType\": \"00\"\n",
      "    }\n",
      "  },\n",
      "  \"chan_id\": {\n",
      "    \"$oid\": \"65e322fb5cd7e1e9490a591c\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Note find_one accepts the same query but returns \n",
    "# only the first one it finds\n",
    "doc=db.channel.find_one(query)\n",
    "pretty_print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed99677f-6153-4770-a256-c816f3c247e6",
   "metadata": {},
   "source": [
    "As you can see the attribute \"serialized_channel_data\" is huge and creates volumious output.   The reason is that it is a pickle format image of the raw \"Inventory\" record for that channel created by obspy's web service reader.  This example shows the common problem that documents can be too big to view with simple json_util dumps or a raw print.   For that reason it is often useful to specify a \"projection\" argument.   Here is an example where we extract and print only net, sta, chan, loc from each of the 3 channel documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fbe1fa6-ecb1-4943-aff4-9ae505f01ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loc': '', 'net': 'TA', 'sta': '134A', 'chan': 'BHE'}\n",
      "{'loc': '', 'net': 'TA', 'sta': '134A', 'chan': 'BHN'}\n",
      "{'loc': '', 'net': 'TA', 'sta': '134A', 'chan': 'BHZ'}\n"
     ]
    }
   ],
   "source": [
    "projection={'net':1,'sta':1,'chan':1,'loc':1,'_id':0}\n",
    "cursor=db.channel.find(query,projection)\n",
    "for doc in cursor:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c899b792-7589-44d6-8283-3504a58d686d",
   "metadata": {},
   "source": [
    "Here is a fancier variant using pandas to print a longer list of attributes in tabular form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bdc6a8b-98cc-41a9-bccf-e4bb59aa258c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  net   sta        lat        lon   elev chan  vang  hang\n",
      "0  TA  134A  32.572899 -98.079498  0.297  BHE  90.0  90.7\n",
      "1  TA  134A  32.572899 -98.079498  0.297  BHN  90.0   0.7\n",
      "2  TA  134A  32.572899 -98.079498  0.297  BHZ   0.0   0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "projection={\n",
    "    'net':1,\n",
    "    'sta':1,\n",
    "    'chan':1,\n",
    "    'lat':1,\n",
    "    'lon':1,\n",
    "    'elev':1,\n",
    "    'hang':1,\n",
    "    'vang':1,\n",
    "    '_id':0,\n",
    "}\n",
    "cursor=db.channel.find(query,projection)\n",
    "doclist=[]\n",
    "for doc in cursor:\n",
    "    doclist.append(doc)\n",
    "df = pd.DataFrame.from_dict(doclist)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e07f977-41da-471e-ab51-e1e143b66dbb",
   "metadata": {},
   "source": [
    "The pandas construct is useful for a number of reasons.  Therefore, let's create a function to simplify that type of printing operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48087e04-4364-4812-94d3-96a0a5c998b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def print_as_table(doclist):\n",
    "    df = pd.DataFrame.from_dict(doclist)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcea1cb2-6563-4a08-946d-bf4cca87480e",
   "metadata": {},
   "source": [
    "#### Multiple key equality matching\n",
    "Next let's do a query with multiple keys.   We will fetch the (shortened) record for the BHN component of a different station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "828b579d-6c52-4d55-a176-85f648f38bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"131A\",\n",
      "  \"lat\": 32.673698,\n",
      "  \"lon\": -100.388802,\n",
      "  \"elev\": 0.622,\n",
      "  \"chan\": \"BHZ\",\n",
      "  \"vang\": 0.0,\n",
      "  \"hang\": 0.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "query={\n",
    "    'sta' : '131A',\n",
    "    'chan' : 'BHZ',\n",
    "}\n",
    "cursor=db.channel.find(query,projection)\n",
    "for doc in cursor:\n",
    "    pretty_print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f854e2a1-6c7d-43fa-a9cb-f8fd69c38e78",
   "metadata": {},
   "source": [
    "#### Range operator examples (compound query)\n",
    "We often want to query by a range of values.  Here is an example that returns the coordinates of all TA stations within a 5 degree box defined by 30 to 35 latitude and -110 to -100 longitude: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3294b296-df65-405b-aaa0-c308a0fa81a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   net   sta        lat         lon   elev\n",
      "0   TA  121A  32.532398 -107.785103  1.652\n",
      "1   TA  130A  32.596100 -100.965202  0.676\n",
      "2   TA  131A  32.673698 -100.388802  0.622\n",
      "3   TA  230A  31.887800 -101.112396  0.742\n",
      "4   TA  231A  31.935301 -100.316299  0.574\n",
      "5   TA  330A  31.406300 -101.175201  0.742\n",
      "6   TA  331A  31.308500 -100.426598  0.615\n",
      "7   TA  431A  30.682400 -100.607903  0.700\n",
      "8   TA  530A  30.148899 -101.337898  0.636\n",
      "9   TA  531A  30.164499 -100.546402  0.661\n",
      "10  TA  MSTX  33.969601 -102.772400  1.167\n",
      "11  TA  X30A  34.446098 -100.874001  0.698\n",
      "12  TA  Y22D  34.073900 -106.921000  1.436\n",
      "13  TA  Y22E  34.074200 -106.920799  1.444\n",
      "14  TA  Y22E  34.074200 -106.920799  1.444\n",
      "15  TA  Y30A  33.876598 -100.897797  0.812\n",
      "16  TA  Y31A  33.962898 -100.261497  0.530\n",
      "17  TA  Z30A  33.286098 -101.128197  0.729\n",
      "18  TA  Z31A  33.318298 -100.143501  0.547\n"
     ]
    }
   ],
   "source": [
    "query={\n",
    "    'lat' : {'$gte' : 30.0,'$lte' : 35.0},\n",
    "    'lon' : {'$gte' : -110.0, '$lte' : -100},\n",
    "}\n",
    "projection={\n",
    "   'net':1,\n",
    "    'sta':1,\n",
    "    'chan':1,\n",
    "    'lat':1,\n",
    "    'lon':1,\n",
    "    'elev':1,\n",
    "    '_id':0, \n",
    "}\n",
    "cursor=db.site.find(query,projection)\n",
    "doclist=[]\n",
    "for doc in cursor:\n",
    "    doclist.append(doc)\n",
    "print_as_table(doclist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bd5907-bf51-4275-ae47-233680b12d63",
   "metadata": {},
   "source": [
    "A variant using a regular expression to only select station names that start with the latter \"Y\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eace96aa-0b9f-48e0-9ef1-f4fcb20db801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  net   sta        lat         lon   elev\n",
      "0  TA  Y22D  34.073900 -106.921000  1.436\n",
      "1  TA  Y22E  34.074200 -106.920799  1.444\n",
      "2  TA  Y22E  34.074200 -106.920799  1.444\n",
      "3  TA  Y30A  33.876598 -100.897797  0.812\n",
      "4  TA  Y31A  33.962898 -100.261497  0.530\n"
     ]
    }
   ],
   "source": [
    "query={\n",
    "    'lat' : {'$gte' : 30.0,'$lte' : 35.0},\n",
    "    'lon' : {'$gte' : -110.0, '$lte' : -100},\n",
    "    'sta' : {'$regex' : 'Y.*'},\n",
    "}\n",
    "cursor=db.site.find(query,projection)\n",
    "doclist=[]\n",
    "for doc in cursor:\n",
    "    doclist.append(doc)\n",
    "print_as_table(doclist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d195c91-52e1-443b-b824-68db10a125f5",
   "metadata": {},
   "source": [
    "#### Geospatial query\n",
    "MongoDB has some very useful geospatial query capabilities.  See the \"MongoDB and MsPASS\" section of the User's Manual for more about this capability.  On the other hand, it is probably best thought of, at least at present, as an advanced feature.   The syntax is complex and, as noted in that section of the manual, MongoDB documentation is less than ideal and many online sources are inconsistent with the current implementation.  For this tutorial I will just show an example that is a variant of that shown in User's Manual page.\n",
    "\n",
    "An IMPORTANT rule about using geospatial searches is that a special index is REQUIRED.  For this example the following is needed to make this work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e7dc365-e621-4598-9921-c5f9925a30dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'location_2dsphere'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.site.create_index({'location' : '2dsphere'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d6bf6b-a8f6-4730-8728-0ac6e8c880f9",
   "metadata": {},
   "source": [
    "Noting:\n",
    "1.  'location' is the key used to tag the geoJSON format documents `save_inventory` created in the site collection.  It is a constant tag in the MsPASS schema for these data.  Note also that if you were running this on the source collection the key has a different name ('epicenter') since the content exactly matches the definition of the jargon term. \n",
    "2. '2dsphere' is a magic string that tells MongoDB to create a special index that uses spherical geometry for spatial calculations.  The alternative is '2d' but the alternative is not advised for most if not all seismology applications.  The '2d' index uses a map projection that produces distorted answers unless the area of study is small. Examples you can find online use a '2d' index for applications like apps that are have data only on a single city.\n",
    "\n",
    "Now that we have an index, we can do a search.  This search produces a similar result to the lat-lon range query above but for a circular (great circle path distance circle that is) region at the center of the same lat-lon box as above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74a6709e-71ae-458f-8f00-022d8c7366f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"65e322fe5cd7e1e9490a608d\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"Y22D\",\n",
      "  \"lat\": 34.0739,\n",
      "  \"lon\": -106.921,\n",
      "  \"coords\": [\n",
      "    -106.921,\n",
      "    34.0739\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -106.921,\n",
      "      34.0739\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.436,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1191024000.0,\n",
      "  \"endtime\": 1575158399.9998999,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"65e322fe5cd7e1e9490a608d\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"65e322fe5cd7e1e9490a6099\"\n",
      "  },\n",
      "  \"loc\": \"01\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"Y22E\",\n",
      "  \"lat\": 34.0742,\n",
      "  \"lon\": -106.920799,\n",
      "  \"coords\": [\n",
      "    -106.920799,\n",
      "    34.0742\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -106.920799,\n",
      "      34.0742\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.444,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1301270400.0,\n",
      "  \"endtime\": 1344297599.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"65e322fe5cd7e1e9490a6099\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"65e322fe5cd7e1e9490a6090\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"Y22E\",\n",
      "  \"lat\": 34.0742,\n",
      "  \"lon\": -106.920799,\n",
      "  \"coords\": [\n",
      "    -106.920799,\n",
      "    34.0742\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -106.920799,\n",
      "      34.0742\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.444,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1301270400.0,\n",
      "  \"endtime\": 1344297599.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"65e322fe5cd7e1e9490a6090\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"65e322fb5cd7e1e9490a5910\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"121A\",\n",
      "  \"lat\": 32.532398,\n",
      "  \"lon\": -107.785103,\n",
      "  \"coords\": [\n",
      "    -107.785103,\n",
      "    32.532398\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -107.785103,\n",
      "      32.532398\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.652,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1202774400.0,\n",
      "  \"endtime\": 1552089599.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"65e322fb5cd7e1e9490a5910\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"65e322fc5cd7e1e9490a5d96\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"MSTX\",\n",
      "  \"lat\": 33.969601,\n",
      "  \"lon\": -102.7724,\n",
      "  \"coords\": [\n",
      "    -102.7724,\n",
      "    33.969601\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -102.7724,\n",
      "      33.969601\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.167,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1208908800.0,\n",
      "  \"endtime\": 1534377599.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"65e322fc5cd7e1e9490a5d96\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "query = {\"location\":{\n",
    "        '$nearSphere': {\n",
    "            '$geometry' : {\n",
    "                'type' : 'Point',\n",
    "                'coordinates' : [-105.0,32.5]\n",
    "            },\n",
    "            '$maxDistance' : 300000.0,\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "# A flaw in the current MongoDB implementation is\n",
    "# count_documents seems to not work with any geospatial \n",
    "# query.  If you remove this comment you will see \n",
    "# the error it throws.  If it works, it means MongoDB \n",
    "# developers fixed the problem\n",
    "#n=db.site.count_documents(query)\n",
    "cursor=db.site.find(query)\n",
    "for doc in cursor:\n",
    "    pretty_print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87c61c3-d365-4f47-966a-1df933e19ce5",
   "metadata": {},
   "source": [
    "Because of the pretty print of the full documents, that is a bit verbose, but it hopefully illustrates the point.  Although geospatial queries are complex, they have a lot of potential use for workflows that need to group data by the spatial location of stations (a \"virtual array\" concept) or by source (stacking of closely spaced sources).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a87b98-bd2c-461d-a784-54e5f7ae4451",
   "metadata": {},
   "source": [
    "### Sorting\n",
    "There are many situations where it is advantageous to \n",
    "sort the return of a query by one or more keys.   Sorting is technically a \"method of the CommandCursor object\" returned by a query but more magic happens when the client passes the query to the MongoDB server to assure the operation is done efficiently.   The reason I point that out here is mostly to clarify why the sort clause appears where it does in typical usage.  The User Manual addresses this in more detail, but here is an example that sorts \n",
    "channel documents to a form sensible for miniseed that \n",
    "uses the net:sta:chan:loc:time-interval as a unique \n",
    "key combination.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78189b0c-e2db-4dfb-b91e-e3c862ac5cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sta                    starttime                      endtime chan\n",
      "0  034A  2010-01-08T00:00:00.000000Z  2011-11-17T17:05:00.000000Z  BHE\n",
      "1  034A  2010-01-08T00:00:00.000000Z  2011-11-17T17:05:00.000000Z  BHN\n",
      "2  034A  2010-01-08T00:00:00.000000Z  2011-11-17T17:05:00.000000Z  BHZ\n",
      "3  035A  2010-01-12T00:00:00.000000Z  2011-11-14T17:40:00.000000Z  BHE\n",
      "4  035A  2010-01-12T00:00:00.000000Z  2011-11-14T17:40:00.000000Z  BHN\n",
      "5  035A  2010-01-12T00:00:00.000000Z  2011-11-14T17:40:00.000000Z  BHZ\n"
     ]
    }
   ],
   "source": [
    "# this is a test to verify sort syntax - delete when completed\n",
    "filter_clause = {\n",
    "    \"_id\":0,\n",
    "    \"sta\":1,\n",
    "    \"chan\":1,\n",
    "    \"starttime\":1,\n",
    "    \"endtime\":1,\n",
    "}\n",
    "sort_clause = [\n",
    "    (\"net\",1),\n",
    "    (\"sta\",1),\n",
    "    (\"chan\",1),\n",
    "    (\"starttime\",1),\n",
    "  ]\n",
    "cursor=db.channel.find({},filter_clause).sort(sort_clause).limit(6)\n",
    "doclist=[]\n",
    "for doc in cursor:\n",
    "    doclist.append(doc)\n",
    "from obspy import UTCDateTime\n",
    "for doc in doclist:\n",
    "    doc['starttime']=UTCDateTime(doc['starttime'])\n",
    "    doc['endtime']=UTCDateTime(doc['endtime'])\n",
    "print_as_table(doclist)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca2bde8-69c8-48f1-ab6f-b7bec85d4c65",
   "metadata": {},
   "source": [
    "Noting:\n",
    "1.  The \"sort\" function call appears after the find function with arguments.   That is the syntax because \"sort\" is a Cursor \"method\".\n",
    "2.  I added a second qualifier, limit, to only return the first 6 documents.  I did that just to keep the volume of the output under control.   The number return is much larger if you remove the `.limit(6)` qualifier.\n",
    "3.  I did a projection and used the `print_as_table` function we defined to make a more readable report. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033b4822-6f41-4deb-987b-2a69313094a8",
   "metadata": {},
   "source": [
    "## Update\n",
    "One has to do an \"update\" to a MongoDB database if you need to change the contents of one or more documents.  Database updates happen in the modern world in inconceivably huge numbers every day in commericial operations.  e.g. if you order something from Amazon all those tracking stages from your clicking history to the time a package is delivered to your home invoke a series of database transactions including, I presume, a lot of updates.  \n",
    "\n",
    "Although updates are a common requirement in commercial databases, a less obvious thing to most people is that updates are rarely if ever needed in data processing with a system like MsPASS.   Most data processing involves three stages:  1) read the data set, 2) process the data set, and 3) save the results.   Some processors may need to do read operations from the database, but updates are rarely needed.  They are also highly undesirable in a data-driven workflow like that because database transactions, from the computer's perspective, are like a human talking to someone on Jupiter; a response to the request for an update takes forever in terms of computer clock cycles.  For that reason, updates should be avoided in any workflow and should absolutely never be embedded in a large, parallel processing sequence. \n",
    "\n",
    "In MsPASS updates can nearly always be avoided by a simple, alternative approach:   if a change is needed that needs to be saved (e.g. you compute a set of new attributes from the data) simply post that data to the associated object's `Metadata` container.   In that model, when the final results are saved the newly computed attributes will be saved with the data.  Then the overhead of writing to the database is absorbed in the normally essential save step anyway.  \n",
    "\n",
    "With that long caveat, there are two standard ways to do updates:  `update_one` changes one document at a time, and `update_many` updates multiple documents with one client-server transaction.  Most people can understand usage of these two methods better by examples.  The examples below focus on updates to \"normalizing\" collections as that, from my experience, is the most common need for updates when using MsPASS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7c6dd2-6bed-4ff4-a4e3-fdf1b0003639",
   "metadata": {},
   "source": [
    "### update_one example\n",
    "Suppose we learned that the recording period for a seismic station are wrong.  That is, with SEED data station information has a time period for which the data are considered valid.   That period is defined by two attributes with the keys \"starttime\" and \"endtime\"  Changing these fields would be highly unusual for data downloaded from the FDSN, but is not at all uncommon for portable deployments while the experiment is in progress.  Our example is contrived as what we are about to do will make the entry we edit wrong.   So the hypothetical situation we are modeling is that we imagine we learned we the \"endtime\" for station O34A is wrong.  We first query the site collection to verify what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68d6d8f8-0eae-4605-a3e1-45871e6780d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents for station O34A =  1\n",
      "O34A 2010-06-11T00:00:00.000000Z 2012-04-18T23:59:59.000000Z\n"
     ]
    }
   ],
   "source": [
    "from obspy import UTCDateTime\n",
    "query={'sta' : 'O34A'}\n",
    "# verify there is only one entry - not always true with this query\n",
    "ndocs=db.site.count_documents(query)\n",
    "print('Number of documents for station O34A = ',ndocs)\n",
    "doc=db.site.find_one(query)\n",
    "print(doc['sta'],\n",
    "    UTCDateTime(doc['starttime']), UTCDateTime(doc['endtime']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97d575f-583b-4eed-8eb8-75de29ffd722",
   "metadata": {},
   "source": [
    "We say, \"ahh the endtime should have been on March 19 not March 18 and our field notes show the actual time was 13:44 UTC. \"   We can make that change with this use of update one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a6f88d4-091d-4654-9538-80c724d3d324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated data for O34A\n",
      "O34A 2010-06-11T00:00:00.000000Z 2012-04-19T13:44:00.000000Z\n"
     ]
    }
   ],
   "source": [
    "new_time=UTCDateTime('2012-04-19T13:44:00.0Z')\n",
    "update_doc={ '$set' :\n",
    "            {'endtime' : new_time.timestamp}\n",
    "           }\n",
    "db.site.update_one(query,update_doc)\n",
    "print('Updated data for O34A')\n",
    "doc=db.site.find_one(query)\n",
    "print(doc['sta'],\n",
    "    UTCDateTime(doc['starttime']), UTCDateTime(doc['endtime']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e75e52-7a6b-4900-9867-48e109f2e2d4",
   "metadata": {},
   "source": [
    "Notice update_one has two required arguments: arg0 is a query operator and arg1 is required to be an 'operator' meaning in has to use one of the 'dollar' operators discussed above.  This one uses '$set' with means replace the value.  In my experience, that is the most common operator for updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea967de6-3bff-4458-8c97-8fac0740d19a",
   "metadata": {},
   "source": [
    "### update_many example\n",
    "The basic argument structure required for `update_many` is the same as `update_one`.   The difference is you should use `update_many` when the query in arg0 is expected to return more than one document that are to be modified.  The example below is the same as  for `update_one` but applied to the \"channel\" collection.   As the `count_documents` output shows below the same query yields 3 documents for channel because the site has a three component sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "109cc173-033c-455e-9ebd-d30a9438a669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of channel documents for O34A= 3\n",
      "Updated data for O34A\n",
      "O34A BHE 2010-06-11T00:00:00.000000Z 2012-04-19T13:44:00.000000Z\n",
      "O34A BHN 2010-06-11T00:00:00.000000Z 2012-04-18T15:20:00.000000Z\n",
      "O34A BHZ 2010-06-11T00:00:00.000000Z 2012-04-18T15:20:00.000000Z\n"
     ]
    }
   ],
   "source": [
    "ndocs=db.channel.count_documents(query)\n",
    "print('number of channel documents for O34A=',ndocs)\n",
    "# we use the same query and update_doc as above\n",
    "db.channel.update_one(query,update_doc)\n",
    "print('Updated data for O34A')\n",
    "cursor=db.channel.find(query)\n",
    "for doc in cursor:\n",
    "    print(doc['sta'],doc['chan'],\n",
    "      UTCDateTime(doc['starttime']), \n",
    "      UTCDateTime(doc['endtime']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772c4d65-c431-42d4-a521-c413a959d898",
   "metadata": {},
   "source": [
    "## Delete\n",
    "The API for deleting documents is very similar to that for find.  There is a `delete_one` method to delete a single document and a `delete_many` method that more-or-less does a find followed by deleting each document the query found.  For instance, the following deletes what we just updated in channel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d641966-3dd2-4493-8215-7291e06dae17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of channel documents for O34A before delete= 3\n",
      "number of channel documents for O34A after delete_many= 0\n"
     ]
    }
   ],
   "source": [
    "# repeating this query to be clear but not required in this context\n",
    "query={'sta' : 'O34A'}\n",
    "ndocs=db.channel.count_documents(query)\n",
    "print('number of channel documents for O34A before delete=',ndocs)\n",
    "ret=db.channel.delete_many(query)\n",
    "ndocs=db.channel.count_documents(query)\n",
    "print('number of channel documents for O34A after delete_many=',ndocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489e35e4-2450-44a7-8233-985a00ed4f37",
   "metadata": {},
   "source": [
    "Handling deletions of waveform data is a much more difficult problem.   In MsPASS there is a special method of our `Database` class called `delete_data`.  That method has to do a lot more than just call the `delete_one` method to remove the database document.  There are two reasons for that:\n",
    "1.  In MsPASS the sample data, which are typically orders of magnitude larger than the \"document\" saved in MongoDB, are stored separately from the \"document\" of name-value pairs.\n",
    "2.  MsPASS also support multiple \"storage modes\" for how to handle the sample data.   It also allow multiple \"format\"s for how that data is represented externally (e.g. miniseed is a \"format\" that is light years from the natural representation of seismic data). At this time there are three basic \"storage modes\":  (1) \"file\", (2) \"gridfs\", and \"url\".  How they need to be handled with a \"delete\" operation is very different.  When \"storage_mode\" is set to \"file\" the sample data are stored in a file system in a set of files.  There the problem is one file should normally contain many waveforms so if a lot of editing is done data will be stranded.  MsPASS has a way to automatically delete files that no longer contain a reference in the database to reduce debris, but it only works if the entire file content is deleted.   Using \"gridfs\" storage is a simpler problem as our waveform delete operator will automatically clear sample data stored in the gridfs system.  If your application requires a lot of editing to remove stale waveforms, gridfs is by far the best choice.  Finally, \"URL\" is pretty much defined to be read-only so the only thing that happens for data indexed that way is that the document vanishes. For data access via the cloud with the new Earthscope system this mode may become common.     \n",
    "\n",
    "One common application of `delete_data` is to clear some temporary save copy that is no longer needed.  In MsPASS when data are saved we recommend ALWAYS using the \"data_tag\" argument to provide a unique tag for data at a specific stage of processing.   With that understand, suppose we saved an intermediate copy of a working dataset with the `data_tag=\"preprocessed\"` and we wanted to clear the disk space associated with that intermediate copy.  The following simple code box would do that (Note it will do nothing here because the db we have been using contains no waveform data so I disabled the code box):  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "50a91782-6463-4b75-a491-96f6ef4a5a77",
   "metadata": {},
   "source": [
    "query={'data_tag' : 'preprocessed'}\n",
    "cursor = db.wf_TimeSeries.find(query)\n",
    "for doc in cursor:\n",
    "    db.delete_data(doc['_id'],\"TimeSeries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe12f2de-4ef0-49eb-88fc-632d924bab49",
   "metadata": {},
   "source": [
    "Note arg0 of this method (currently) requires the ObjectId of the document to be deleted.  arg1 must be either \"TimeSeries\" or \"Seismogram\" or the method will throw an exception."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4561cd-1ade-4f84-9115-aedf9b4fb709",
   "metadata": {},
   "source": [
    "## Create part 2:  data import\n",
    "### Import PhaseNet picks\n",
    "A very common need in MsPASS or any project where you want to utilize MongoDB is the need to import data in some standard or weird format and put it into a form you can manage with MongoDB.   I'll close this tutorial with two examples.  The first reads data in a standard format called \"comma separated value (csv)\".  The second is a good example of a weird (aka clumsy) legacy format that has been around for decades and exists only because of inertia. \n",
    "\n",
    "Our example with a stardard format is the output of a newer package called __[PhaseNet](https://github.com/AI4EPS/PhaseNet)__.  PhaseNet uses a neural net for picking seismic phases.  We will be cracking an output file from this package found in the data directory where you are assumed to have run this tutorial.  (Thanks to __[Jianhua Gong](https://earth.indiana.edu/directory/faculty/gong-ginny.html)__ from supplying these data.)  The data are a standard \"csv\" format file and are relatively easy to read pandas.   This next box loads the file of picks into memory as a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cefeca52-0836-4a61-851a-32e48aa15afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           file_name               begin_time  \\\n",
      "0  X9.BB060..HH*__20120919T000000Z__20120920T0000...  2012-09-19T00:00:00.000   \n",
      "1  X9.BB060..HH*__20120919T000000Z__20120920T0000...  2012-09-19T00:00:00.000   \n",
      "2  X9.BB060..HH*__20120919T000000Z__20120920T0000...  2012-09-19T00:00:00.000   \n",
      "3  X9.BB060..HH*__20120919T000000Z__20120920T0000...  2012-09-19T00:00:00.000   \n",
      "\n",
      "                                          station_id  phase_index  \\\n",
      "0  X9.BB060..HH*__20120919T000000Z__20120920T0000...      1390027   \n",
      "1  X9.BB060..HH*__20120919T000000Z__20120920T0000...      4120059   \n",
      "2  X9.BB060..HH*__20120919T000000Z__20120920T0000...      4123674   \n",
      "3  X9.BB060..HH*__20120919T000000Z__20120920T0000...      4252371   \n",
      "\n",
      "                phase_time  phase_score phase_type  \n",
      "0  2012-09-19T03:51:40.270        0.337          P  \n",
      "1  2012-09-19T11:26:40.590        0.867          P  \n",
      "2  2012-09-19T11:27:16.740        0.417          P  \n",
      "3  2012-09-19T11:48:43.710        0.828          P  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/picks.csv')\n",
    "print(df[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af29efd-d9e1-4b33-ae22-2f5c81d2c217",
   "metadata": {},
   "source": [
    "We used pandas in kind of the reverse way before to reformat MongoDB documents to a more easily read form.  The print above illustrates that.   We are now, however, trying to do the the inverse operation; saving the contents of this DataFrame to MongoDB.   There is, however, a very easy way to do that.  The first step, illustrated in the next box, is to convert the DataFrame to something MongoDB can digest.  The DataFrame API has a stock way to do that with it's `to_dict` method illustrated in the box below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75bc1740-5723-4ba8-8a54-56008f8cbf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of to_dict output= <class 'list'>  size= 35917\n",
      "First document in output of to_dict\n",
      "{\n",
      "  \"file_name\": \"X9.BB060..HH*__20120919T000000Z__20120920T000000Z.mseed\",\n",
      "  \"begin_time\": \"2012-09-19T00:00:00.000\",\n",
      "  \"station_id\": \"X9.BB060..HH*__20120919T000000Z__20120920T000000Z.mseed\",\n",
      "  \"phase_index\": 1390027,\n",
      "  \"phase_time\": \"2012-09-19T03:51:40.270\",\n",
      "  \"phase_score\": 0.337,\n",
      "  \"phase_type\": \"P\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "doclist=df.to_dict('records')\n",
    "print('type of to_dict output=',type(doclist),' size=',len(doclist))\n",
    "print('First document in output of to_dict')\n",
    "pretty_print(doclist[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aadd590-44d6-461e-b205-19d0ac110f06",
   "metadata": {},
   "source": [
    "The print illustrates `to_dict` converts the DataFrame to a python list of dictionaries.   We print the first component of the list to show what it contains.  \n",
    "\n",
    "It is good practice at this point to release the DataFrame.  We wouldn't have to do that, but if the file were large it could be an issue so it is best to do a bit of housecleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b442c6c9-078c-43e4-a928-becbbd72f3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e091e11-9f49-4abc-98cd-02765c9c521a",
   "metadata": {},
   "source": [
    "This particular example illustrates the nearly universal problem that data imported from anything independent from your own work has some kind of mismatch with your needs.   The data we just imported are phase pick estimates from a package called \"PhaseNet\" that used a neural net to \"pick\" P and S phases arrival times.   A detail of earthquake catalog preparation, of which \"picking\" is a basic operation, is that each \"pick\" is defined by the channel of seismic data from which it was derived.   Ultimately that channel need to be associated with a particular instrument with a known location on the earth for it to be used for an earthquake location (a primary purpose of \"picks\").  This particular implementation does that particularly weird way with the attribute seen in the DataFrame and conversion above with the key `station_id`.   (\"Weird\", by the way, is the norm with any research application that hasn't had a standard applied.)  The SEED standard has set how this supposed to be done for something like 30 years.  All we really need are \"seed station code names\" and a time to match each arrival the the full set of metadata for a given seismic station.  A full \"seed station code\" consists of four keywords commonly called the \"network\", \"station\", \"channel\", and \"location\" code.  In MsPASS we use the CSS3.0 abbreviations from Antelope of \"net\", \"sta\", \"chan\", and \"loc\" respectively.   For these data we are parsing we see the \"station_id\" attribute has \"net\" and \"sta\" defined at the start of the string separated by a \".\" character.  For example, the first entry printed above has "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d1d792d-a489-4a51-89b8-9975191b76ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X9.BB060..HH*__20120919T000000Z__20120920T000000Z.mseed\n"
     ]
    }
   ],
   "source": [
    "print(doclist[0]['station_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba59c4e7-6c79-4b3b-8c98-71869a88b64d",
   "metadata": {},
   "source": [
    "So \"net\" is \"X9\" and \"sta\" is \"BB060\".   \"HH*\" means \"all H channels\" but that is ambiguous and rightly so.  For practical purpose all we usually care about for processing downstream is where the instrument was located so \"net\" plus \"sta\" is all we really need.   The whole point of that long discussion was to explain why when we actually write these data to MongoDB we use the python \"split\" method to extract the \"net\" and \"sta\" values and insert them into the data we are saving.  The following does that in a loop.  Once the data are edited in that way we insert the results into MongoDB with a standard create (C of CRUD) method called insert_many.  (Note insert_many used this way is orders of magnitude faster than if we had called insert_one after each doc was edited in the loop.  Why is easily learned from internet sources but the basic point is the data are written in blocks instead as single transactions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f8143cb-dd44-4dfb-bcbf-02be13f9630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in doclist:\n",
    "    station_id = doc['station_id']\n",
    "    slist=station_id.split('.')\n",
    "    net=slist[0]\n",
    "    sta=slist[1]\n",
    "    doc['sta']=sta\n",
    "    doc['net']=net\n",
    "\n",
    "insert_many_output=db.arrival.insert_many(doclist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0befdd9c-6693-4339-87c4-a3be1586a242",
   "metadata": {},
   "source": [
    "Let's verify that worked by verifying the count compared to what we put in and looking at the first document we saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f3fec14-8217-43d0-9181-bf8d30c7730b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of doclist =  35917\n",
      "Number of arrival documents saved= 35917\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"65e322fe5cd7e1e9490a6123\"\n",
      "  },\n",
      "  \"file_name\": \"X9.BB060..HH*__20120919T000000Z__20120920T000000Z.mseed\",\n",
      "  \"begin_time\": \"2012-09-19T00:00:00.000\",\n",
      "  \"station_id\": \"X9.BB060..HH*__20120919T000000Z__20120920T000000Z.mseed\",\n",
      "  \"phase_index\": 1390027,\n",
      "  \"phase_time\": \"2012-09-19T03:51:40.270\",\n",
      "  \"phase_score\": 0.337,\n",
      "  \"phase_type\": \"P\",\n",
      "  \"sta\": \"BB060\",\n",
      "  \"net\": \"X9\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('Size of doclist = ',len(doclist))\n",
    "print('Number of arrival documents saved=',db.arrival.count_documents({}))\n",
    "doc=db.arrival.find_one()\n",
    "pretty_print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78804f9-0c0f-4b91-8439-f0088698d902",
   "metadata": {},
   "source": [
    "### Load Centroid Moment Tensor Catalog\n",
    "The centroid moment tensor catalog is a heavily used catalog of earthquake source information. For those unfamiliar with the concept, a Moment Tensor can be thought of as the union of a focal mechanism and magnitude. It arises from a general theoretical framework for elastic waves and is the best theoretical model we know for any seismic source. The point of this tutorial, however, a lesson in how to read data not directly supported by MsPASS, not a lesson in seismology. The standard CMT catalog is a type example of data distributed in an archaic, nonstandard, and/or complicated format data. All three of those pretty much describe the CMT format.  It is well documented __[here](https://www.ldeo.columbia.edu/~gcmt/projects/CMT/catalog/allorder.ndk_explained)__ and the data we will use in this section can be downloaded directly __[here](https://www.globalcmt.org/CMTfiles.html)__.  \"Well documented\", however, does not mean simple.  The format is a legacy of card images as the lines are limited to 80 ascii characters.  Each CMT solution spans 5 lines with a few keywords inserted to give humans a hope of figuring out what is what.   \n",
    "\n",
    "With that long background, here is an implementation of a reader for \"ndk\" format files from the CMT project.   Run the next code box to define a couple key functions and then go to the text box following for a description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a118297f-2217-408f-a459-d5f1928d268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module contains a set of utilities to parse the archaic so \n",
    "called \"ndk\" format text files used to distribute the Global \n",
    "Centroid Moment Tensor catalog.  ndk is an obnoxious text format \n",
    "with obvious archaic roots to the dark ages of punched cards.  \n",
    "It uses 5, 80-column lines for each earthquake in the catalog. \n",
    "That type of data is very difficult to parse in python, but \n",
    "that is what this does.   \n",
    "\n",
    "Created on Wed Jan  3 06:03:05 2024\n",
    "\n",
    "@author: pavlis\n",
    "\"\"\"\n",
    "\n",
    "def read_ndk_file(path,apply_checks=True):\n",
    "    \"\"\"\n",
    "    Reads the contents of the \"ndk\" format file assumed to be the leaf \n",
    "    of the string defined by the \"path\" argument.  The result is a really \n",
    "    just an image of the file returned by the standard python function \n",
    "    readlines.  The only thing this function adds is a sanity check \n",
    "    on the file content to verify the files has the weird structure of \n",
    "    5 lines per CMT solution with some key words in the expected place.\n",
    "    That can be turned off by setting the \"apply_checks\" False.  In that \n",
    "    case is little more than open, readlines, and close. \n",
    "    \"\"\"\n",
    "    fh = open(path,'r')\n",
    "    alllines = fh.readlines()\n",
    "    fh.close()\n",
    "    # This is a sanity check.  Every third line in the full catalog \n",
    "    # I have has this magic string.  The format description doesn't \n",
    "    # state this as a requirement, but it seems to be so.\n",
    "    # If you get this message you may want to turn off default check\n",
    "    if apply_checks:\n",
    "        if len(alllines)%5 != 0:\n",
    "            message = \"read_ndk_file: file {} appears to be corrupted\\n\".format(path)\n",
    "            message += \"File has {} lines which is not a multiple of 5 that is propertly of ndk files\\n\".format(len(alllines))\n",
    "            raise RuntimeError(message)\n",
    "        i=2\n",
    "        while i<len(alllines):\n",
    "            testline=alllines[i]\n",
    "            teststring=testline[0:8]\n",
    "            if teststring != \"CENTROID\":\n",
    "                message = \"read_ndk_file: file {} appears to be corrupted at line {}\\n\".format(path,i)\n",
    "                message += \"Line content:  [{}]\".format(testline)\n",
    "                message += \"That line should contain the magic string CENTROID\"\n",
    "                raise RuntimeError(message)\n",
    "            i += 5\n",
    "    return alllines\n",
    "\n",
    "def parse_ndk_image(lines)->list:\n",
    "    \"\"\"\n",
    "    Parses the binary image of an ndk file read with the read_ndk_file \n",
    "    function.   That means it assumes arg0 is a list of strings \n",
    "    from the formatted ndk text file.  The algorithm assumes the \n",
    "    file line count is a multiple of 5, which is specified by the \n",
    "    archaic ndk file structure.   \n",
    "    \n",
    "    \n",
    "    Returns a list of python dicts with keys for attributes set as constants \n",
    "    in this function.  That effectively imposes a schema definition \n",
    "    that could be used with MongoDB but would require coordination \n",
    "    with the constants in this function.\n",
    "    \"\"\"\n",
    "    # These tuples define the format for each line and the key \n",
    "    # to which each value should be associated\n",
    "    # each tuple is start, end, key, type (s,f,i for string, float, and int)  \n",
    "    # date strings have to be treated specially\n",
    "    form_def = []\n",
    "    # the document on this format is wrong on some of these fields\n",
    "    # I think the riginal format may have dropped the decimal points for \n",
    "    # float values  Also ms and mb fields are not properly defined\n",
    "    # had to infer this\n",
    "    f = [\n",
    "          [0,3,'location_source','s'],\n",
    "          [4,14,'date','s'],\n",
    "          [16,25,'time_of_day','s'],\n",
    "          [27,33,'lat','f'],\n",
    "          [34,41,'lon','f'],\n",
    "          [42,47,'depth','f'],\n",
    "          [48,51,'mb','f'],\n",
    "          [52,54,'Ms','f'],\n",
    "          [56,79,'geographic_comment','s']\n",
    "        ]\n",
    "    form_def.append(f)\n",
    "    # line 2\n",
    "    f = [\n",
    "            [0,15,'CMT_event','s'],\n",
    "            [17,60,'CMT_data_used','s'],\n",
    "            [62,67,'invesion_type','s'],\n",
    "            [69,79,'moment_rate_function','s']\n",
    "        ]\n",
    "    form_def.append(f)\n",
    "    # line 3\n",
    "    f = [\n",
    "            [0,57,'centroid','s'],  # this token will need to be split into multiple attributes\n",
    "            [59,61,'depth_type','s'],\n",
    "            [64,79,'timestamp','s']\n",
    "        ]\n",
    "    form_def.append(f)\n",
    "    # line 4\n",
    "    f = [\n",
    "            [0,1,'exponent','i'],\n",
    "            [2,79,'MT_components','s'],  # will definitely need to be split up to be useful\n",
    "        ]\n",
    "    form_def.append(f)\n",
    "    # line 5\n",
    "    f = [\n",
    "            [0,2,'version','s'],\n",
    "            [3,47,'MT_pc','s'],\n",
    "            [49,55,'moment','f'],\n",
    "            [57,79,'sdr','s'], # strike-dip-rake needs to be split\n",
    "        ]\n",
    "    form_def.append(f)\n",
    "    \n",
    "    # loop over the list of lines 5 at a time. \n",
    "    # range increment makes this simpler\n",
    "    doclist=[]\n",
    "    for l0 in range(0,len(lines),5):\n",
    "        doc = dict()\n",
    "        # i runs over line number but ii run 0 to 4\n",
    "        ii = 0\n",
    "        for i in range(l0,l0+5):\n",
    "            s = lines[i]\n",
    "            f = form_def[ii]\n",
    "            for j in range(len(f)):\n",
    "                #print(l0,i,ii,j)\n",
    "                start = f[j][0]\n",
    "                end = f[j][1]\n",
    "                key = f[j][2]\n",
    "                type_def = f[j][3]\n",
    "                sval = s[start:end]\n",
    "                # too bad python doesn't have a swtich-case as that would be used here\n",
    "                if type_def == 's':\n",
    "                    val = sval\n",
    "                elif type_def == 'f':\n",
    "                    val = float(sval)\n",
    "                elif type_def == 'i':\n",
    "                    val = int(sval)\n",
    "                else:\n",
    "                    raise ValueError(\"Illegal value for type in foraat=\"+type_def)\n",
    "                doc[key] = val\n",
    "            ii += 1  \n",
    "        doclist.append(doc)\n",
    "        \n",
    "    return doclist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635b39e0-e185-4881-813f-31d3d2359800",
   "metadata": {},
   "source": [
    "A lot of lines of code there, but the docstrings for the two functions describe what they do.  We run both functions to crack translate the file into a form we can push to MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60c33212-f3ee-4c63-a7a7-210491ca55f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents inserted= 56832\n",
      "Size of the cmt collection is now =  56832\n"
     ]
    }
   ],
   "source": [
    "# This form works for running from standard MsPASS container \n",
    "# Change may be needed if file is downloaded \n",
    "fname='/home/data/jan76_dec20.ndk'\n",
    "lines = read_ndk_file(fname)\n",
    "doclist=parse_ndk_image(lines)\n",
    "del lines\n",
    "insout=db.cmt.insert_many(doclist)\n",
    "print(\"Number of documents inserted=\",len(insout.inserted_ids))\n",
    "print(\"Size of the cmt collection is now = \",db.cmt.count_documents({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669b8120-8672-4257-b9e8-81bedfc12483",
   "metadata": {},
   "source": [
    "Note the `del lines` statement is not essential, but demonstrates good practice for memory management.   The first function simply loads the file into memory as a list of strings while the second (`parse_ndk_image`) converts that to a list of python dictionaries.   Once the conversion is finished the output of the first function is no longer needed.  The last line uses the `insert_many` method of `Database.collection` to add one document for each CMT solution to a collection I called \"cmt\".  Let's look at one of the documents that produced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fddcf594-272e-46d9-8514-c56826a2db86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"65e322ff5cd7e1e9490aed70\"\n",
      "  },\n",
      "  \"location_source\": \"MLI\",\n",
      "  \"date\": \" 1976/01/0\",\n",
      "  \"time_of_day\": \"01:29:39.\",\n",
      "  \"lat\": -28.61,\n",
      "  \"lon\": -177.64,\n",
      "  \"depth\": 59.0,\n",
      "  \"mb\": 6.2,\n",
      "  \"Ms\": 0.0,\n",
      "  \"geographic_comment\": \"KERMADEC ISLANDS REGION\",\n",
      "  \"CMT_event\": \"M010176A       \",\n",
      "  \"CMT_data_used\": \"B:  0    0   0 S:  0    0   0 M: 12   30 13\",\n",
      "  \"invesion_type\": \"CMT: \",\n",
      "  \"moment_rate_function\": \"BOXHD:  9.\",\n",
      "  \"centroid\": \"CENTROID:     13.8 0.2 -29.25 0.02 -176.96 0.01  47.8  0.\",\n",
      "  \"depth_type\": \"FR\",\n",
      "  \"timestamp\": \"O-0000000000000\",\n",
      "  \"exponent\": 2,\n",
      "  \"MT_components\": \"  7.680 0.090  0.090 0.060 -7.770 0.070  1.390 0.160  4.520 0.160 -3.260 0.06\",\n",
      "  \"version\": \"V1\",\n",
      "  \"MT_pc\": \"   8.940 75 283   1.260  2  19 -10.190 15 11\",\n",
      "  \"moment\": 9.56,\n",
      "  \"sdr\": \"202 30   93  18 60   8\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "doc=db.cmt.find_one()\n",
    "pretty_print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8b8e53-4b7c-4b1b-aa5a-8b2aa978add3",
   "metadata": {},
   "source": [
    "As you can see, the reason the CMT format is so complex is that there are a lot of attributes for each CMT solution.  There are actually a lot more than the document layout suggests.  My implementation parsed the file by field widths described in the __[format description_document](https://www.ldeo.columbia.edu/~gcmt/projects/CMT/catalog/allorder.ndk_explained)__.  If I were going to actually use this further, I would change the code to split out the attributes above with strings containing multiple numbers.  e.g. the most fundamental quantities in the CMT are the 12 numbers above following the key \"MT_components\".   The last box of this tutorial shows how to fix the \"MT_components\" entry and convert it to a MongoDB \"subdocument\" with the data stored properly as floats instead of a formatted data string.  The subdocument translates the components to names defined in the format description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3dd3e4fe-592d-4649-b030-d7919fb241ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7.680', '0.090', '0.090', '0.060', '-7.770', '0.070', '1.390', '0.160', '4.520', '0.160', '-3.260', '0.06']\n"
     ]
    }
   ],
   "source": [
    "mtcomp=doc['MT_components']\n",
    "cstrl = mtcomp.split()\n",
    "print(cstrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c12e1d7-7f4f-466e-8a56-97099600c145",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor=db.cmt.find({}).limit(3)\n",
    "for doc in cursor:\n",
    "    id=doc['_id']   # we need this later for update\n",
    "    mtcomp=doc['MT_components']\n",
    "    slist = mtcomp.split()   # python function parsing tokens by white space\n",
    "    subdoc = dict()\n",
    "    #Mrr, Mtt, Mpp, Mrt, Mrp, Mtp,\n",
    "    subdoc['Mrr'] = slist[0]\n",
    "    subdoc['Mrr_sigma'] = slist[1]\n",
    "    subdoc['Mtt'] = slist[2]\n",
    "    subdoc['Mtt_sigma'] = slist[3]\n",
    "    subdoc['Mpp'] = slist[4]\n",
    "    subdoc['Mpp_sigma'] = slist[5]\n",
    "    subdoc['Mrt'] = slist[6]\n",
    "    subdoc['Mrt_sigma'] = slist[7]\n",
    "    subdoc['Mrp'] = slist[8]\n",
    "    subdoc['Mrp_sigma'] = slist[9]\n",
    "    subdoc['Mtp'] = slist[10]\n",
    "    subdoc['Mtp_sigma'] = slist[11]\n",
    "    matcher={'_id' : id}\n",
    "    update_command = {'$set' : {'MT_components' : subdoc}}\n",
    "    db.cmt.update_one(matcher,update_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bba952c-4cf8-4e4c-902b-68990d274cda",
   "metadata": {},
   "source": [
    "To see what that did, here is the same document displayed above but with the change we just made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f36cbc7a-a57d-4677-b450-940309c29562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"65e322ff5cd7e1e9490aed70\"\n",
      "  },\n",
      "  \"location_source\": \"MLI\",\n",
      "  \"date\": \" 1976/01/0\",\n",
      "  \"time_of_day\": \"01:29:39.\",\n",
      "  \"lat\": -28.61,\n",
      "  \"lon\": -177.64,\n",
      "  \"depth\": 59.0,\n",
      "  \"mb\": 6.2,\n",
      "  \"Ms\": 0.0,\n",
      "  \"geographic_comment\": \"KERMADEC ISLANDS REGION\",\n",
      "  \"CMT_event\": \"M010176A       \",\n",
      "  \"CMT_data_used\": \"B:  0    0   0 S:  0    0   0 M: 12   30 13\",\n",
      "  \"invesion_type\": \"CMT: \",\n",
      "  \"moment_rate_function\": \"BOXHD:  9.\",\n",
      "  \"centroid\": \"CENTROID:     13.8 0.2 -29.25 0.02 -176.96 0.01  47.8  0.\",\n",
      "  \"depth_type\": \"FR\",\n",
      "  \"timestamp\": \"O-0000000000000\",\n",
      "  \"exponent\": 2,\n",
      "  \"MT_components\": {\n",
      "    \"Mrr\": \"7.680\",\n",
      "    \"Mrr_sigma\": \"0.090\",\n",
      "    \"Mtt\": \"0.090\",\n",
      "    \"Mtt_sigma\": \"0.060\",\n",
      "    \"Mpp\": \"-7.770\",\n",
      "    \"Mpp_sigma\": \"0.070\",\n",
      "    \"Mrt\": \"1.390\",\n",
      "    \"Mrt_sigma\": \"0.160\",\n",
      "    \"Mrp\": \"4.520\",\n",
      "    \"Mrp_sigma\": \"0.160\",\n",
      "    \"Mtp\": \"-3.260\",\n",
      "    \"Mtp_sigma\": \"0.06\"\n",
      "  },\n",
      "  \"version\": \"V1\",\n",
      "  \"MT_pc\": \"   8.940 75 283   1.260  2  19 -10.190 15 11\",\n",
      "  \"moment\": 9.56,\n",
      "  \"sdr\": \"202 30   93  18 60   8\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "doc = db.cmt.find_one()\n",
    "pretty_print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae21947-31d5-40c9-9aa1-816b1eca571b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
