{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "554edf59-2c25-4210-8197-d4706709fd38",
   "metadata": {},
   "source": [
    "# Extended USArray data preprocessing stage 2\n",
    "This notebook does the first stage of actual preprocessing of the extended USArray data set.  It should be run after the notebook curently called index_mseed_*.ipynb where the * is normally a year.  It must be run before a related notebook called Preproces2seis_*.ipynb.  After experimenting with several variants this approach seems to be the best solution for performance with a workable memory footprint.   It does a fairly long string of operations all contained in the function atomic_ts_processor.  \n",
    "\n",
    "Notice the approach here is processing the data as ensembles grouped by source_id.   The parallel section submits data by ensemble the cluster rather than doing the entire data et in one massive map operator.  That was done because currently dask will not handle bags of that size and we alway seem to abort on a memory fault.   Another reason the ensemble approach is a good one is it allows this job to be rerun if there are problems.  Note how the query that looks for source_id values in the output wf_TimeSeries collection and runs only ids that are not already present.  In the rare case that a job would abort with only part of an ensemble saved, that could drop data but the only way that should happen is if the write hit a file limit.  In that case, we would know as we couldn't continue until the file limit (e.g. quota) was resolved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9da46d-fa80-4c4a-b500-7af04be13de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change for different calendar year\n",
    "year = 2014\n",
    "dbname = \"usarray{}\".format(year)\n",
    "tsdir=\"./wf_TimeSeries/{}\".format(year)\n",
    "dbmaster=\"usarray48\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c091f6-7d4d-4bdb-973d-9fc7f19ffb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mspasspy.client as msc\n",
    "mspass_client = msc.Client(database_name=dbname)\n",
    "# waveform data indexed to here\n",
    "db = mspass_client.get_database()\n",
    "# master database with source and receiver metadata\n",
    "dbmd = mspass_client.get_database(dbmaster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38371123-fa1f-457e-ad1c-bcc12e8914c8",
   "metadata": {},
   "source": [
    "First do imports and define special functions used for this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536bce0c-cc54-4f42-965b-7f05597be17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.algorithms.window import WindowData\n",
    "from mspasspy.algorithms.signals import detrend\n",
    "from mspasspy.ccore.algorithms.basic import TimeWindow\n",
    "from mspasspy.ccore.utility import ErrorSeverity\n",
    "from mspasspy.algorithms.resample import (ScipyResampler,\n",
    "                                          ScipyDecimator,\n",
    "                                          resample,\n",
    "                                         )\n",
    "from mspasspy.db.normalize import ObjectIdMatcher\n",
    "from mspasspy.algorithms.calib import ApplyCalibEngine\n",
    "from mspasspy.util.seismic import number_live\n",
    "from obspy import UTCDateTime\n",
    "from obspy.geodetics import gps2dist_azimuth,kilometers2degrees\n",
    "from obspy.taup import TauPyModel\n",
    "from dask.distributed import as_completed\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "def set_PStime(d,Ptimekey=\"Ptime\",Stimekey=\"Stime\",model=None):\n",
    "    \"\"\"\n",
    "    Function to calculate P and S wave arrival time and set times \n",
    "    as the header (Metadata) fields defined by Ptimekey and Stimekey.\n",
    "    Tries to handle some complexities of the travel time calculator \n",
    "    returns when one or both P and S aren't calculatable.  That is \n",
    "    the norm in or at the edge of the core shadow.  \n",
    "    \n",
    "    :param d:  input TimeSeries datum.  Assumes datum's Metadata \n",
    "      contains stock source and channel attributes.  \n",
    "    :param Ptimekey:  key used to define the header attribute that \n",
    "      will contain the computed P time.  Default \"Ptime\".\n",
    "    :param model:  instance of obspy TauPyModel travel time engine. \n",
    "      Default is None.   That mode is slow as an new engine will be\n",
    "      constructed on each call to the function.  Normal use should \n",
    "      pass an instance for greater efficiency.  \n",
    "    \"\"\"\n",
    "    if d.live:\n",
    "        if model is None:\n",
    "            model = TauPyModel(model=\"iasp91\") \n",
    "        # extract required source attributes\n",
    "        srclat=d[\"source_lat\"]\n",
    "        srclon=d[\"source_lon\"]\n",
    "        srcz=d[\"source_depth\"]\n",
    "        srct=d[\"source_time\"] \n",
    "        # extract required channel attributes\n",
    "        stalat=d[\"channel_lat\"]\n",
    "        stalon=d[\"channel_lon\"]\n",
    "        staelev=d[\"channel_elev\"]\n",
    "        # set up and run travel time calculator\n",
    "        georesult=gps2dist_azimuth(srclat,srclon,stalat,stalon)\n",
    "        # obspy's function we just called returns distance in m in element 0 of a tuple\n",
    "        # their travel time calculator it is degrees so we need this conversion\n",
    "        dist=kilometers2degrees(georesult[0]/1000.0)\n",
    "        arrivals=model.get_travel_times(source_depth_in_km=srcz,\n",
    "                                            distance_in_degree=dist,\n",
    "                                            phase_list=['P','S'])\n",
    "        # always post this for as it is not cheap to compute\n",
    "        # WARNING:  don't use common abbrevation delta - collides with data dt\n",
    "        d['epicentral_distance']=dist\n",
    "        # these are CSS3.0 shorthands s - station e - event\n",
    "        esaz = georesult[1]\n",
    "        seaz = georesult[2]\n",
    "        # css3.0 names esax = event to source azimuth; seaz = source to event azimuth\n",
    "        d['esaz']=esaz\n",
    "        d['seaz']=seaz\n",
    "        # get_travel_times returns an empty list if a P time cannot be \n",
    "        # calculated.  We trap that condition and kill the output \n",
    "        # with an error message\n",
    "        if len(arrivals)==2:\n",
    "            Ptime=srct+arrivals[0].time\n",
    "            rayp = arrivals[0].ray_param\n",
    "            Stime=srct+arrivals[1].time\n",
    "            rayp_S = arrivals[1].ray_param\n",
    "            d.put(Ptimekey,Ptime)\n",
    "            d.put(Stimekey,Stime)\n",
    "            # These keys are not passed as arguments but could be - a choice\n",
    "            # Ray parameter is needed for free surface transformation operator\n",
    "            # note tau p calculator in obspy returns p=R sin(theta)/V_0\n",
    "            d.put(\"rayp_P\",rayp)\n",
    "            d.put(\"rayp_S\",rayp_S)\n",
    "        elif len(arrivals)==1:\n",
    "            if arrivals[0].name == 'P':\n",
    "                Ptime=srct+arrivals[0].time\n",
    "                rayp = arrivals[0].ray_param\n",
    "                d.put(Ptimekey,Ptime)\n",
    "                d.put(\"rayp_P\",rayp)\n",
    "            else:\n",
    "                # Not sure we can assume name is S\n",
    "                if arrivals[0].name == 'S':\n",
    "                    Stime=srct+arrivals[0].time\n",
    "                    rayp_S = arrivals[0].ray_param\n",
    "                    d.put(Stimekey,Stime)\n",
    "                    d.put(\"rayp_S\",rayp_S)\n",
    "                else:\n",
    "                    message = \"Unexpected single phase name returned by taup calculator\\n\"\n",
    "                    message += \"Expected phase name S but got \" + arrivals[0].name\n",
    "                    d.elog.log_error(\"set_PStime\",\n",
    "                                     message,\n",
    "                                     ErrorSeverity.Invalid)\n",
    "                    d.kill()\n",
    "        else:\n",
    "            # in this context this only happens if no P or S could be calculated\n",
    "            # That shouldn't ever happen but we need this safety in he event it does\n",
    "            message = \"Travel time calculator failed completely\\n\"\n",
    "            message += \"Could not calculator P or S phase time\"\n",
    "            d.elog.log_error(\"set_PStime\",\n",
    "                             message,\n",
    "                             ErrorSeverity.Invalid)\n",
    "            d.kill()\n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def set_file_path(e,dir=tsdir):\n",
    "    \"\"\"\n",
    "    This function is used to set dir and dfile for this workflow for each \n",
    "    ensemble.  Note these are set in the ensemble's metadata container \n",
    "    not the members.   We don't set them for members as Database.save_data \n",
    "    only uses kwarg dir and dfile with the metadata as a fallback.\n",
    "    \"\"\"\n",
    "    e['dir']=dir\n",
    "    dfile = \"dfile_undefined\"\n",
    "    for d in e.member:\n",
    "        if d.live and 'dfile' in d:\n",
    "            dfile=d['dfile']\n",
    "            base,ext = dfile.split(\".\")\n",
    "            dfile=base\n",
    "            break\n",
    "    e[\"dfile\"] = dfile\n",
    "    return e\n",
    "\n",
    "def dbmdquery(year,padsize=86400.0):\n",
    "    \"\"\"\n",
    "    Constructs a MongoDB query dictionary to use as a query argument for normalization matcher classes.\n",
    "    (All standard BasicMatcher children have a query argument in the constructor for this purpose.)\n",
    "    The query is a range spanning specified calendar year.   The interval is extended by padsize \n",
    "    seconds.   (default is one day = 86400.0)\n",
    "\n",
    "    Note this query is appropriate for channel not site.\n",
    "    \"\"\"\n",
    "    # uses obspy's UTCDateTime class to create time range in epoch time using the \n",
    "    # calendar strings for convenience\n",
    "    tstr = \"{}-01-01T00:00:00.0\".format(year)\n",
    "    st = UTCDateTime(tstr)\n",
    "    starttime = st.timestamp - padsize\n",
    "    tstr = \"{}-01-01T00:00:00.0\".format(year+1)\n",
    "    et = UTCDateTime(tstr)\n",
    "    endtime = et.timestamp + padsize\n",
    "    # not sure the and is required but better safe than sorry\n",
    "    query = { \"$and\" :\n",
    "             [\n",
    "                 {\"starttime\" : {\"$lte\" : endtime}},\n",
    "                 {\"endtime\" : {\"$gte\" : starttime}}\n",
    "             ]\n",
    "    }\n",
    "    return query\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19edb463-0a68-4451-9709-d5985515da1e",
   "metadata": {},
   "source": [
    "This function is the one used in map operator run on each enemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61ecb40-6e9a-40cf-b7a9-192ced9db52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atomic_ts_processor(d,decimator,resampler,ttmodel,win,calib_engine):\n",
    "    \"\"\"\n",
    "    This function puts all the processing functions for input TimeSeres (d).  It is called in this \n",
    "    workflow in a map operator applied to ensemble members.  \n",
    "    \"\"\"\n",
    "    d = detrend(d,type=\"constant\")\n",
    "    d = resample(d,decimator,resampler)\n",
    "    d = set_PStime(d,model=ttmodel)\n",
    "    if d.live and \"Ptime\" in d:\n",
    "        ptime = d[\"Ptime\"]\n",
    "        d.ator(ptime)\n",
    "        d = WindowData(d,win.start,win.end)\n",
    "        d.rtoa()\n",
    "    d = calib_engine.apply_calib(d)\n",
    "    return d\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e6b737-53c6-4f00-bb91-e8527a122570",
   "metadata": {},
   "source": [
    "This is the driver script comparable to a fortran main.  The algorithm here makes sense only for large ensemles like the extended usarray data set.   We process the data in blocks that are the common source gathers.   That was found to reduce the memory footprint of the processing with a modest cost in overhead to submit each ensemble as a different job to the cluster.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227cfea1-3103-4a36-9405-e6f837829bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.ccore.seismic import TimeSeriesEnsemble\n",
    "from mspasspy.io.distributed import read_distributed_data\n",
    "ttmodel = TauPyModel(model=\"iasp91\")\n",
    "t0 = time.time()\n",
    "# important to note these matchers are loaded from dbmd\n",
    "# bulk_normalize above will set the matching ids\n",
    "timequery = dbmdquery(year)\n",
    "chan_matcher = ObjectIdMatcher(dbmd,\n",
    "                               query=timequery,\n",
    "                               collection=\"channel\",\n",
    "                               attributes_to_load=[\"_id\",\"lat\",\"lon\",\"elev\",\"hang\",\"vang\"],\n",
    "                              )\n",
    "tstr1 = \"{}-01-01T00:00:00.0\".format(year)\n",
    "tstr2 = \"{}-01-01T00:00:00.0\".format(year+1)\n",
    "st1=UTCDateTime(tstr1)\n",
    "st2=UTCDateTime(tstr2)\n",
    "srcquery = {\"time\" : {\"$gte\" : st1.timestamp, \"$lt\" : st2.timestamp}}\n",
    "source_matcher = ObjectIdMatcher(dbmd,\n",
    "                                query=srcquery,\n",
    "                                collection=\"source\",\n",
    "                                attributes_to_load=[\"_id\",\"lat\",\"lon\",\"depth\",\"time\"],\n",
    "                                load_if_defined=[\"magnitude\"],\n",
    "                                )\n",
    "target_sample_rate=20.0\n",
    "resampler=ScipyResampler(target_sample_rate)\n",
    "decimator=ScipyDecimator(target_sample_rate)\n",
    "calib_engine = ApplyCalibEngine(dbmd)\n",
    "tswin = TimeWindow(-200.0,500.0)\n",
    "srcidlist_ms=db.wf_miniseed.distinct('source_id')\n",
    "\n",
    "srcidlist_finished = db.wf_TimeSeries.distinct('source_id')\n",
    "if len(srcidlist_finished)>0:\n",
    "    srcidlist=[]\n",
    "    for sid in srcidlist_ms:\n",
    "        if sid not in srcidlist_finished:\n",
    "            srcidlist.append(sid)\n",
    "else:\n",
    "    srcidlist = srcidlist_ms\n",
    "\n",
    "\n",
    "print(\"Number of distinct sources in wf_miniseed= \",len(srcidlist_ms))\n",
    "print(\"Number to process this run=\",len(srcidlist))\n",
    "# reduce size for testing - fails submitting all at once\n",
    "#sidtmp=[]\n",
    "#for i in range(10):\n",
    "#    sidtmp.append(srcidlist[i])\n",
    "#srcidlist=sidtmp\n",
    "\n",
    "for sid in srcidlist:\n",
    "    print(\"working on  \",sid)\n",
    "    query = {\"source_id\" : sid}\n",
    "    mydata = read_distributed_data(db,\n",
    "                                   query=query,\n",
    "                                   collection=\"wf_miniseed\",\n",
    "                                   normalize=[chan_matcher,source_matcher],\n",
    "                                   npartitions=60,\n",
    "                                  )\n",
    "    mydata=mydata.map(atomic_ts_processor,\n",
    "                            decimator,\n",
    "                            resampler,\n",
    "                            ttmodel,\n",
    "                            tswin,\n",
    "                            calib_engine)\n",
    "    dlist = mydata.compute()\n",
    "    # package into ensemble for a faster write \n",
    "    ens=TimeSeriesEnsemble(len(dlist))\n",
    "    for d in dlist:\n",
    "        ens.member.append(d)\n",
    "    ens.set_live()\n",
    "    ens = set_file_path(ens)\n",
    "    ens = db.save_data(ens,\n",
    "                       return_data=True,\n",
    "                       collection=\"wf_TimeSeries\",\n",
    "                       storage_mode=\"file\",\n",
    "                       dir=tsdir,\n",
    "                       data_tag=\"preprocessed_map\",\n",
    "                       )\n",
    "    del ens\n",
    "    \n",
    "    \n",
    "t = time.time()\n",
    "print(\"Run time = \",t-t0,\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0968e936-1359-490d-8ed8-80f92381567a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c2b5f8-b206-4bd7-841b-3258dac4bd0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
