{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9170ade2-9d78-4f36-8d12-76bd0419242f",
   "metadata": {},
   "source": [
    "# Extended USArray data preprocessing stage 2\n",
    "This notebook does the second stage of preprocessing of the extended USArray data set. It reads common source gathers input previously assumed created in wf_TimeSeries and does the following processing steps:\n",
    "1.  Runs bundle_seed_data to transform the data to a SeismogramEnsemble \n",
    "2.  Applies the free surface transformation to all ensemble members\n",
    "3.  Runs broadband_snr_QC on all members.  \n",
    "\n",
    "Step 3 tends to kill a lot of data but that is a key point.   The cemetery contains the bodies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e810137a-ae9e-44ff-bcc8-d31c286f133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to year of data being processed\n",
    "year = 2014\n",
    "dbname = \"usarray{}\".format(year)\n",
    "wfdir = \"./wf_Seismogram/{}\".format(year)\n",
    "dbmaster=\"usarray48\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c091f6-7d4d-4bdb-973d-9fc7f19ffb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mspasspy.client as msc\n",
    "mspass_client = msc.Client(database_name=dbname)\n",
    "db = mspass_client.get_database()\n",
    "dbmd = mspass_client.get_database(dbmaster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38371123-fa1f-457e-ad1c-bcc12e8914c8",
   "metadata": {},
   "source": [
    "First do imports and define special functions used for this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536bce0c-cc54-4f42-965b-7f05597be17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.algorithms.window import WindowData\n",
    "from mspasspy.ccore.algorithms.basic import TimeWindow\n",
    "from mspasspy.ccore.utility import ErrorSeverity\n",
    "from mspasspy.db.normalize import normalize,ObjectIdMatcher\n",
    "from mspasspy.algorithms.MTPowerSpectrumEngine import MTPowerSpectrumEngine\n",
    "from mspasspy.algorithms.bundle import bundle_seed_data\n",
    "from mspasspy.ccore.seismic import SlownessVector\n",
    "from mspasspy.algorithms.basic import free_surface_transformation,rotate_to_standard\n",
    "from mspasspy.algorithms.snr import broadband_snr_QC\n",
    "from mspasspy.util.seismic import number_live\n",
    "import dask.bag as dbg\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "def apply_FST(d,rayp_key=\"rayp_P\",seaz_key='seaz',vp0=6.0,vs0=3.5):\n",
    "    \"\"\"\n",
    "    Apply free surface transformation operator of Kennett (1983) to an input `Seismogram` \n",
    "    object.   Assumes ray parameter and azimuth data are stored as Metadata in the \n",
    "    input datum.  If the ray parameter or azimuth key are not defined an error \n",
    "    message will be posted and the datum will be killed before returning.  \n",
    "    :param d:  datum to process\n",
    "    :type d:  Seismogram\n",
    "    :param rayp_key:   key to use to extract ray parameter to use to compute the \n",
    "    free surface transformation operator.  Note function assumes the ray parameter is\n",
    "    spherical coordinate form:  R sin(theta)/V.   Default is \"rayp_P\".\n",
    "    :param seaz_key:   key to use to extract station to event azimuth. Default is \"seaz\".\n",
    "    :param vp0:  surface P wave velocity (km/s) to use for free surface transformation \n",
    "    :param vs0:  surface S wave velocity (km/s) to use for free surface transformation.\n",
    "    \"\"\"\n",
    "    if d.is_defined(rayp_key) and d.is_defined(seaz_key):\n",
    "        rayp = d[rayp_key]\n",
    "        seaz = d[seaz_key]\n",
    "        # Some basic seismology here.  rayp is the spherical earth ray parameter\n",
    "        # R sin(theta)/v.  Free surface transformation needs apparent velocity \n",
    "        # at Earth's surface which is sin(theta)/v when R=Re.   Hence the following\n",
    "        # simple convertion to get apparent slowness at surface  sin(theta)/v\n",
    "        Re=6378.1\n",
    "        umag = rayp/Re   # magnitude of slowness vector\n",
    "        # seaz is back azimuth - slowness vector points in direction of propagation\n",
    "        # with is 180 degrees away from back azimuth\n",
    "        az = seaz + 180.0\n",
    "        # component slowness vector components in local coordinates\n",
    "        ux = umag * math.sin(az)\n",
    "        uy = umag * math.cos(az)\n",
    "        # FST implementation requires this special class called a Slowness Vector\n",
    "        u = SlownessVector(ux,uy)\n",
    "        d = free_surface_transformation(d,u,vp0,vs0)\n",
    "    else:\n",
    "        d.kill()\n",
    "        message = \"one of required attributes rayp_P or seaz were not defined for this datum\"\n",
    "        d.elog.log_error(\"apply_FST\",message,ErrorSeverity.Invalid)\n",
    "        \n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "def set_file_path(e,dir=wfdir):\n",
    "    \"\"\"\n",
    "    This function is used to set dir and dfile for this workflow for each \n",
    "    ensemble.  Note these are set in the ensemble's metadata container \n",
    "    not the members.   We don't set them for members as Database.save_data \n",
    "    only uses kwarg dir and dfile with the metadata as a fallback.\n",
    "    \"\"\"\n",
    "    e['dir']=dir\n",
    "    dfile = \"dfile_undefined\"\n",
    "    for d in e.member:\n",
    "        if d.live and 'dfile' in d:\n",
    "            dfile=d['dfile']\n",
    "            break\n",
    "    e[\"dfile\"] = dfile\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e307e480-0923-4e95-b209-e28689061e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function was copied from Preprocess2ts - it maybe should be a local .py file\n",
    "def dbmdquery(year,padsize=86400.0):\n",
    "    \"\"\"\n",
    "    Constructs a MongoDB query dictionary to use as a query argument for normalization matcher classes.\n",
    "    (All standard BasicMatcher children have a query argument in the constructor for this purpose.)\n",
    "    The query is a range spanning specified calendar year.   The interval is extended by padsize \n",
    "    seconds.   (default is one day = 86400.0)\n",
    "    \"\"\"\n",
    "    # uses obspy's UTCDateTime class to create time range in epoch time using the \n",
    "    # calendar strings for convenience\n",
    "    tstr = \"{}-01-01T00:00:00.0\".format(year)\n",
    "    st = UTCDateTime(tstr)\n",
    "    starttime = st.timestamp() - padsize\n",
    "    tstr = \"{}-01-01T00:00:00.0\".format(year+1)\n",
    "    et = UTCDateTime(tstr)\n",
    "    endtime = et.timestamp() + padsize\n",
    "    # not sure the and is required but better safe than sorry\n",
    "    query = { \"$and\" :\n",
    "             [\n",
    "                 {\"starttime\" : {\"$lte\" : endtime}},\n",
    "                 {\"endtime\" : {\"$gte\" : starttime}}\n",
    "             ]\n",
    "    }\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19edb463-0a68-4451-9709-d5985515da1e",
   "metadata": {},
   "source": [
    "These are higher level functions map operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61ecb40-6e9a-40cf-b7a9-192ced9db52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsensembles(source_id,db,data_tag=None):\n",
    "    \"\"\"\n",
    "    Reader for this script. Could be done with read_distributed_data and a query generator \n",
    "    to create a list of python dictionaries as input to read_distributed_data.   This does the \n",
    "    same thing more or less and is a bit clearer.\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"Debug:  entered read_tsensembles\")\n",
    "    query = {\"source_id\" : source_id}\n",
    "    if data_tag:\n",
    "        query[\"data_tag\"] = data_tag\n",
    "    n = db.wf_TimeSeries.count_documents(query)\n",
    "    print(\"Found \",n,\" documents for nsemble with source_id=\",source_id)\n",
    "    cursor = db.wf_TimeSeries.find(query)\n",
    "    ens = db.read_data(cursor,collection=\"wf_TimeSeries\")\n",
    "    print(\"Number of members in this ensemble=\",len(ens.member))\n",
    "    return ens\n",
    "\n",
    "def normalize_ensembles(ens,matcher):\n",
    "    \"\"\"\n",
    "    Workaround for bug in normalize function where decorators won't work.\n",
    "    This function should be removed when that bug is resolved.\n",
    "    \"\"\"\n",
    "    if ens.live:\n",
    "        for i in range(len(ens.member)):\n",
    "            ens.member[i] = normalize(ens.member[i],matcher)\n",
    "    return ens\n",
    "    \n",
    "def process2seis(ens,\n",
    "            swin,\n",
    "            nwin,\n",
    "            signal_engine,\n",
    "            noise_engine,\n",
    "                ):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print(\"Processing TimeSeriesEnsemble with \",len(ens.member),\" members\")\n",
    "    ens3c = bundle_seed_data(ens)\n",
    "    del ens\n",
    "    N = len(ens3c.member)\n",
    "    for i in range(N):\n",
    "        ens3c.member[i] = apply_FST(ens3c.member[i])\n",
    "        ens3c.member[i] = broadband_snr_QC(ens3c.member[i],\n",
    "                            component=2,\n",
    "                            signal_window=swin,\n",
    "                            noise_window=nwin,\n",
    "                            use_measured_arrival_time=True,\n",
    "                            measured_arrival_time_key=\"Ptime\",\n",
    "                            noise_spectrum_engine=noise_engine,\n",
    "                            signal_spectrum_engine=signal_engine,\n",
    "                                )\n",
    "    return ens3c\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e6b737-53c6-4f00-bb91-e8527a122570",
   "metadata": {},
   "source": [
    "This is the driver script comparable to a fortran main."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227cfea1-3103-4a36-9405-e6f837829bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "chanquery=dbmdquery(year)\n",
    "chan_matcher = ObjectIdMatcher(db,\n",
    "                               query=chanquery,\n",
    "                               collection=\"channel\",\n",
    "                               attributes_to_load=[\"_id\",\"lat\",\"lon\",\"elev\",\"hang\",\"vang\"],\n",
    "                              )\n",
    "\n",
    "# for usage on IU system each node has 2*64=128 cores so using 128 workers for the run\n",
    "# makes this twice that\n",
    "npartitions=100\n",
    "# seismogram output directory\n",
    "dir_seismogram=\"wf_Seismogram/2013\"\n",
    "swin = TimeWindow(-5.0,100.0)\n",
    "nwin = TimeWindow(-195.0,-5.0)\n",
    "# spectrum estimation engines for broadband_snr_QC\n",
    "dt = 0.05\n",
    "nsamp_noise = int((nwin.end-nwin.start)/dt) + 1\n",
    "nsamp_sig = int((swin.end-swin.start)/dt) + 1\n",
    "signal_engine=MTPowerSpectrumEngine(nsamp_sig,5.0,8,2*nsamp_sig,dt)\n",
    "noise_engine=MTPowerSpectrumEngine(nsamp_noise,5.0,10,2*nsamp_noise,dt)\n",
    "srcidlist_ms=db.wf_TimeSeries.distinct('source_id')\n",
    "\n",
    "srcidlist_finished = db.wf_Seismogram.distinct('source_id')\n",
    "if len(srcidlist_finished)>0:\n",
    "    srcidlist=[]\n",
    "    for sid in srcidlist_ms:\n",
    "        if sid not in srcidlist_finished:\n",
    "            srcidlist.append(sid)\n",
    "else:\n",
    "    srcidlist = srcidlist_ms\n",
    "\n",
    "\n",
    "print(\"Number of distinct sources in wf_TimeSeries= \",len(srcidlist_ms))\n",
    "print(\"Number to process this run=\",len(srcidlist))\n",
    "# reduce size for testing \n",
    "#sidtmp=[]\n",
    "#for i in range(4):\n",
    "#    sidtmp.append(srcidlist[i])\n",
    "#srcidlist=sidtmp\n",
    "#print(\"Debug process list:  \",srcidlist)\n",
    "mydata = dbg.from_sequence(srcidlist)\n",
    "mydata = mydata.map(read_tsensembles,db)\n",
    "# note default behavior for normalize is to normalize all members\n",
    "#mydata = mydata.map(normalize,chan_matcher,handles_ensembles=False)\n",
    "#workaround for above until bug is fixed in normalize\n",
    "mydata = mydata.map(normalize_ensembles,chan_matcher)\n",
    "mydata = mydata.map(process2seis,\n",
    "                      swin,\n",
    "                      nwin,\n",
    "                      signal_engine,\n",
    "                      noise_engine,\n",
    "                               )\n",
    "mydata = mydata.map(set_file_path)\n",
    "mydata = mydata.map(db.save_data,\n",
    "                    collection=\"wf_Seismogram\",\n",
    "                    storage_mode=\"file\",\n",
    "                    dir=wfdir,\n",
    "                    data_tag=\"FST_and_QCed\",\n",
    "                   )\n",
    "out=mydata.compute()\n",
    "print(out)\n",
    "t = time.time()\n",
    "print(\"Run time = \",t-t0,\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b312080b-6560-486e-acc7-7798a64a9ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
