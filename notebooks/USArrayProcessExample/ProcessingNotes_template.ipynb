{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac2d98cd-3d4a-4e89-9354-5c4327b75caf",
   "metadata": {},
   "source": [
    "# Processing Notes\n",
    "Best practice for handling large data sets with MsPASS require a notebook like this one.  The purpose of a notebook like this is to record progress in working through a large data set and any problems that required additional efforts to address.   The entire point is to provide a record that will allow someone else to reproduce your work.\n",
    "\n",
    "This notebook may contain just plain text, but can also contain code fragments used to demonstrate issues that happen.   In general, it should never be designed as a notebook that would be run independently.   It is reasonable to put this stock incantation for this set of notebooks as any code blocks you would want to run will likely need this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a01638-541b-4f72-aebf-0ffc41f87579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change for different calendar year\n",
    "year = 2014\n",
    "dbname = \"usarray{}\".format(year)\n",
    "tsdir=\"./wf_TimeSeries/{}\".format(year)\n",
    "dbmaster=\"usarray48\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41264258-6651-433d-9b7c-4f95faae86c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mspasspy.client as msc\n",
    "mspass_client = msc.Client(database_name=dbname)\n",
    "# waveform data indexed to here\n",
    "db = mspass_client.get_database()\n",
    "# master database with source and receiver metadata\n",
    "dbmd = mspass_client.get_database(dbmaster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e021cb5e-5114-46e1-b267-6d19320ea00c",
   "metadata": {},
   "source": [
    "# Creating Master Database\n",
    "Record any issues in running CreateMasterDatabase.ipynb.   If nothing else record the timing data and when the work was done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4406dbb1-5c3e-4c3e-acc8-22667132013e",
   "metadata": {},
   "source": [
    "# Yearly Processing\n",
    "Record your notes below for each year.  Insert additional boxes if necessary.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a3d32a-fd30-48db-851b-6d515c6e4e53",
   "metadata": {},
   "source": [
    "## 2005\n",
    "I have an archive copy of data I downloaded for this year that is inconsistent with the later data.  I do not think a copy of the data exist on Frontera.   Rather than use the questionable data I have I think the best approach for these data is to download them again from Earthscope.   This is a realitively small piece of the data as Earthscope launched that year and there were only a handful of TA stations running until late in the year.  I do not, however, know how many other broadband stations will be retrieved but it still will be smaller than any other year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ede379-d12f-4edf-99de-b780631f87ef",
   "metadata": {},
   "source": [
    "## 2006 \n",
    "This year of data is like 2005.  The only difference is the size will be much larger the TA was expanded a lot during this second year of the TA construction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f7cc81-a400-47ff-9a07-41b6b128ab27",
   "metadata": {},
   "source": [
    "## 2006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b33121e-5355-496e-9c7d-ec1f88266d7c",
   "metadata": {},
   "source": [
    "## 2007"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e46171-9bad-45a8-993b-2b3cdeda8062",
   "metadata": {},
   "source": [
    "## 2008"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b17c83-5185-406e-b6bb-2cf0b9c24086",
   "metadata": {},
   "source": [
    "## 2009"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c5283e-af7a-4907-ae39-374b185eed7a",
   "metadata": {},
   "source": [
    "## 2010"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9d345b-6a27-42a5-908f-e5bd54642034",
   "metadata": {},
   "source": [
    "## 2011"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdc0c03-284e-4a46-8b2d-32e73de03d66",
   "metadata": {},
   "source": [
    "## 2012\n",
    "The data from this year was used to develop the initial prototypes for this workflow.  There are around 4.5 M miniseed waveforms from around 1000 events.   A number of bugs were worked out on these data, but reprocessing should go smoothly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f4b1d6-1650-438a-bb23-26d49e7eb07b",
   "metadata": {},
   "source": [
    "## 2013\n",
    "The data from this year was used for second order prototyping after the 2012 data were processed.   The data 2013 are similar to 2012 in size but with only just under 900 events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216ad965-746f-4973-9da9-2a3bfbc97fda",
   "metadata": {},
   "source": [
    "## 2014\n",
    "Our download of the data from 2014 has some serious issues that have not been resolved.   The data set downloaed earlier has 1052 event files, which is comparable to 2012 and 2013.   However, 2012 and 2013 have around 4 M waveform segments created with index_mseed_files while the data we have archived had around 45 M segments.  That created a huge slowdown in all database operations and I had problems getting bulk_normalize work at all.   Inspection of the MongoDB log file shows only that the error was preceded by a \"Slow Query\" warning.   The exception thrown shows a timeout error when bulk_normalize was running count_documents for the full wf_miniseed collection.  I (glp) know from interactive work with this database that all operations of wf_miniseed were very slow so I am nearly certain this problem was created by a bloated wf_miniseed collection.  It also illustrates why working with these data in yearly chunks is prudent.   In any case, there is a solution that I haven't implemented.   In line normalize calls are not subject to this problem because the only database operations they do is to load the data for matching in normalize.\n",
    "\n",
    "I found pretty clear evidence these data are seriously messed up with the data fragmented for some mysterious reason.  Evidence for this came from two simple algorithms:\n",
    "1.  Counting the number of waveforms for each station.  Some had reasonable numbers around 4000 but there were also many with an order of magnitude more indexed segments (i.e. 50-60 thousand).   That is either a gap problem or the data in the files being scrambled by some unknown problem in the download.\n",
    "2.  I ran OriginTimeMatcher and counted matches with wf_miniseed documents.   Only about 10% were hits which is consistent with 1.\n",
    "\n",
    "To process the 2014 data we will need to rerun the download procedure to see if the problem persists.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b83ac7-0dae-4cb0-8ce8-623e3b2eaf91",
   "metadata": {},
   "source": [
    "## 2015"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
