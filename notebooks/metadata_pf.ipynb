{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use of Metadata, MetadataDefinitions, and AntelopePf\n",
    "\n",
    "## Metadata\n",
    "\n",
    "MsPASS makes extensive use of a C++ object we call Metadata.  A Metadata object is one of a many options we could have used to define the generalization of the idea of a header familar to many seismologists.  Headers are used, for example, in SAC and all seismic reflection packages we are aware of.   A \"header\", however, can be thought of as a implementation detail for a more general concept:  fetching parameters with a name-value pair relationship.  We use Metadata in preference to a python dict because the implementation is cleaner at the C++ level.  It is also, in principle, faster since the same methods visible through python wrapper are accesible in C++ code.   The purpose of this tutorial is to give users familiarity of this core class used for a wide variety of purposes.  \n",
    "\n",
    "This tutorial first teaches how to utilize the Metadata object in python.  When that is understood, we move to the AntelopePf, which is a child of Metadata with expanded capabilities.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to tell python what a Metadata object is.  We use the following common python incantation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.ccore.utility import Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create an empty Metadata container in the standard python way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "md=Metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Metadata container is more flexible, but in MsPASS to provide a cleaner mapping to MongoDB we restrict the contents to the four lowest common demoninator types native to most computer languages: \n",
    "* real numbers, which are always promoted in MsPASS to double (64 bit floats)\n",
    "* integers, which are always promoted to 64 bit signed ints\n",
    "* character strings, which are handled in C++ as string objects and wrapped to python strings.  i.e. the conversion between C++ and python strings should be seamless.\n",
    "* booleans - meaning values that are \"True\" or \"False\" in python.   \n",
    "\n",
    "There are putters can create each of these core types by a clear name convention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.put_double(\"real_example\",10.45)\n",
    "md.put_long(\"int_example\",42)\n",
    "md.put_string(\"foo\",\"bar\")  # the classic programmer idiom\n",
    "md.put_bool(\"bool_example\",True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also \"overloaded\" versions of the same that can depend on the python interpreters rules for assigning type from a literal.   These four lines do nearly the same thing as the previous four lines of python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.put(\"overreal\",99.45)\n",
    "md.put(\"overint\",2)\n",
    "md.put(\"overstr\",\"foobar\")\n",
    "md.put(\"overbool\",False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C++ code could use operator<< to dump the contents of md, but we currently have no equivalent in python.  For now you need to use getters.  This small example shows getters to pull a subset of the 8 entries currently stored in md."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real example = 99.45\n",
      "int example= 42\n",
      "string example= bar\n",
      "boolean example= True\n"
     ]
    }
   ],
   "source": [
    "x=md.get_double('overreal')\n",
    "i=md.get_long('int_example')\n",
    "s=md.get_string('foo')\n",
    "b=md.get_bool('bool_example')\n",
    "print(\"real example =\",x)\n",
    "print(\"int example=\",i)\n",
    "print(\"string example=\",s)\n",
    "print(\"boolean example=\",b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In interactive scripts or for testing it is often helpful to know what data are defined.  For that purpose we provide the keys method used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'foo', 'real_example', 'overreal', 'overstr', 'overbool', 'overint', 'int_example', 'bool_example'}\n"
     ]
    }
   ],
   "source": [
    "keys=md.keys()\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Python Class\n",
    "\n",
    "A thorny problem in real data analysis with header attributes, which is what Metadata generalizes, is that the type of a parameter must match what is stored.  For example, if we tried to fetch the parameter \"foo\" from md as an integer or real value, the result would make no sense.    The Metadata class will throw a C++ exception that your python code may need to handle.   The current wrappers for Metadata cast the exception message to a stock python RuntimeError. Hence, if you have a section of code where a key-value pair may not be defined or is subject to a type mismatch, you should use a construct like the the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MsPASSError('Error in Metadata get method.   Type mismatch in attem to get data with key=foo\\nboost::any bad_any_cast wrote this message:  \\nboost::bad_any_cast: failed conversion using boost::any_cast\\nTrying to convert to data of type=float\\nActual entry has type=std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >\\n', <ErrorSeverity.Suspect: 2>)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    s=md.get_string(\"foo\")  # this will work\n",
    "    x=md.get_double(\"foo\")  # this will throw an exception we need to handle\n",
    "except Exception as e: \n",
    "    print(repr(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates a best-practice error handler that uses the base class Exception and the built in repr function to convert the generic error to a more readable form.  It is remains a bit ugly because of mismatch is handling the newline (\\n) character in ipython, but the gist of the error message should be clear.\n",
    "\n",
    "The general problem of data types with attributes like those we extract from Metadata has two end members in the computing world:  (1) python, which is completely agnostic about type, and (2) strongly typed languages like C/C++ and FORTRAN.  Since MsPASS is a hybrid with C++ implementation of some compute intensive, core algorithms and python for higher level processing this clash in concept has to be handled. An additional constraint comes from the use of MongoDB or any database engine.  Chaos will reign if a name key is used for attributes of different types; a problem far too easy to create even accidentally with python. Since the main purpose of MsPASS is to provide a framework for handling long-running, compute intensive data processing jobs, we have to enforce some type restrictions on attributes to avoid mysterious downstream behaviour and bugs that are difficult to impossible to find.  We do this through a core mspass object we call *Schema* because it is used to enforce types in database reads and writes.  More about its use in database interactions is described elsewhere.  Here we focus on how to use the Schema class to handle Metadata keys and types properly;.    \n",
    "\n",
    "Any function using MsPASS data objects that manipulate Metadata (e.g. posting a computed attribute to a Seismogram object) should include the following import definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.db.schema import MetadataSchema\n",
    "mdsc=MetadataSchema()\n",
    "TimeSeries_schema=mdsc.TimeSeries\n",
    "Seismogram_schema=mdsc.Seismogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we first create an instance of an overall schema class used to define recognized Metadata keys.  The last two lines extract specific instances for TimeSeries and Seismogram objects.  The two have slightly different definitions because of a difference in concept.  For example, a TimeSeries might contain the angle definitions hang and vang to define the orientation of the sensor, but the same attributes would be meaningless to misleading if posted to a Seismogram object (an assembled three-component bundle of data which have 3 vector values at each time step).  \n",
    "\n",
    "We can compare their defined keys the code in the next box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeries defined Metadata keys\n",
      "dict_keys(['_id', 'npts', 'delta', 'sampling_rate', 'starttime', 'time_standard', 'calib', 'site_id', 'channel_id', 'source_id', 'net', 'sta', 'site_lat', 'site_lon', 'site_elev', 'site_starttime', 'site_endtime', 'loc', 'chan', 'channel_hang', 'channel_vang', 'channel_lat', 'channel_lon', 'channel_elev', 'channel_edepth', 'channel_starttime', 'channel_endtime', 'source_lat', 'source_lon', 'source_depth', 'source_time', 'source_magnitude'])\n",
      "Seismogram defined Metadata keys\n",
      "dict_keys(['_id', 'npts', 'delta', 'sampling_rate', 'starttime', 'time_standard', 'calib', 'site_id', 'channel_id', 'source_id', 'tmatrix', 'net', 'sta', 'loc', 'site_lat', 'site_lon', 'site_elev', 'site_starttime', 'site_endtime', 'source_lat', 'source_lon', 'source_depth', 'source_time', 'source_magnitude'])\n"
     ]
    }
   ],
   "source": [
    "mdkeys=TimeSeries_schema.keys()\n",
    "print('TimeSeries defined Metadata keys')\n",
    "print(mdkeys)\n",
    "mdkeys=Seismogram_schema.keys()\n",
    "print('Seismogram defined Metadata keys')\n",
    "print(mdkeys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the type of any key through the type method.  This script prints the full table of types for all defined keys for a Seismogram object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_id  has type= <class 'bson.objectid.ObjectId'>\n",
      "npts  has type= <class 'int'>\n",
      "delta  has type= <class 'float'>\n",
      "sampling_rate  has type= <class 'float'>\n",
      "starttime  has type= <class 'float'>\n",
      "time_standard  has type= <class 'str'>\n",
      "calib  has type= <class 'float'>\n",
      "site_id  has type= <class 'bson.objectid.ObjectId'>\n",
      "channel_id  has type= <class 'list'>\n",
      "source_id  has type= <class 'bson.objectid.ObjectId'>\n",
      "tmatrix  has type= <class 'list'>\n",
      "net  has type= <class 'str'>\n",
      "sta  has type= <class 'str'>\n",
      "loc  has type= <class 'str'>\n",
      "site_lat  has type= <class 'float'>\n",
      "site_lon  has type= <class 'float'>\n",
      "site_elev  has type= <class 'float'>\n",
      "site_starttime  has type= <class 'float'>\n",
      "site_endtime  has type= <class 'float'>\n",
      "source_lat  has type= <class 'float'>\n",
      "source_lon  has type= <class 'float'>\n",
      "source_depth  has type= <class 'float'>\n",
      "source_time  has type= <class 'float'>\n",
      "source_magnitude  has type= <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "for k in mdkeys:\n",
    "    t=Seismogram_schema.type(k)\n",
    "    print(k,' has type=',t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While writing a script if you aren't sure what a key represents you can use the concept method.  Here, for example, we ask for a brief description of what the attribute \"tmatrix\" should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three-component data's transformation matrix\n"
     ]
    }
   ],
   "source": [
    "s=Seismogram_schema.concept('tmatrix')\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we chose the *tmatrix* attribute as an example because it is not represented by a simple numeric or string type.   The transformation matrix is a 3x3 matrix which we store as a list of 9 floats.  This is a good illustration of the flexibility of Metadata in allowing more than simple types line integers and floats.\n",
    "\n",
    "## Aliases\n",
    "\n",
    "To provide a mechanism for MsPASS to legacy packages like SAC, the Schema object has a generic aliases mechanism.   For example, if we wanted to know alternative names that can be handled automatically for the attribute called \"source_lat\", we can issue this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['source.lat', 'EVLA', 'origin.lat', 'source_lat']\n"
     ]
    }
   ],
   "source": [
    "print(Seismogram_schema.aliases('source_lat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that we can use the SAC name, EVLA, or the Antelope name, origin.lat, or a similar name source.llat as an alternative to source_lat and it will be handled automatically by database readers and writers.\n",
    "\n",
    "There are a collection of other useful methods defined in schema objects for dealing with aliases.  The following script illustrates their use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVLA is a valid alias\n",
      "The unique key for EVLA for database records is  source_lat\n",
      "The keyword source_lat has one or more aliases defined\n",
      "Valid aliases are ['source.lat', 'EVLA', 'origin.lat', 'source_lat']\n"
     ]
    }
   ],
   "source": [
    "if(Seismogram_schema.is_alias('EVLA')):\n",
    "    print('EVLA is a valid alias')\n",
    "    ukey=Seismogram_schema.unique_name('EVLA')\n",
    "    print('The unique key for EVLA for database records is ',ukey)\n",
    "if(Seismogram_schema.has_alias('source_lat')):\n",
    "    print('The keyword source_lat has one or more aliases defined')\n",
    "    print('Valid aliases are',Seismogram_schema.aliases('source_lat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of aliases is an important component of MsPASS to simplify utilizing legacy software as part of a workflow.   A typical example would be some specialized program that was built around the SAC data format or an anticipated development in MsPASS of running SAC from a MsPASS workflow.   To support that type of application MsPASS has two methods in schema classes called *add_alias*.   *add_alias* can be used to add an alias value to any key.  The inverse is called *clear_aliases* and resets all aliases that had been set previously.   It is called, for example, internally before saving the Metadata contents of a TimeSeries or Seismgogram to the database to reset all aliases.  That assures all data from a common concept have common names when stored in MongoDB.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "The schema objects contain additional, advanced methods for handling a feature of MongoDB calls \"normalization\".   (See https://docs.mongodb.com/manual/core/data-model-design/ for the concepts of normalization in MongoDB)   The key point is that some attributes like receiver coordinates and response information are better stored in a single place and found by a cross-reference mechanism rather than being duplicated many times or risk mistakes from incorrect associations.   Methods available to support this feature are:\n",
    "\n",
    "1.  *is_ normalized* - test whether an attribute is expected to be normalized\n",
    "2.  *collection* - return the MongoDB collection name where the master of an attribute should be located.\n",
    "3.  *readonly, writeable* - test for whether an attribute is marked readonly.   Normalized data are normally marked readonly* (immutable) because they should be set only on construction and never altered by a processing workflow.   \n",
    "4.  *set_readonly, set_writeable* - backdoor methods to set (set_readonly) or override locks (set_writeable) on an attribute.   These functions should not be used unless essential.   \n",
    "\n",
    "Finally, there are two methods for manually defining new attributes not defined in the master namespace.\n",
    "\n",
    "1. *add* - define a new attribute with type and concept properties\n",
    "2. *add_alias* - define a new alias for an existing attribute. \n",
    "\n",
    "These also should be used with caution as it is preferable for custom applications to edit the master list used to construct MetadataDefinitions objects.  \n",
    "\n",
    "TODO:   add and add_alias are currently not working.  The following will not work as it should.  Until this is cleared users are warned to avoid using this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial metadata with MsPASS names\n",
      "Value with key= source_depth  is  10.0\n",
      "Value with key= source_lon  is  -75.0\n",
      "Value with key= npts  is  100\n",
      "Value with key= source_lat  is  10.0\n",
      "Value with key= site_lat  is  45.0\n",
      "Value with key= site_lon  is  55.0\n",
      "Metadata after applying aliases\n",
      "Value with key= source_depth  is  10.0\n",
      "Value with key= source_lon  is  -75.0\n",
      "Value with key= npts  is  100\n",
      "Value with key= source_lat  is  10.0\n",
      "Value with key= site_lat  is  45.0\n",
      "Value with key= site_lon  is  55.0\n"
     ]
    }
   ],
   "source": [
    "from mspasspy.ccore.seismic import TimeSeries\n",
    "def printmd(md):\n",
    "    keys=md.keys()\n",
    "    for k in keys:\n",
    "        print(\"Value with key=\",k,\" is \",md[k])\n",
    "\n",
    "# We create an seismogram object to demonstrate these are used for data objects\n",
    "sacmd=TimeSeries(100)\n",
    "sacmd.put_double(\"source_lat\",10.0)\n",
    "sacmd.put_double(\"source_lon\",-75.0)\n",
    "sacmd.put_double(\"source_depth\",10.0)\n",
    "sacmd.put_double(\"site_lat\",45.0)\n",
    "sacmd.put_double(\"site_lon\",55.0)\n",
    "print(\"Initial metadata with MsPASS names\")\n",
    "printmd(sacmd)\n",
    "# We create a dict as a convenient container to define the aliases in this example\n",
    "sacaliases={\n",
    "    'source_lat':'EVLA',\n",
    "    'source_lon':'EVLO',\n",
    "    'source_depth':'EVDP',\n",
    "    'site_lat':'STLA',\n",
    "    'site_lon':'STLO'\n",
    "}\n",
    "for k in sacaliases:\n",
    " TimeSeries_schema.add_alias(k,sacaliases[k])\n",
    "print(\"Metadata after applying aliases\")\n",
    "printmd(sacmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:  revise when these methods are working\n",
    "\n",
    "Notice that apply_aliases silently skipped the inconsistency that *site_elev* is not defined in sacmd.  That was an intentional design because we expect global alias lists (e.g. applying a set of fixed aliases for sac, obspy, or Antelope) to be the norm.  \n",
    "\n",
    "Also notice that three additional Metadata attributes appeared when the printmd function was called:  starttime(real), npts (integer), and delta (real).   These are three attributes wired into private variables in the mspass data objects.   The C++ api guarantees these Metadata name-value pairs are consistent with internal values.  They appear in this output because the printmd lists all defined Metadata fields.  Note finally printmd uses the alternative access method for Metadata fields using the key in the same syntax as a python dict.   For purely pythonic interactions that approach is appropriate because python just returns the right type.  In contrast calling a method like get_double that is dogmatic about type will generate an exception if the type does not match.  Which you should use will depend on context.\n",
    "\n",
    "The *clear_aliases* method of *MetadataDefiniions* will restore all entries to the standard value as illustrated here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value with key= source_depth  is  10.0\n",
      "Value with key= source_lon  is  -75.0\n",
      "Value with key= npts  is  100\n",
      "Value with key= source_lat  is  10.0\n",
      "Value with key= site_lat  is  45.0\n",
      "Value with key= site_lon  is  55.0\n"
     ]
    }
   ],
   "source": [
    "TimeSeries_schema.clear_aliases(sacmd)\n",
    "printmd(sacmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *apply_aliases* and *clear_aliases* methods provide a generic support for working with different namespaces.  The cost of applying them is not large, but not tiny either.  For efficiency avoid unnecessary alias definitions by grouping processes using a common namespace when possible.   On the other hand, when a block of processing steps requiring aliases are finished you should immediately call *clear_aliases* to avoid downstream errors.  We emphasize *clear_aliases* resets ALL metadata defined as an alias by the MetadataDefinitions object.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## AntelopePf\n",
    "\n",
    "The Metadata object is an implementation of the core idea of fetching an attribute defined by a name-value pair, where the name is what is commonly called a key and the value is the data we want to associate with the key.   As we just saw this maps exactly into the concept of a header that has been proven a useful concept since the earliest days of seismic data processing pioneered by the oil and gas industry in the 1960s.  \n",
    "\n",
    "The *AntelopePf* is an implementation of a different, but related problem in all data processing.  Almost any algorithm has at least one tunable parameter that needs to be defined.  In general, the more generic an algorithm is the more parameters will be needed to define the way it should behave.  In the early days of data processing this problem was often solved by creating special format \"input files\" that a program read at startup.   Computer scientists realized decades ago that this was a generic problem and thus had generic solutions.   Hundreds of solutions to this problem exist with configuration files of various formats.  Today the most common is probably xml. We elected to not use xml for our initial development of mspass for two reasons:\n",
    "1.  xml is not a human readable format, but a language for robots (computers).  It is very hard for a human being to create a valid xml file by entering the data manually.  We needed a format that was easy for a human to construct.\n",
    "2.  Many seismologists utilize BRTT's \"parameter files\", because of the generous license agreement BRTT provides for U.S. scientists.   Furthermore, both of primary authors of MsPASS were familiar with parameter files and we had an open source implementation we could build on from Pavlis's plane wave migration code.\n",
    "\n",
    "We thus adopted the \"parameter files\" syntax to implement an extension of Metadata we call an *AntelopePf*. The *AntelopePf* is a child of *Metadata* so the same methods introduced for *Metadata* can be used for an *AntelopePf*.   The use, however, is more than a little subtle and is best understood from an example.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a concrete example of the kind of complex parameter file that *AntelopePf* was designed to handle.  The following is the default configuration for a new deconvolution routine in MsPASS we call CNR3CDecon (for Colored Noise 3C (Three-component) Deconvolution):\n",
    "```\n",
    "########################################################################\n",
    "operator_nfft 4096\n",
    "#damping_factor 1000.0\n",
    "damping_factor 1.0\n",
    "snr_regularization_floor 2.0\n",
    "target_sample_interval 0.05\n",
    "deconvolution_data_window_start -2.0\n",
    "deconvolution_data_window_end 30.0\n",
    "time_bandwidth_product 4.5\n",
    "number_tapers 8\n",
    "shaping_wavelet_dt 0.05\n",
    "shaping_wavelet_type ricker\n",
    "shaping_wavelet_frequency 1.0\n",
    "shaping_wavelet_frequency_for_inverse 0.5\n",
    "noise_window_start -30.0\n",
    "noise_window_end -5.0\n",
    "\n",
    "taper_type cosine\n",
    "CosineTaper &Arr{\n",
    "  data_taper &Arr{\n",
    "    front0 -2.0\n",
    "    front1 -1.0\n",
    "    tail1 27.0\n",
    "    tail0 29.5\n",
    "  }\n",
    "  wavelet_taper &Arr{\n",
    "   front0 -0.75\n",
    "   front1 -0.25\n",
    "   tail1 2.5\n",
    "   tail0 3.0\n",
    "  }\n",
    "}\n",
    "LinearTaper &Arr{\n",
    "  data_taper &Arr{\n",
    "    front0 -2.0\n",
    "    front1 -1.0\n",
    "    tail1 27.0\n",
    "    tail0 29.5\n",
    "  }\n",
    "  wavelet_taper &Arr{\n",
    "   front0 -0.75\n",
    "   front1 -0.25\n",
    "   tail1 2.5\n",
    "   tail0 3.0\n",
    "  }\n",
    "}\n",
    "########################################################################\n",
    "\n",
    "```\n",
    "We have supplied a copy of the data above in a file called data/test.pf.  You should then be able to load this file with the following:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.ccore.utility import AntelopePf\n",
    "pf=AntelopePf('data/test.pf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters outside the curly brackets and the \"&Arr\" tags are handled by Metadata methods - they are simple name value pairs.  Here are a couple examples.  You can extend these to test your knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple parameter operator_nfft has this value: 4096\n",
      "Simple parameter damping_factor has this value: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Simple parameter operator_nfft has this value:\",pf.get_long(\"operator_nfft\"))\n",
    "print(\"Simple parameter damping_factor has this value:\",pf.get_double(\"damping_factor\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*AntelopePf* extends *Metadata* with two primary methods:   *get_branch* and *get_tbl*.  This little code fragment illustrates the *get_branch* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata keys for CosineTaper branch  (an empty list) set()\n",
      "Metadata keys found for wavelet_taper branch:  {'tail0', 'front0', 'front1', 'tail1'}\n"
     ]
    }
   ],
   "source": [
    "pfb1=pf.get_branch('CosineTaper')\n",
    "# Note the example pf has no simple name-value pairs under the CosineTaper tag.  \n",
    "# This illustrates that is o\n",
    "keys=pfb1.keys()\n",
    "print('Metadata keys for CosineTaper branch  (an empty list)',keys)\n",
    "pfb2=pfb1.get_branch('wavelet_taper')\n",
    "keys=pfb2.keys()\n",
    "print('Metadata keys found for wavelet_taper branch: ',keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other extension of an AntelopePf beyond a name-value pair is what they defines with the symbol\n",
    "\"&Tbl&\".  An AntelopePf \"&Arr\" is the same concept as a python dict but with the \"value\" being another dict.\n",
    "Parameters outlined by an &Tbl, in constrast, map directly into a python list of strings.\n",
    "The required approach is the strings have to be parsed to define the set of parameters you need that\n",
    "are stored by this mechanism.   \n",
    "\n",
    "An example should help clarify this idea.\n",
    "Here is the default parameter file for the Antelope contrib program export_to_mspass that can be used to take a data set defined by an Antelope database and import it to MsPASS.\n",
    "```\n",
    "required &Tbl{\n",
    "dt delta real\n",
    "origin.depth source_depth real\n",
    "origin.lat source_lat real\n",
    "origin.lon source_lon real\n",
    "origin.time source_time real\n",
    "site.lat site_lat real\n",
    "site.lon site_lon real\n",
    "site.elev site_elev real\n",
    "nsamp npts int\n",
    "sta sta string\n",
    "evid source_id int\n",
    "U11 U11 real\n",
    "U12 U12 real\n",
    "U13 U13 real\n",
    "U21 U21 real\n",
    "U22 U22 real\n",
    "U23 U23 real\n",
    "U31 U31 real\n",
    "U32 U32 real\n",
    "U33 U33 real\n",
    "}\n",
    "optional &Tbl{\n",
    "origin.mb mb real\n",
    "origin.ms ms real\n",
    "arrival.iphase iphase string\n",
    "assoc.phase phase string\n",
    "orid orid int\n",
    "}\n",
    "```\n",
    "The above data are contained in another file data/test2.pf.   You should be able to load it by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "origin.mb mb real\n",
      "origin.ms ms real\n",
      "arrival.iphase iphase string\n",
      "assoc.phase phase string\n",
      "orid orid int\n"
     ]
    }
   ],
   "source": [
    "pf2=AntelopePf('data/test2.pf')\n",
    "tbllist=pf2.get_tbl('optional')\n",
    "print(type(tbllist))\n",
    "for entry in tbllist:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example illustrates the *get_tbl* method returns the data between the \"optional &Tbl{\" and the \"}\" at the end of the data file as a python list of strings - one list element per line.   This can be used by any program where the input can be defined as a sequence of lines.  This example uses a format where token 1 is the antelope database attribute name that is to be fetch, token 2 is the name that is to be assigned for the export file, and token 3 is the name used to define the type of the data expected (Antelope's database has type constraints for the same reasons we noted above).  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
