{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f8408e2-8694-4d3f-8437-fdba5fabc518",
   "metadata": {},
   "source": [
    "# Earthscope Course 2024 - Session 2\n",
    "## *Gary L. Pavlis* and *Ian Wang*\n",
    "\n",
    "## Overview\n",
    "The learning objectives of this tutorial are:\n",
    "1. Gain basic skills in using the MongoDB Database to manage seismic data.\n",
    "2. Gain a more complete understanding of content of the different seismic data types used in MsPASS and how they are handled with the MongoDB database inside MsPASS.\n",
    "3. Understand what we mean by \"atomic data\" versus \"ensembles\".   \n",
    "\n",
    "In session 1 we utilized MongoDB but gave you examples that are more-or-less like an incantation for a magic trick.   Through this tutorial our aim is to help you see through the magicians trick to know how it is done so you can use MongoDB effectively in your work.  The integrated database of MsPASS is one of the most important reasons MsPASS is the best solution today for handling the enormous volume of seismology data available today.  \n",
    "\n",
    "Numerous pedagogic materials exist online for learning MongoDB, but this notebook focuses on key features the authors has found useful in seismology research.  It is best used in conjunction with two other sources:\n",
    "1.  The section of the User's Manual titled \"[Using MongoDB with MsPASS](http://www.mspass.org/user_manual/mongodb_and_mspass.html)\".\n",
    "2.  As with most modern IT topics a web search for details of some topics addressed in this tutorial may be helpful if the MsPASS User's Manual proves inadequate.\n",
    "\n",
    "Embedded within the tutorial on MongoDB it is natural to teach the concepts for objectives 2 and 3.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8c262b-4e39-44cb-8c76-9f02ab571088",
   "metadata": {},
   "source": [
    "## MongoDB Core Concepts\n",
    "### Client-server model\n",
    "MongoDB is a client-server system.  That bit of jargon \n",
    "has some important implications:\n",
    "\n",
    "1.  All database commands issued from python are not executed directly by the python interpreter.  Instead instructions are sent to the MongoDB server.   In MsPASS the server is launched inside a container.   Unless you are running this notebook on a cluster with multiple nodes, you can verify the server is running by launching a terminal in the jupyterlab interface and running the command `ps -A`.  You should get output similar to the following that shows the server as the CMD with the name `mongod`:\n",
    "```\n",
    "root@b0d79c4cc440:/home/scoped# ps -A\n",
    "  PID TTY          TIME CMD\n",
    "    1 ?        00:00:00 tini\n",
    "    8 ?        00:00:00 start-mspass.sh\n",
    "   15 ?        00:07:27 dask-scheduler\n",
    "   21 ?        00:06:47 dask-worker\n",
    "   22 ?        00:01:44 mongod\n",
    "   23 ?        00:00:44 jupyter-lab\n",
    "   34 ?        00:00:00 python3.10\n",
    "   37 ?        00:10:43 python3.10\n",
    "  154 ?        00:00:20 python\n",
    "  364 ?        00:00:01 python\n",
    " 1010 pts/0    00:00:00 bash\n",
    " 1036 pts/0    00:00:00 ps\n",
    "```\n",
    "2.  All database IO passes through a network data connection on network \"port number 27017\".   That is important to know as a fundamental issue because a network communication channel is not the fastest data pipe on most computers. It can also create a need to work around a firewall on some systems.  \n",
    "3.  To communicate with MongoDB, your program must create a connection to the \"server\".  In the jargon of modern computing you have to create a \"client\" that will act as your agent to talk to the arrogant MongoDB \"server\" (the mongod program running in the background).  \n",
    "\n",
    "The \"client-server model\" is ubiquitous in the modern computing environment.   To show that here are three we used one way or the other in session 1:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb75c3e3-0c75-4624-851c-1275c0e8a7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of dbclient is  <class 'mspasspy.db.client.DBClient'>\n"
     ]
    }
   ],
   "source": [
    "from obspy.clients.fdsn import Client as ObspyClient\n",
    "from mspasspy.client import Client\n",
    "obspy_client = ObspyClient()\n",
    "mspass_client = Client(database_name='Earthscope2024')\n",
    "dbclient = mspass_client.get_database_client()\n",
    "print(\"The type of dbclient is \", type(dbclient))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8490264-3aaa-4130-84cd-4ddcb874ce36",
   "metadata": {},
   "source": [
    "Noting:\n",
    "1. *obspy_client* is an instance of obspy's FDSN web service client we used to interact with FDSN web services.\n",
    "2. *mspass_client* is a top-level \"client\" use din MsPASS.   It is more-or-less a container we use to interact with the two main services that are central to MsPASS:  (a) MongoDB and (b) the \"client\" used to interact with the parallel \"scheduler\" (dask or spark) that we will learn about in session 3.\n",
    "3. *dbclient* is an instance of the \"client\" mentioned above for interacting with MongoDB.  Notice we fetch it from *mspass_client* object with it's `get_database_client` method.  \n",
    "A geeky detail worth noting here, which is illustrated by the print statement in the last line, is that the symbol *dbclient* is an instance of a class called `DBClient`.  `DBClient` is a \"subclass\" of `pymongo.MongoClient`.  I point that out because all internet sources that are MongoDB introductions will create an instance of [MongoClient](https://pymongo.readthedocs.io/en/stable/api/pymongo/mongo_client.html)` instead of the MsPASS extension [DBClient](http://www.mspass.org/python_api/mspasspy.db.html#module-mspasspy.db.client).  An important \"extension\" DBClient adds is illustrated by the next code box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09f3f532-0e7f-41a0-8f89-031e42a217a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = dbclient.get_database(\"Earthscope2024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878cd8f2-7e47-45c5-8e5b-1bdcdebc1c90",
   "metadata": {},
   "source": [
    "This incantation runs the `get_database` \"method\" of the class called `DBClient`.   It returns what we call a \"database handle\" in the User's Manual.   The MsPASS \"database handle\" is a python class that is itself a subclass  of another pymongo class.  Both have the name `Database`, but the [MsPASS version](http://www.mspass.org/python_api/mspasspy.db.html#mspasspy.db.database.Database) adds a number of extensions for handling of seismic data.   The main ones of interest are readers and writers for seismic data objects, station metadata, and source metadata.  A key point is almost all MsPASS workflows begin with a variation of the combination of the two python code boxes above.   In particular, this is a copy of what we ran in code box 2 of Session 1.   A version of this changing only the database name, which in this case is \"Earthscope2024\", should appear at the top of almost any MsPASS python script/notebook.   (This one is disabled as we already created *db* so we don't need it here.)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "baa30cff-b81c-4b4f-9608-c8b52ea215e4",
   "metadata": {},
   "source": [
    "from mspasspy.db.database import Database   # This isn't strictly needed but used here because db set below is an instance of this class\n",
    "import mspasspy.client as msc\n",
    "dbclient=msc.Client(database_name='Earthscope2024')\n",
    "db = dbclient.get_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cacdae8-d3ef-41b0-803b-e79cf92b53a9",
   "metadata": {},
   "source": [
    "When you call the `get_database` method as shown above the \"handle\" is created/constructed and can be accessed for the rest of your python workflow with the symbol you put on the left hand side of the expression (`db` in this example).  That name, of course, can be anything you want it to be, but for all examples in the MsPASS documentation we used `db` as a standard symbol to reduce confusion, but that should be viewed as simply a notation convention not a rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2894ea2b-a29a-44ee-bf69-94de61092783",
   "metadata": {},
   "source": [
    "### Documents and Collections\n",
    "In the lecture part of this session we will discuss the MongoDB jargon terms `document` and `collection` at length.   We will not repeat that material here, but note from here on I assume you know what those two terms mean.   If you don't know what these terms mean consult the [Using MongoDB with MsPASS](http://www.mspass.org/user_manual/mongodb_and_mspass.html) section of the User's Manual or a bewildering array of internet source before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6b1788-2105-4c8d-a116-d0ce94d5ddad",
   "metadata": {},
   "source": [
    "## CRUD\n",
    "A near universal mnemonic found in books on database theory and online tutorials is the acronymn CRUD.  CRUD is short for Create-Read-Update-Delete.  It is used to as a mnenomic to remember those four primry functions any operational database must be capable of doing.   In this class we will focus exclusively on  the C (Create==writers) and R (readers).   Reading and writing are pretty much essential for all MsPASS workflows.  Updates and deletes, in contrast, are rarely needed and, in fact, are usually ill advised and best done not within a larger workflow but as a sidebar to fix some problem.  A notable exception is the [normalize_mseed]() function we used in session 1 to add cross-reference ids between `wf_miniseed` and `channel` documents.  That is a pure update function, but it does the work more efficiently due to some tricks done under the hood.   In any case, to reduce information overload this class will focus on read and write operations.  When you use MsPASS if you understand the syntax for the C and R functions of MongoDB the forms for the U and D functions are completely logical.   You can also consult the sections in the [User Manual on CRUD operations](http://www.mspass.org/user_manual/CRUD_operations.html) and the section titled [\"Using MongoDB with MsPASS\"](http://www.mspass.org/user_manual/mongodb_and_mspass.html).\n",
    "\n",
    "### Create\n",
    "#### CommandCursor concept\n",
    "The first letter in the CRUD acronynm is \"Create\".  For all applications some form of \"create\" is an essential first step to put some kind of data into your database.  We already did that in the first session of this class.  There we populated several collections.   We can see what they are with the `list_collecions` method of the `Database` class.  This shows the usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "935b13dd-22ee-4d31-aed7-e3bb49ff0319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wf_TimeSeries\n",
      "channel\n",
      "abortions\n",
      "wf_Seismogram\n",
      "elog\n",
      "wf_miniseed\n",
      "source\n",
      "cemetery\n",
      "history_object\n",
      "fs.chunks\n",
      "site\n",
      "fs.files\n"
     ]
    }
   ],
   "source": [
    "cursor=db.list_collections()\n",
    "for doc in cursor:\n",
    "    print(doc['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7e2182-0f20-45bb-953a-2b4060c8893a",
   "metadata": {},
   "source": [
    "As you can see there are a lot more there than the \"wf\" collections and \"channel\", \"site\", and \"source\" what were explicitly discussed in the last class.   We will examine some of them in more detail later, but for now focus on the block of python code that created that output.   The `list_collections` method returned a special data type used in pymongmo.  To see what that is consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf82687-6647-4ebd-8bae-433debbfb17a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pymongo.command_cursor.CommandCursor"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a62519-c7b7-4657-ad15-8dcf0afd5c66",
   "metadata": {},
   "source": [
    "Every database system I know of implements some version of the concept encapsulated by the pymongo class called a [CommandCursor](https://pymongo.readthedocs.io/en/stable/api/pymongo/command_cursor.html).   A \"cursor\" is a standard return from any query like operation in any database system.  A MongoDB `CommandCursor` is technically a __[forward iterator](https://www.boost.org/sgi/stl/ForwardIterator.html)__.   That means it acts like a list that can only be traversed \"forward\" with a construct like that above.   It is not at all the same thing, however, as a python list.   It is a handle that interacts with the database to sequentially return documents.  The above example would not require any complexity.  Where it is fundamentally different is if the number of elements in the list exceed the memory buffer size of the client that handles io with the MonogDB server.  Then the client-server pair manage the grungy work of trying to keep the memory buffer full and assuring the client does no have to wait for data to arrive.  The important thing that means is that when reading a very large amount of data (e.g. processing millions of TimeSeries objects driven by wf_miniseed records) sequential reads with a cursor almost never have to wait for data.  In addition, we will see examples below where the understanding that a mostly acts [CommandCursor](https://pymongo.readthedocs.io/en/stable/api/pymongo/command_cursor.html) mostly acts like a python list is fundamental to many database driven algorithms.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4505be-ab08-442f-bae4-ef1ed74254be",
   "metadata": {},
   "source": [
    "#### MsPASS writers\n",
    "If you read books and online tutorials on MongoDB you will find that the standard \"Create\" functionality is defined by two \"collection-level\" methods called `insert` and `insert_many`.  The pymongo documentation on both can be found [here](https://www.w3schools.com/python/python_mongodb_insert.asp).   Both have their uses, but are rarely used in seismic processing with MsPASS.   The fundamental reason is that both `insert` and `insert_many` are low-level operators that only work on \"documents\" (i.e. python dictionaries).  The data we aim to manage with MongoDB in MsPASS are more complex data objects that cannot always be reduced to \"documents\" or need to be converted to that form.   For that reason in MsPASS we have implemented the following high level writers that do most \"Create\" operations.  Examples of all of these can be found by reviewing the notebook from session 1 of this class:\n",
    "1. [index_mseed_file](http://www.mspass.org/python_api/mspasspy.db.html#mspasspy.db.database.Database.index_mseed_file) is used to scan a file and creating one or more wf_miniseed documents that define an index for waveform segments stored in the file processed.   \n",
    "2. [save_data](http://www.mspass.org/python_api/mspasspy.db.html#mspasspy.db.database.Database.save_data) is used to save all seismic data objects (i.e. `TimeSeries',`Seismogram`, `TimeSeriesEnsemble`, or `SeismogramEnsemble`).   \n",
    "3. [save_inventory](http://www.mspass.org/python_api/mspasspy.db.html#mspasspy.db.database.Database.read_inventory) is used to save station metadata downloaded via web services with obspy and bundled into the obspy [Inventory](https://docs.obspy.org/packages/autogen/obspy.core.inventory.inventory.Inventory.html) object that is more-or-less an image of the [StationXML](https://docs.fdsn.org/projects/stationxml/en/latest/overview.html) data retrieved.  \n",
    "4. [save_catalog](http://www.mspass.org/python_api/mspasspy.db.html#mspasspy.db.database.Database.save_catalog) is used to save earthquake source data downloaded via web services with obspy and bundled into obspy's [Catalog object]().  Like `Inventory` the `Catatlog` object is more-or-less an image of the new [QuakeML format](https://quake.ethz.ch/quakeml) for distributing earthquake source parameters. \n",
    "5. [write_distributed_data](http://www.mspass.org/python_api/mspasspy.io.html#mspasspy.io.distributed.write_distributed_data)  is the parallel writer for seismic data objects.  We will discuss this function in more detail in session 3 of the class.\n",
    "\n",
    "All the MsPASS writers use `insert` and/or `insert_many` operations to save data to MongoDB.   For source and receiver data that process is more straightforward.  Each relevant source or channel/station record in an input QuakeML or StationXML file image is saved as one document in a MongoDB collection (nominally \"source\" and \"channel\" respectively).  Saving seismic data objects is much more complex for three reasons:\n",
    "1.   We recognized that large data storage is a rapidly evolving technology today and we aimed to abstract the process to support multiple versions of what we call \"storage mode\".  The rest of this session will give an overview of that concept.\n",
    "2.   As we will show in the presentation done in parallel with this notebook, the seismic data objects in MsPASS are conceptually defined as composed of four different containers:   (a) the Metadata container that translates directly into a MongoDB document, (b) the sample data, (c) an [error log](http://www.mspass.org/python_api/mspasspy.ccore.html#mspasspy.ccore.utility.ErrorLogger) container, and (d) a [ProcessingHistory](http://www.mspass.org/python_api/mspasspy.ccore.html#mspasspy.ccore.utility.ProcessingHistory) container used to (optionally) store what we call \"object-level history\" (see [this section](http://www.mspass.org/user_manual/processing_history_concepts.html) of the User Manual).\n",
    "3.   We aimed to create a single function to save atomic data and ensemble data.  Ensembles are mostly groups of atomic data, but present some special challenges the `save_data` method needed to handle.   Using the common function to automatically handle those details simplifies usage.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a151eff4-b243-4a63-bb71-1961d99a78ec",
   "metadata": {},
   "source": [
    "#### Storage Mode Options\n",
    "The [save_data](http://www.mspass.org/python_api/mspasspy.db.html#mspasspy.db.database.Database.save_data) method of `Database` has an argument called \"storage_mode\".   It determines how the sample data are saved.   The options are string keywords that must be one of the following:\n",
    "1.  \"gridfs\" (the default) saves the sample data internally in the MongoDB data area.  Because in session 1 we did all our saves without specifying the storage option, all the processed data created there have their `Metadata` content stored in the collection (see list above) called \"wf_TimeSeries\" or \"wf_Seismogram\".   The sample data are stored in file-like objects managed by MongoDB and with documents needed to define them in the two collections called \"fs.files\" and \"fs.chunks\".  Gridfs is convenient storage because it is easier to manage as an integrated and bombproof storage area managed completely by MongoDB.   The dark side is we know from experience writing data to gridfs can cause an io bottleneck since the more voluminous sample data have to pass through the same io channel as the Metadata write operations (the actual calls to `insert` to \"wf_TimeSeries` or `wf_Seismogram`.\n",
    "2.  \"files\", as the name suggests, writes data to conventional computer files.   When using the option `storage_mode=\"file\"` in a call to `save_data` by definition the writer has to know what file it should open and use to save the requested data.  The best way to do that is to set an explicit value for `dir` and `dfile` in the line where you call `save_data`.  If the arguments are not defined, `save_data` attempts to extract values from each atomic datum's Metadata container using the same keywords. (i.e. attempts to retrieve two string values with the keys \"dir\" and \"dfile\".  If that fails, it falls to the last resort;  \"dir\" will be set as to the run directory and \"dfile\" will be defined by a unique, random string. In all cases the file name is then generated by using the stock python `join` method of the `os.path`.   That is, the file name is generated as `fname=os.path.join(dir,dfile)`.  `save_data` then attempts to open the file.  If successful it seeks to the end of the file, posts the byte offset as the \"foff\" attribute, and writes the sample data.   The default is a raw binary dump, but as described in the User Manual and docstring for [save_data](http://www.mspass.org/python_api/mspasspy.db.html#mspasspy.db.database.Database.save_data) the output can be written in any format supported by obspy's writer (subject to major issues of rigid namespace requirements for many formats.)  All should recognize that using files requires some thought beforehand about how the files should be named and organized.   The model used is heavily project dependent and outside the scope of this course.\n",
    "\n",
    "There is one more detail about writing data that is an important performance issue.  That is, when working with atomic data MsPASS will always open a file, write data, and then close the file.  All three operations take nontrivial amounts of time to complete.  The excessive open-close commands are intrinsic bottlenecks on even a desktop system.   The fastest write model is to possible with ensemble written in the default binary mode.   In that mode, a file is opened only once for each ensemble and the data are dumped sequentially to the same file using a the low-level C fwrite function.\n",
    "\n",
    "The small python code below illustrates the different modes of writing data and times their relative performance.   The output at the end demonstrates the difference in performance for the different approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df5b40a8-9d3b-425b-808d-020ef8c49a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ensembled processed= 20\n",
      "Total time spent reading= 85.33140468597412\n",
      "Total time writing with gridfs= 128.1169364452362\n",
      "Total time with atomic writes to files= 69.78627610206604\n",
      "Total time with ensemble writes= 65.72909188270569\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "# first we make sure to create a file directory we will call \"wf3c\" to separate it from where we have the miniseed data stored\n",
    "current_directory = os.getcwd()\n",
    "dir = os.path.join(current_directory, 'wf3c')\n",
    "# dir='/home/wf3c'   # note this assumes running in the docker container with /home bound to \n",
    "if not os.path.exists(dir):\n",
    "    os.mkdir(dir)\n",
    "reading_time=0.0\n",
    "atomic_gridfs_write_time=0.0\n",
    "atomic_file_write_time=0.0\n",
    "ensemble_write_time=0.0\n",
    "# drive the processing by source_id - builds on output from session 1\n",
    "srcids = db.wf_Seismogram.distinct('source_id')[:2] # we only use 2 events here to reduce output size\n",
    "Nens = len(srcids)   # right because we know there are waveforms for each source document\n",
    "for sid in srcids:\n",
    "    # we will learn more about this query structure shortly\n",
    "    query = {'source_id' : sid, 'data_tag' : 'serial_preprocessed'}\n",
    "    t0 = time.time()\n",
    "    cursor = db.wf_Seismogram.find(query)\n",
    "    ensemble=db.read_data(cursor,collection='wf_Seismogram')\n",
    "    t = time.time()\n",
    "    reading_time += (t-t0)\n",
    "    # time atomic writes with gridfs default (done by the writer)\n",
    "    t0 = time.time()\n",
    "    # note use of data_tag to allow us to later ignore these data\n",
    "    db.save_data(ensemble,collection='wf_Seismogram',data_tag='gridfs_write_test')\n",
    "    t = time.time()\n",
    "    atomic_gridfs_write_time += (t-t0)\n",
    "    # now write to files as atomic writes - loop over members\n",
    "    t0 = time.time()\n",
    "    dfile = str(sid) + \".dat\"\n",
    "    for d in ensemble.member:\n",
    "        db.save_data(d,collection='wf_Seismogram',storage_mode='file',dir=dir,dfile=dfile,data_tag='atomic_file_write_test')\n",
    "    t = time.time()\n",
    "    atomic_file_write_time += (t-t0)\n",
    "    # finally write the files with ensemble writer - use the same file names but they get appended\n",
    "    t0 = time.time()\n",
    "    dfile = str(sid) + \".dat\"\n",
    "    db.save_data(ensemble,collection='wf_Seismogram',storage_mode='file',dir=dir,dfile=dfile,data_tag='ensemble_file_write_test')\n",
    "    t = time.time()\n",
    "    ensemble_write_time += (t-t0)\n",
    "print(\"Number of ensembled processed=\",Nens)\n",
    "print(\"Total time spent reading=\",reading_time)\n",
    "print(\"Total time writing with gridfs=\",atomic_gridfs_write_time)\n",
    "print(\"Total time with atomic writes to files=\",atomic_file_write_time)\n",
    "print(\"Total time with ensemble writes=\",ensemble_write_time) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e85b9ed-e3ef-4e52-bfb4-aa25e3a2c774",
   "metadata": {},
   "source": [
    "You will get different results on different hardware and operating systems.   You should definitely see that gridfs is always signficantly slower.   Whether or not the atomic write is faster or slower than the ensemble write is more variable.   The reasons are deep in the weeds of the implementation and are not important for this class.   The big lesson is to use files for performance, but do be careful about how you organize the files.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799dec86-5ab3-4f5f-9963-99a845ec0276",
   "metadata": {},
   "source": [
    "## In class session 1\n",
    "Answer questions with this tag in Homework2.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eecefc-0f72-490c-818a-52ef44fd51ba",
   "metadata": {},
   "source": [
    "### Read\n",
    "The R of CRUD is \"Read\" and is more-or-less the inverse of \"create\".   The keyword used for pulling \"documents\" from a MongoDB database, however, is `find`.  There are two basic methods in the core MongoDB API for fetching documents:  `find_one` and `find`.  They behave completely differently.\n",
    "\n",
    "#### find_one\n",
    "\n",
    "Let's begin with a simple application of `find_one`.  As the name implies it always returns one and only one document.  Here is a default application to the \"site\" collection that was created under the hood when we ran `save_inventory` above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c9a86d7-21ac-4478-97ac-909d8ec349fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of a document =  <class 'dict'>\n",
      "This is the content of that document\n",
      "{'_id': ObjectId('666d5452f63ce193f69019c4'), 'loc': '', 'net': 'TA', 'sta': '034A', 'lat': 27.064699, 'lon': -98.683296, 'coords': [-98.683296, 27.064699], 'location': {'type': 'Point', 'coordinates': [-98.683296, 27.064699]}, 'elev': 0.155, 'edepth': 0.0, 'starttime': 1262908800.0, 'endtime': 1321574399.0, 'site_id': ObjectId('666d5452f63ce193f69019c4')}\n"
     ]
    }
   ],
   "source": [
    "doc = db.site.find_one()\n",
    "print(\"The type of a document = \",type(doc))\n",
    "print(\"This is the content of that document\")\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca751521-c2a9-4f1b-b7b1-0d95782ac56c",
   "metadata": {},
   "source": [
    "As the output demonstrates a `find_one` returns data in a python dictionary.   You might also note the raw `print(doc)` output is a bit challenging to read.   For the rest of this tutorial we will use a simple little function defined below called `pretty_print` to make the output a bit easier to read.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc855df9-552a-4ee4-a4d1-092ad9e47462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019c4\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"034A\",\n",
      "  \"lat\": 27.064699,\n",
      "  \"lon\": -98.683296,\n",
      "  \"coords\": [\n",
      "    -98.683296,\n",
      "    27.064699\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -98.683296,\n",
      "      27.064699\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 0.155,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1262908800.0,\n",
      "  \"endtime\": 1321574399.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019c4\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from bson import json_util\n",
    "def pretty_print(doc,indent=2):\n",
    "    print(json_util.dumps(doc,indent=indent))\n",
    "doc=db['site'].find_one()\n",
    "pretty_print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c88284-19d6-4a94-8db2-121941c65074",
   "metadata": {},
   "source": [
    "Things of note in that box are:\n",
    "1.  The `pretty_print` function definition is a bit trivial, which is why it isn't a standard MsPASS function.   It uses the `json_util.dumps` function to create the curly bracket formatted print that is a lot easier to understand than the raw dump of the python dictionary.   It shows more clearly that a document is always made of up of one or more key-value pairs.\n",
    "2.  This example intentionally uses a variant of the syntax for interacting with the database handle.   Note in the first box I used `db.site` while in the second I used `db['site']`.   A powerful but confusing, in my opinion, feature of python is its capability to create that type of syntactic alternative incantation.   Technically, what it does is specify a \"collection\", which in this case is named \"site\".  In the jargon of MongoDB the `find` and `find_one` methods, which are the core MongoDB \"read\" methods, are \"collection operation\".   You should realize that `db` is the top-level symbol that refers to the \"whole\" database that is assumed to contain one more \"collection\"s.  The two incantations used above are alternative ways to get a handle to a specific \"collection\".  You will see both in books and online sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52b0c09-a5ea-4a26-ab87-04bf4da7c2b6",
   "metadata": {},
   "source": [
    "### find\n",
    "We are now ready to dig more deeply into the standard MongoDB query function called `find`.  We've used it many times already above and in session 1.  We now dig deeper into `find` as it is the standard query interface for MongoDB.  Like `find_one` it is a \"collection\" method so it normally occurs in a construct like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62544a35-1bec-414b-a3c1-33373dc511a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of the object returned by find is  <class 'pymongo.cursor.Cursor'>\n"
     ]
    }
   ],
   "source": [
    "cursor = db.wf_miniseed.find({})   # as we will see {} means all \n",
    "print(\"The type of the object returned by find is \",type(cursor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e525f-5c77-480a-8a7d-134872781b0e",
   "metadata": {},
   "source": [
    "Note that is exactly the same type that was returned earlier when we ran the command `cursor=db.list_collection()`.   We described the generic concept of a command cursor their.  In this case, the cursor we just created should be though for as way to sequentially return the documents the query would return.  Our example, as the comment states, is MongoDB's way of asking for \"all\".   We will see why in a moment went we get deeper into MongoDB's query language.  \n",
    "\n",
    "First, let's verify find_one does what we just described.   We don't want the previous example as it would blast over 26,000 documents to us.  Here is a trick to limit that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d014ad89-0f0b-4afe-9100-9d89344fb3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d53c640282871ba98e64a\"\n",
      "  },\n",
      "  \"sta\": \"034A\",\n",
      "  \"net\": \"TA\",\n",
      "  \"chan\": \"BHE\",\n",
      "  \"sampling_rate\": 40.0,\n",
      "  \"delta\": 0.025,\n",
      "  \"starttime\": 1299639019.000001,\n",
      "  \"last_packet_time\": 1299641348.450001,\n",
      "  \"foff\": 0,\n",
      "  \"nbytes\": 86016,\n",
      "  \"npts\": 96000,\n",
      "  \"endtime\": 1299641418.9750009,\n",
      "  \"storage_mode\": \"file\",\n",
      "  \"format\": \"mseed\",\n",
      "  \"dir\": \"/home/wf\",\n",
      "  \"dfile\": \"Event_16.msd\",\n",
      "  \"time_standard\": \"UTC\",\n",
      "  \"channel_endtime\": 1321549500.0,\n",
      "  \"channel_hang\": 89.1,\n",
      "  \"channel_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019c4\"\n",
      "  },\n",
      "  \"channel_starttime\": 1262908800.0,\n",
      "  \"channel_vang\": 90.0,\n",
      "  \"site_endtime\": 1321574399.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019c4\"\n",
      "  },\n",
      "  \"site_starttime\": 1262908800.0\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d53c740282871ba98e64b\"\n",
      "  },\n",
      "  \"sta\": \"034A\",\n",
      "  \"net\": \"TA\",\n",
      "  \"chan\": \"BHN\",\n",
      "  \"sampling_rate\": 40.0,\n",
      "  \"delta\": 0.025,\n",
      "  \"starttime\": 1299639019.000001,\n",
      "  \"last_packet_time\": 1299641341.1750011,\n",
      "  \"foff\": 86016,\n",
      "  \"nbytes\": 86016,\n",
      "  \"npts\": 96001,\n",
      "  \"endtime\": 1299641419.000001,\n",
      "  \"storage_mode\": \"file\",\n",
      "  \"format\": \"mseed\",\n",
      "  \"dir\": \"/home/wf\",\n",
      "  \"dfile\": \"Event_16.msd\",\n",
      "  \"time_standard\": \"UTC\",\n",
      "  \"channel_endtime\": 1321549500.0,\n",
      "  \"channel_hang\": 359.1,\n",
      "  \"channel_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019c5\"\n",
      "  },\n",
      "  \"channel_starttime\": 1262908800.0,\n",
      "  \"channel_vang\": 90.0,\n",
      "  \"site_endtime\": 1321574399.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019c4\"\n",
      "  },\n",
      "  \"site_starttime\": 1262908800.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "cursor.limit(2)\n",
    "for doc in cursor:\n",
    "    pretty_print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1b6b01-9933-457e-bba5-c2c450c97fdd",
   "metadata": {},
   "source": [
    "The point here is that we used the \"method\" of \"CommandCursor\" called \"limit\" to only retrieve the first 2 documents instead of all 26,000 + in wf_miniseed.  \n",
    "\n",
    "To emphasize the point that a `CommandCursor` is a \"forward iterator\" consider this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96100e3d-6528-4b6f-bc14-d4e1734da77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in cursor:\n",
    "    pretty_print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcd6fbf-0c5c-44ed-a9ea-29090f2cf06e",
   "metadata": {},
   "source": [
    "That produced no output because the cursor had already been traversed.  (Note in some contexts that kind of construct would also throw an python exception.)  The solution is an application of the \"rewind\" method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48b09eb5-a0b1-4e74-b20a-7670bbc3a17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d53c640282871ba98e64a\"\n",
      "  },\n",
      "  \"sta\": \"034A\",\n",
      "  \"net\": \"TA\",\n",
      "  \"chan\": \"BHE\",\n",
      "  \"sampling_rate\": 40.0,\n",
      "  \"delta\": 0.025,\n",
      "  \"starttime\": 1299639019.000001,\n",
      "  \"last_packet_time\": 1299641348.450001,\n",
      "  \"foff\": 0,\n",
      "  \"nbytes\": 86016,\n",
      "  \"npts\": 96000,\n",
      "  \"endtime\": 1299641418.9750009,\n",
      "  \"storage_mode\": \"file\",\n",
      "  \"format\": \"mseed\",\n",
      "  \"dir\": \"/home/wf\",\n",
      "  \"dfile\": \"Event_16.msd\",\n",
      "  \"time_standard\": \"UTC\",\n",
      "  \"channel_endtime\": 1321549500.0,\n",
      "  \"channel_hang\": 89.1,\n",
      "  \"channel_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019c4\"\n",
      "  },\n",
      "  \"channel_starttime\": 1262908800.0,\n",
      "  \"channel_vang\": 90.0,\n",
      "  \"site_endtime\": 1321574399.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019c4\"\n",
      "  },\n",
      "  \"site_starttime\": 1262908800.0\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d53c740282871ba98e64b\"\n",
      "  },\n",
      "  \"sta\": \"034A\",\n",
      "  \"net\": \"TA\",\n",
      "  \"chan\": \"BHN\",\n",
      "  \"sampling_rate\": 40.0,\n",
      "  \"delta\": 0.025,\n",
      "  \"starttime\": 1299639019.000001,\n",
      "  \"last_packet_time\": 1299641341.1750011,\n",
      "  \"foff\": 86016,\n",
      "  \"nbytes\": 86016,\n",
      "  \"npts\": 96001,\n",
      "  \"endtime\": 1299641419.000001,\n",
      "  \"storage_mode\": \"file\",\n",
      "  \"format\": \"mseed\",\n",
      "  \"dir\": \"/home/wf\",\n",
      "  \"dfile\": \"Event_16.msd\",\n",
      "  \"time_standard\": \"UTC\",\n",
      "  \"channel_endtime\": 1321549500.0,\n",
      "  \"channel_hang\": 359.1,\n",
      "  \"channel_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019c5\"\n",
      "  },\n",
      "  \"channel_starttime\": 1262908800.0,\n",
      "  \"channel_vang\": 90.0,\n",
      "  \"site_endtime\": 1321574399.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019c4\"\n",
      "  },\n",
      "  \"site_starttime\": 1262908800.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "cursor.rewind()\n",
    "cursor.limit(2)\n",
    "for doc in cursor:\n",
    "   pretty_print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd55e96-3d6f-442f-a5b5-a7139dbdd6fd",
   "metadata": {},
   "source": [
    "### Mongo Query Language (MQL)\n",
    "#### Single key match and basics\n",
    "I will run a set of examples of increasing levels of complexity.   This particular section of this tutorial is intended as a hands on supplement the lecture in this class and the material in the User Manual section titled [\"Using MongoDB with MsPASS\"](http://www.mspass.org/user_manual/mongodb_and_mspass.html).  A point made there worth repeating is that we have found no book or online source that describe the syntax rules of the MQL language.   The following quote from the User's Manual is thus worth emphasizing that is our take on the rules defining MQL:\n",
    "\n",
    "1.  All queries use a python dictionary to contain the instructions.\n",
    "2.  The key of a dictionary used for query normally refers to an attribute\n",
    "    in documents of the collection being queried.  There is an exception\n",
    "    for the logical OR and logical AND operators (discussed below).\n",
    "3.  The \"value\" of each key-value pair is normally itself a python\n",
    "    dictionary.   The contents of the dictionary define a simple\n",
    "    language (Mongo Query Language) that resolves True for a match\n",
    "    and False if there is no match.  The key point is the overall\n",
    "    expression the query dictionary has to resolve to a boolean condition.\n",
    "4.  The keys of the dict containers that are on the value side of\n",
    "    a query dict are normally operators.  Operators are defined with\n",
    "    strings that begin with the \"$\" symbol.\n",
    "5.  Simple queries are a single key-value pair with the value either\n",
    "    a constant or a dictionary with a single operator key.  e.g.\n",
    "    to a test for the \"sta\" attribute being the constant \"AAK\" the\n",
    "    query could be either `{\"sta\" : \"AAK\"}` or `{\"sta\" : {\"$eq\" : \"AAK\"}}`.\n",
    "    The form with constant value only works for \"$eq\".\n",
    "6.  Compound queries (e.g. time interval expressions) have a value\n",
    "    with multiple operator keys.\n",
    "7.  There is an implied logical AND operation\n",
    "    between multiple key operations.  An OR must be specified differently\n",
    "    (see below).\n",
    "\n",
    "With that knowledge of the MQL syntax rules, the rest of this section demonstrates those ideas with examples.  For this exercise we are going to focus on the \"site\" collection as it has fewer complexities and has relatively small documents compared to anything else we could work with here. \n",
    "\n",
    "First, a unique match query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81e37a54-a0d6-4012-a015-96548eccc7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of site documents for station 134A= 1\n",
      "Number of channel documents for station 134A= 3\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019f4\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"134A\",\n",
      "  \"lat\": 32.572899,\n",
      "  \"lon\": -98.079498,\n",
      "  \"coords\": [\n",
      "    -98.079498,\n",
      "    32.572899\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -98.079498,\n",
      "      32.572899\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 0.297,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1258329600.0,\n",
      "  \"endtime\": 1315526399.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019f4\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "query={'sta' : '134A'}\n",
    "nsite=db.site.count_documents(query)\n",
    "print(\"Number of site documents for station 134A=\",nsite)\n",
    "nchannel=db.channel.count_documents(query)\n",
    "print(\"Number of channel documents for station 134A=\",nchannel)\n",
    "cursor=db.site.find(query)\n",
    "for doc in cursor:\n",
    "    pretty_print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c6507-78b9-4e23-885f-54aa885f9594",
   "metadata": {},
   "source": [
    "Notice:\n",
    "1.  I used another important collection method called `count_documents` to fetch the expected number of documents the query would yield.  Standard practice in working through many queries is to do a check that the number it returns makes sense.\n",
    "2.  We see there is one and only one station matching query is site and three matching in channel.  The reason channel has three, of course, is that there is a three-component sensor at that station that defines the recording channels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed99677f-6153-4770-a256-c816f3c247e6",
   "metadata": {},
   "source": [
    "### Projections\n",
    "There is a lot of extra stuff in the document we retrieved.  We often want a simple \"report\" that only displays a subset of the content we are interested.  SQL users will recognize this functionality as a SELECT clause in the SQL queries.   The same idea in MongoDB is called, for reasons known only to the developers of MongoDB, a \"projection\".   \n",
    "Here is an example where we extract and print only net, sta, chan, loc, hang, and vang from each of the 3 channel documents our query returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fbe1fa6-ecb1-4943-aff4-9ae505f01ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loc': '', 'net': 'TA', 'sta': '134A', 'chan': 'BHE', 'vang': 90.0, 'hang': 90.7}\n",
      "{'loc': '', 'net': 'TA', 'sta': '134A', 'chan': 'BHN', 'vang': 90.0, 'hang': 0.7}\n",
      "{'loc': '', 'net': 'TA', 'sta': '134A', 'chan': 'BHZ', 'vang': 0.0, 'hang': 0.0}\n"
     ]
    }
   ],
   "source": [
    "projection={'net':1,'sta':1,'chan':1,'loc':1,'vang':1,'hang':1,'_id':0}\n",
    "cursor=db.channel.find(query,projection)\n",
    "for doc in cursor:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c899b792-7589-44d6-8283-3504a58d686d",
   "metadata": {},
   "source": [
    "Noting the \"projection\" symbol is a python dictionary==MongoDB document.   The oddity is that a 0 value means False and 1 means True.   That definition says we want to retrieve everything listed with 1 and drop everything else.  The oddity of setting \"_id\" to 0 is necessary because by default the id is always retrieved in a find/find_one operation.  That incantation says we don't want to see it here. \n",
    "\n",
    "Here is a fancier variant using pandas to print the same attributes in tabular form dropping \"loc\" since we see it is always empty in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bdc6a8b-98cc-41a9-bccf-e4bb59aa258c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  net   sta        lat        lon   elev chan  vang  hang\n",
      "0  TA  134A  32.572899 -98.079498  0.297  BHE  90.0  90.7\n",
      "1  TA  134A  32.572899 -98.079498  0.297  BHN  90.0   0.7\n",
      "2  TA  134A  32.572899 -98.079498  0.297  BHZ   0.0   0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "projection={\n",
    "    'net':1,\n",
    "    'sta':1,\n",
    "    'chan':1,\n",
    "    'lat':1,\n",
    "    'lon':1,\n",
    "    'elev':1,\n",
    "    'hang':1,\n",
    "    'vang':1,\n",
    "    '_id':0,\n",
    "}\n",
    "cursor=db.channel.find(query,projection)\n",
    "doclist=[]\n",
    "for doc in cursor:\n",
    "    doclist.append(doc)\n",
    "df = pd.DataFrame.from_dict(doclist)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e07f977-41da-471e-ab51-e1e143b66dbb",
   "metadata": {},
   "source": [
    "The pandas construct is useful for a number of reasons.  Therefore, let's create a function to simplify that type of printing operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48087e04-4364-4812-94d3-96a0a5c998b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def print_as_table(doclist):\n",
    "    df = pd.DataFrame.from_dict(doclist)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcea1cb2-6563-4a08-946d-bf4cca87480e",
   "metadata": {},
   "source": [
    "#### Multiple key equality matching\n",
    "Next let's do a query with multiple keys.   We will fetch the (shortened) record for the BHN component of a different station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "828b579d-6c52-4d55-a176-85f648f38bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"131A\",\n",
      "  \"lat\": 32.673698,\n",
      "  \"lon\": -100.388802,\n",
      "  \"elev\": 0.622,\n",
      "  \"chan\": \"BHZ\",\n",
      "  \"vang\": 0.0,\n",
      "  \"hang\": 0.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "query={\n",
    "    'sta' : '131A',\n",
    "    'chan' : 'BHZ',\n",
    "}\n",
    "cursor=db.channel.find(query,projection)\n",
    "for doc in cursor:\n",
    "    pretty_print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f854e2a1-6c7d-43fa-a9cb-f8fd69c38e78",
   "metadata": {},
   "source": [
    "#### Range operator examples (compound query)\n",
    "We often want to query by a range of values.  Here is an example that returns the coordinates of all TA stations within a 5 degree box defined by 30 to 35 latitude and -110 to -100 longitude: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3294b296-df65-405b-aaa0-c308a0fa81a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   net   sta        lat         lon   elev\n",
      "0   TA  121A  32.532398 -107.785103  1.652\n",
      "1   TA  123A  32.634899 -106.262199  1.206\n",
      "2   TA  124A  32.700100 -105.454399  2.078\n",
      "3   TA  125A  32.658798 -104.657303  1.212\n",
      "4   TA  126A  32.646198 -104.020401  1.032\n",
      "..  ..   ...        ...         ...    ...\n",
      "71  TA  Z27A  33.314999 -103.214500  1.197\n",
      "72  TA  Z28A  33.288399 -102.386597  1.045\n",
      "73  TA  Z29A  33.259499 -101.706200  0.938\n",
      "74  TA  Z30A  33.286098 -101.128197  0.729\n",
      "75  TA  Z31A  33.318298 -100.143501  0.547\n",
      "\n",
      "[76 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "query={\n",
    "    'lat' : {'$gte' : 30.0,'$lte' : 35.0},\n",
    "    'lon' : {'$gte' : -110.0, '$lte' : -100},\n",
    "}\n",
    "projection={\n",
    "   'net':1,\n",
    "    'sta':1,\n",
    "    'chan':1,\n",
    "    'lat':1,\n",
    "    'lon':1,\n",
    "    'elev':1,\n",
    "    '_id':0, \n",
    "}\n",
    "cursor=db.site.find(query,projection)\n",
    "doclist=[]\n",
    "for doc in cursor:\n",
    "    doclist.append(doc)\n",
    "print_as_table(doclist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bd5907-bf51-4275-ae47-233680b12d63",
   "metadata": {},
   "source": [
    "A variant using a regular expression to only select station names that start with the latter \"Y\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eace96aa-0b9f-48e0-9ef1-f4fcb20db801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   net   sta        lat         lon   elev\n",
      "0   TA  Y22D  34.073900 -106.921000  1.436\n",
      "1   TA  Y22E  34.074200 -106.920799  1.444\n",
      "2   TA  Y22E  34.074200 -106.920799  1.444\n",
      "3   TA  Y23A  33.931499 -106.054901  1.789\n",
      "4   TA  Y24A  33.925701 -105.436096  1.827\n",
      "5   TA  Y25A  33.922901 -104.692802  1.364\n",
      "6   TA  Y26A  33.923199 -103.824600  1.371\n",
      "7   TA  Y27A  33.883900 -103.163300  1.253\n",
      "8   TA  Y28A  33.908600 -102.247902  1.068\n",
      "9   TA  Y29A  33.860199 -101.671204  0.991\n",
      "10  TA  Y30A  33.876598 -100.897797  0.812\n",
      "11  TA  Y31A  33.962898 -100.261497  0.530\n"
     ]
    }
   ],
   "source": [
    "query={\n",
    "    'lat' : {'$gte' : 30.0,'$lte' : 35.0},\n",
    "    'lon' : {'$gte' : -110.0, '$lte' : -100},\n",
    "    'sta' : {'$regex' : 'Y.*'},\n",
    "}\n",
    "cursor=db.site.find(query,projection)\n",
    "doclist=[]\n",
    "for doc in cursor:\n",
    "    doclist.append(doc)\n",
    "print_as_table(doclist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d195c91-52e1-443b-b824-68db10a125f5",
   "metadata": {},
   "source": [
    "#### Geospatial query\n",
    "MongoDB has some very useful geospatial query capabilities.  See the [\"MongoDB and MsPASS\"](http://www.mspass.org/user_manual/mongodb_and_mspass.html) section of the User's Manual for more about this capability.  On the other hand, it is probably best thought of, at least at present, as an advanced feature.   The syntax is complex and, as noted in that section of the manual, MongoDB documentation is less than ideal and many online sources are inconsistent with the current implementation.  For this tutorial I will just show an example that is a variant of that shown in User's Manual page.\n",
    "\n",
    "An IMPORTANT rule about using geospatial searches is that a special index is REQUIRED.  For this example the following is needed to make this work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e7dc365-e621-4598-9921-c5f9925a30dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'location_2dsphere'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.site.create_index({'location' : '2dsphere'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d6bf6b-a8f6-4730-8728-0ac6e8c880f9",
   "metadata": {},
   "source": [
    "Noting:\n",
    "1.  'location' is the key used to tag the geoJSON format documents `save_inventory` created in the site collection.  It is a constant tag in the MsPASS schema for these data.  Note also that if you were running this on the source collection the key has a different name ('epicenter') since the content exactly matches the definition of the jargon term. \n",
    "2. '2dsphere' is a magic string that tells MongoDB to create a special index that uses spherical geometry for spatial calculations.  The alternative is '2d' but the alternative is not advised for most if not all seismology applications.  The '2d' index uses a map projection that produces distorted answers unless the area of study is small. Examples you can find online use a '2d' index for applications like apps that are have data only on a single city.\n",
    "3. An advanced topic, which is a side issue for this discussion of geospatial queries, is that any key that used frequently in a find operations on large collections should have an index created.  All indexs produce some form of hash table that allows the MongoDB server to find documents without doing a linear search through the entire collection.   We have found multiple order of magnitude differences in performance with million scale collections.  \n",
    "\n",
    "Now that we have an index, we can do a search.  This search produces a similar result to the lat-lon range query above but for a circular (great circle path distance circle that is) region at the center of the same lat-lon box as above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74a6709e-71ae-458f-8f00-022d8c7366f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019dc\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"125A\",\n",
      "  \"lat\": 32.658798,\n",
      "  \"lon\": -104.657303,\n",
      "  \"coords\": [\n",
      "    -104.657303,\n",
      "    32.658798\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -104.657303,\n",
      "      32.658798\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.212,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1205366400.0,\n",
      "  \"endtime\": 1266537599.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019dc\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f6901a36\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"225A\",\n",
      "  \"lat\": 32.1101,\n",
      "  \"lon\": -104.822899,\n",
      "  \"coords\": [\n",
      "    -104.822899,\n",
      "    32.1101\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -104.822899,\n",
      "      32.1101\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.703,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1206489600.0,\n",
      "  \"endtime\": 1266623999.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f6901a36\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019d9\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"124A\",\n",
      "  \"lat\": 32.7001,\n",
      "  \"lon\": -105.454399,\n",
      "  \"coords\": [\n",
      "    -105.454399,\n",
      "    32.7001\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -105.454399,\n",
      "      32.7001\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 2.078,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1204070400.0,\n",
      "  \"endtime\": 1263859199.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019d9\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f6901a33\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"224A\",\n",
      "  \"lat\": 32.076,\n",
      "  \"lon\": -105.522598,\n",
      "  \"coords\": [\n",
      "    -105.522598,\n",
      "    32.076\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -105.522598,\n",
      "      32.076\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.487,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1204156800.0,\n",
      "  \"endtime\": 1263686399.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f6901a33\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5457f63ce193f6902450\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"Z25A\",\n",
      "  \"lat\": 33.279701,\n",
      "  \"lon\": -104.717102,\n",
      "  \"coords\": [\n",
      "    -104.717102,\n",
      "    33.279701\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -104.717102,\n",
      "      33.279701\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.233,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1208304000.0,\n",
      "  \"endtime\": 1268092799.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5457f63ce193f6902450\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f6901a39\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"226B\",\n",
      "  \"lat\": 32.077801,\n",
      "  \"lon\": -104.165398,\n",
      "  \"coords\": [\n",
      "    -104.165398,\n",
      "    32.077801\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -104.165398,\n",
      "      32.077801\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 0.981,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1235174400.0,\n",
      "  \"endtime\": 1266451199.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f6901a39\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019df\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"126A\",\n",
      "  \"lat\": 32.646198,\n",
      "  \"lon\": -104.020401,\n",
      "  \"coords\": [\n",
      "    -104.020401,\n",
      "    32.646198\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -104.020401,\n",
      "      32.646198\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.032,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1205539200.0,\n",
      "  \"endtime\": 1266364799.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019df\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5457f63ce193f690244d\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"Z24A\",\n",
      "  \"lat\": 33.3298,\n",
      "  \"lon\": -105.364899,\n",
      "  \"coords\": [\n",
      "    -105.364899,\n",
      "    33.3298\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -105.364899,\n",
      "      33.3298\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.863,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1210809600.0,\n",
      "  \"endtime\": 1267919999.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5457f63ce193f690244d\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019d6\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"123A\",\n",
      "  \"lat\": 32.634899,\n",
      "  \"lon\": -106.262199,\n",
      "  \"coords\": [\n",
      "    -106.262199,\n",
      "    32.634899\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -106.262199,\n",
      "      32.634899\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.206,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1210723200.0,\n",
      "  \"endtime\": 1263945599.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019d6\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f6901a8d\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"325A\",\n",
      "  \"lat\": 31.371099,\n",
      "  \"lon\": -104.971199,\n",
      "  \"coords\": [\n",
      "    -104.971199,\n",
      "    31.371099\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -104.971199,\n",
      "      31.371099\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.666,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1203897600.0,\n",
      "  \"endtime\": 1263686399.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f6901a8d\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f6901a8a\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"324A\",\n",
      "  \"lat\": 31.442499,\n",
      "  \"lon\": -105.482803,\n",
      "  \"coords\": [\n",
      "    -105.482803,\n",
      "    31.442499\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -105.482803,\n",
      "      31.442499\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.441,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1203724800.0,\n",
      "  \"endtime\": 1263513599.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f6901a8a\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5457f63ce193f6902453\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"Z26A\",\n",
      "  \"lat\": 33.271599,\n",
      "  \"lon\": -103.979797,\n",
      "  \"coords\": [\n",
      "    -103.979797,\n",
      "    33.271599\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -103.979797,\n",
      "      33.271599\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.163,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1208390400.0,\n",
      "  \"endtime\": 1267833599.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5457f63ce193f6902453\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5457f63ce193f690244a\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"Z23A\",\n",
      "  \"lat\": 33.2621,\n",
      "  \"lon\": -106.231903,\n",
      "  \"coords\": [\n",
      "    -106.231903,\n",
      "    33.2621\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -106.231903,\n",
      "      33.2621\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.278,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1210723200.0,\n",
      "  \"endtime\": 1263945599.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5457f63ce193f690244a\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f6901a30\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"223A\",\n",
      "  \"lat\": 32.006199,\n",
      "  \"lon\": -106.427597,\n",
      "  \"coords\": [\n",
      "    -106.427597,\n",
      "    32.006199\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -106.427597,\n",
      "      32.006199\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.232,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1206662400.0,\n",
      "  \"endtime\": 1263686399.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f6901a30\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019e2\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"127A\",\n",
      "  \"lat\": 32.676399,\n",
      "  \"lon\": -103.357498,\n",
      "  \"coords\": [\n",
      "    -103.357498,\n",
      "    32.676399\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -103.357498,\n",
      "      32.676399\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.16,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1205625600.0,\n",
      "  \"endtime\": 1267660799.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019e2\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5457f63ce193f69023f3\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"Y25A\",\n",
      "  \"lat\": 33.922901,\n",
      "  \"lon\": -104.692802,\n",
      "  \"coords\": [\n",
      "    -104.692802,\n",
      "    33.922901\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -104.692802,\n",
      "      33.922901\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.364,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1208304000.0,\n",
      "  \"endtime\": 1268006399.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5457f63ce193f69023f3\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f6901a90\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"326A\",\n",
      "  \"lat\": 31.3165,\n",
      "  \"lon\": -103.9786,\n",
      "  \"coords\": [\n",
      "    -103.9786,\n",
      "    31.3165\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -103.9786,\n",
      "      31.3165\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 0.982,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1205884800.0,\n",
      "  \"endtime\": 1266191999.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f6901a90\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5457f63ce193f69023f0\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"Y24A\",\n",
      "  \"lat\": 33.925701,\n",
      "  \"lon\": -105.436096,\n",
      "  \"coords\": [\n",
      "    -105.436096,\n",
      "    33.925701\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -105.436096,\n",
      "      33.925701\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.827,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1208995200.0,\n",
      "  \"endtime\": 1269215999.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5457f63ce193f69023f0\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f6901a3c\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"227A\",\n",
      "  \"lat\": 32.012001,\n",
      "  \"lon\": -103.292397,\n",
      "  \"coords\": [\n",
      "    -103.292397,\n",
      "    32.012001\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -103.292397,\n",
      "      32.012001\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 0.879,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1206057600.0,\n",
      "  \"endtime\": 1266278399.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f6901a3c\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5457f63ce193f69023ed\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"Y23A\",\n",
      "  \"lat\": 33.931499,\n",
      "  \"lon\": -106.054901,\n",
      "  \"coords\": [\n",
      "    -106.054901,\n",
      "    33.931499\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -106.054901,\n",
      "      33.931499\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.789,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1207958400.0,\n",
      "  \"endtime\": 1268438399.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5457f63ce193f69023ed\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5457f63ce193f6902456\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"Z27A\",\n",
      "  \"lat\": 33.314999,\n",
      "  \"lon\": -103.2145,\n",
      "  \"coords\": [\n",
      "    -103.2145,\n",
      "    33.314999\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -103.2145,\n",
      "      33.314999\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.197,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1208390400.0,\n",
      "  \"endtime\": 1267747199.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5457f63ce193f6902456\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f6901a93\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"327A\",\n",
      "  \"lat\": 31.369101,\n",
      "  \"lon\": -103.492302,\n",
      "  \"coords\": [\n",
      "    -103.492302,\n",
      "    31.369101\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -103.492302,\n",
      "      31.369101\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 0.784,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1205971200.0,\n",
      "  \"endtime\": 1266105599.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f6901a93\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5453f63ce193f6901ae7\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"425A\",\n",
      "  \"lat\": 30.7862,\n",
      "  \"lon\": -104.985703,\n",
      "  \"coords\": [\n",
      "    -104.985703,\n",
      "    30.7862\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -104.985703,\n",
      "      30.7862\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.337,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1205539200.0,\n",
      "  \"endtime\": 1265500799.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5453f63ce193f6901ae7\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5457f63ce193f69023f6\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"Y26A\",\n",
      "  \"lat\": 33.923199,\n",
      "  \"lon\": -103.8246,\n",
      "  \"coords\": [\n",
      "    -103.8246,\n",
      "    33.923199\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -103.8246,\n",
      "      33.923199\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.371,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1208476800.0,\n",
      "  \"endtime\": 1268265599.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5457f63ce193f69023f6\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5457f63ce193f6902447\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"Z22A\",\n",
      "  \"lat\": 33.255501,\n",
      "  \"lon\": -106.963898,\n",
      "  \"coords\": [\n",
      "    -106.963898,\n",
      "    33.255501\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -106.963898,\n",
      "      33.255501\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.497,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1207785600.0,\n",
      "  \"endtime\": 1269129599.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5457f63ce193f6902447\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f6901a2d\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"222A\",\n",
      "  \"lat\": 32.104599,\n",
      "  \"lon\": -107.101303,\n",
      "  \"coords\": [\n",
      "    -107.101303,\n",
      "    32.104599\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -107.101303,\n",
      "      32.104599\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.324,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1202947200.0,\n",
      "  \"endtime\": 1263772799.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f6901a2d\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5453f63ce193f6901aea\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"426A\",\n",
      "  \"lat\": 30.6689,\n",
      "  \"lon\": -104.029297,\n",
      "  \"coords\": [\n",
      "    -104.029297,\n",
      "    30.6689\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -104.029297,\n",
      "      30.6689\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.943,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1205280000.0,\n",
      "  \"endtime\": 1265759999.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5453f63ce193f6901aea\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5456f63ce193f6902378\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"X25A\",\n",
      "  \"lat\": 34.5271,\n",
      "  \"lon\": -104.662102,\n",
      "  \"coords\": [\n",
      "    -104.662102,\n",
      "    34.5271\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -104.662102,\n",
      "      34.5271\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.494,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1209254400.0,\n",
      "  \"endtime\": 1269129599.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5456f63ce193f6902378\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5457f63ce193f69023f9\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"Y27A\",\n",
      "  \"lat\": 33.8839,\n",
      "  \"lon\": -103.1633,\n",
      "  \"coords\": [\n",
      "    -103.1633,\n",
      "    33.8839\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -103.1633,\n",
      "      33.8839\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.253,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1208476800.0,\n",
      "  \"endtime\": 1268956799.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5457f63ce193f69023f9\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f6901a3f\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"228A\",\n",
      "  \"lat\": 32.118,\n",
      "  \"lon\": -102.591797,\n",
      "  \"coords\": [\n",
      "    -102.591797,\n",
      "    32.118\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -102.591797,\n",
      "      32.118\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 0.954,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1234224000.0,\n",
      "  \"endtime\": 1291852799.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f6901a3f\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5456f63ce193f6902375\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"X24A\",\n",
      "  \"lat\": 34.564602,\n",
      "  \"lon\": -105.434898,\n",
      "  \"coords\": [\n",
      "    -105.434898,\n",
      "    34.564602\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -105.434898,\n",
      "      34.564602\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.917,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1209081600.0,\n",
      "  \"endtime\": 1268524799.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5456f63ce193f6902375\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019e5\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"128A\",\n",
      "  \"lat\": 32.6213,\n",
      "  \"lon\": -102.485001,\n",
      "  \"coords\": [\n",
      "    -102.485001,\n",
      "    32.6213\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -102.485001,\n",
      "      32.6213\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 0.966,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1233878400.0,\n",
      "  \"endtime\": 1291679999.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019e5\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5453f63ce193f6901aed\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"427A\",\n",
      "  \"lat\": 30.8498,\n",
      "  \"lon\": -103.401802,\n",
      "  \"coords\": [\n",
      "    -103.401802,\n",
      "    30.8498\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -103.401802,\n",
      "      30.8498\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.031,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1205625600.0,\n",
      "  \"endtime\": 1266019199.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5453f63ce193f6901aed\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f6901a96\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"328A\",\n",
      "  \"lat\": 31.3818,\n",
      "  \"lon\": -102.8097,\n",
      "  \"coords\": [\n",
      "    -102.8097,\n",
      "    31.3818\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -102.8097,\n",
      "      31.3818\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 0.755,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1208822400.0,\n",
      "  \"endtime\": 1266105599.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f6901a96\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5456f63ce193f69023d5\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"Y22D\",\n",
      "  \"lat\": 34.0739,\n",
      "  \"lon\": -106.921,\n",
      "  \"coords\": [\n",
      "    -106.921,\n",
      "    34.0739\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -106.921,\n",
      "      34.0739\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.436,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1191024000.0,\n",
      "  \"endtime\": 1575158399.9998999,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5456f63ce193f69023d5\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5456f63ce193f69023db\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"Y22E\",\n",
      "  \"lat\": 34.0742,\n",
      "  \"lon\": -106.920799,\n",
      "  \"coords\": [\n",
      "    -106.920799,\n",
      "    34.0742\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -106.920799,\n",
      "      34.0742\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.444,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1301270400.0,\n",
      "  \"endtime\": 1344297599.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5456f63ce193f69023db\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5456f63ce193f69023e4\"\n",
      "  },\n",
      "  \"loc\": \"01\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"Y22E\",\n",
      "  \"lat\": 34.0742,\n",
      "  \"lon\": -106.920799,\n",
      "  \"coords\": [\n",
      "    -106.920799,\n",
      "    34.0742\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -106.920799,\n",
      "      34.0742\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.444,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1301270400.0,\n",
      "  \"endtime\": 1344297599.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5456f63ce193f69023e4\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5456f63ce193f690237b\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"X26A\",\n",
      "  \"lat\": 34.5508,\n",
      "  \"lon\": -103.810303,\n",
      "  \"coords\": [\n",
      "    -103.810303,\n",
      "    34.5508\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -103.810303,\n",
      "      34.5508\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.393,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1209427200.0,\n",
      "  \"endtime\": 1269043199.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5456f63ce193f690237b\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5456f63ce193f6902372\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"X23A\",\n",
      "  \"lat\": 34.581001,\n",
      "  \"lon\": -106.188103,\n",
      "  \"coords\": [\n",
      "    -106.188103,\n",
      "    34.581001\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -106.188103,\n",
      "      34.581001\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.948,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1208563200.0,\n",
      "  \"endtime\": 1268524799.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5456f63ce193f6902372\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5457f63ce193f6902459\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"Z28A\",\n",
      "  \"lat\": 33.288399,\n",
      "  \"lon\": -102.386597,\n",
      "  \"coords\": [\n",
      "    -102.386597,\n",
      "    33.288399\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -102.386597,\n",
      "      33.288399\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.045,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1233014400.0,\n",
      "  \"endtime\": 1291507199.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5457f63ce193f6902459\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019d3\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"121A\",\n",
      "  \"lat\": 32.532398,\n",
      "  \"lon\": -107.785103,\n",
      "  \"coords\": [\n",
      "    -107.785103,\n",
      "    32.532398\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -107.785103,\n",
      "      32.532398\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.652,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1202774400.0,\n",
      "  \"endtime\": 1552089599.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019d3\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5454f63ce193f6901ff4\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"MSTX\",\n",
      "  \"lat\": 33.969601,\n",
      "  \"lon\": -102.7724,\n",
      "  \"coords\": [\n",
      "    -102.7724,\n",
      "    33.969601\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -102.7724,\n",
      "      33.969601\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.167,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1208908800.0,\n",
      "  \"endtime\": 1534377599.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5454f63ce193f6901ff4\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f6901a2a\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"221A\",\n",
      "  \"lat\": 32.009399,\n",
      "  \"lon\": -107.778198,\n",
      "  \"coords\": [\n",
      "    -107.778198,\n",
      "    32.009399\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -107.778198,\n",
      "      32.009399\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.277,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1202688000.0,\n",
      "  \"endtime\": 1263859199.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f6901a2a\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5453f63ce193f6901b3e\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"526A\",\n",
      "  \"lat\": 30.0609,\n",
      "  \"lon\": -104.089798,\n",
      "  \"coords\": [\n",
      "    -104.089798,\n",
      "    30.0609\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -104.089798,\n",
      "      30.0609\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.405,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1204675200.0,\n",
      "  \"endtime\": 1265759999.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5453f63ce193f6901b3e\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5453f63ce193f6901b41\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"527A\",\n",
      "  \"lat\": 30.145599,\n",
      "  \"lon\": -103.6119,\n",
      "  \"coords\": [\n",
      "    -103.6119,\n",
      "    30.145599\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -103.6119,\n",
      "      30.145599\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.419,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1204761600.0,\n",
      "  \"endtime\": 1265673599.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5453f63ce193f6901b41\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019e8\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"129A\",\n",
      "  \"lat\": 32.630901,\n",
      "  \"lon\": -101.866203,\n",
      "  \"coords\": [\n",
      "    -101.866203,\n",
      "    32.630901\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -101.866203,\n",
      "      32.630901\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 0.876,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1233360000.0,\n",
      "  \"endtime\": 1291766399.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5452f63ce193f69019e8\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5453f63ce193f6901af0\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"428A\",\n",
      "  \"lat\": 30.726299,\n",
      "  \"lon\": -102.6847,\n",
      "  \"coords\": [\n",
      "    -102.6847,\n",
      "    30.726299\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -102.6847,\n",
      "      30.726299\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 0.982,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1205107200.0,\n",
      "  \"endtime\": 1266105599.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5453f63ce193f6901af0\"\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"_id\": {\n",
      "    \"$oid\": \"666d5456f63ce193f690237e\"\n",
      "  },\n",
      "  \"loc\": \"\",\n",
      "  \"net\": \"TA\",\n",
      "  \"sta\": \"X27A\",\n",
      "  \"lat\": 34.6469,\n",
      "  \"lon\": -103.097397,\n",
      "  \"coords\": [\n",
      "    -103.097397,\n",
      "    34.6469\n",
      "  ],\n",
      "  \"location\": {\n",
      "    \"type\": \"Point\",\n",
      "    \"coordinates\": [\n",
      "      -103.097397,\n",
      "      34.6469\n",
      "    ]\n",
      "  },\n",
      "  \"elev\": 1.304,\n",
      "  \"edepth\": 0.0,\n",
      "  \"starttime\": 1209340800.0,\n",
      "  \"endtime\": 1268783999.0,\n",
      "  \"site_id\": {\n",
      "    \"$oid\": \"666d5456f63ce193f690237e\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "query = {\"location\":{\n",
    "        '$nearSphere': {\n",
    "            '$geometry' : {\n",
    "                'type' : 'Point',\n",
    "                'coordinates' : [-105.0,32.5]\n",
    "            },\n",
    "            '$maxDistance' : 300000.0,\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "# A flaw in the current MongoDB implementation is\n",
    "# count_documents seems to not work with any geospatial \n",
    "# query.  If you remove this comment you will see \n",
    "# the error it throws.  If it works, it means MongoDB \n",
    "# developers fixed the problem\n",
    "#n=db.site.count_documents(query)\n",
    "cursor=db.site.find(query)\n",
    "for doc in cursor:\n",
    "    pretty_print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87c61c3-d365-4f47-966a-1df933e19ce5",
   "metadata": {},
   "source": [
    "Because of the pretty print of the full documents, that is a bit verbose, but it hopefully illustrates the point.  Although geospatial queries are complex, they have a lot of potential use for workflows that need to group data by the spatial location of stations (a \"virtual array\" concept) or by source (stacking of closely spaced sources).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a87b98-bd2c-461d-a784-54e5f7ae4451",
   "metadata": {},
   "source": [
    "### Sorting\n",
    "There are many situations where it is advantageous to \n",
    "sort the return of a query by one or more keys.   Sorting is technically a \"method of the CommandCursor object\" returned by a query but more magic happens when the client passes the query to the MongoDB server to assure the operation is done efficiently.   The reason I point that out here is mostly to clarify why the sort clause appears where it does in typical usage.  The User Manual addresses this in more detail, but here is an example that sorts \n",
    "channel documents to a form sensible for miniseed that \n",
    "uses the net:sta:chan:loc:time-interval as a unique \n",
    "key combination.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78189b0c-e2db-4dfb-b91e-e3c862ac5cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sta                    starttime                      endtime chan\n",
      "0  034A  2010-01-08T00:00:00.000000Z  2011-11-17T17:05:00.000000Z  BHE\n",
      "1  034A  2010-01-08T00:00:00.000000Z  2011-11-17T17:05:00.000000Z  BHN\n",
      "2  034A  2010-01-08T00:00:00.000000Z  2011-11-17T17:05:00.000000Z  BHZ\n",
      "3  035A  2010-01-12T00:00:00.000000Z  2011-11-14T17:40:00.000000Z  BHE\n",
      "4  035A  2010-01-12T00:00:00.000000Z  2011-11-14T17:40:00.000000Z  BHN\n",
      "5  035A  2010-01-12T00:00:00.000000Z  2011-11-14T17:40:00.000000Z  BHZ\n"
     ]
    }
   ],
   "source": [
    "# this is a test to verify sort syntax - delete when completed\n",
    "filter_clause = {\n",
    "    \"_id\":0,\n",
    "    \"sta\":1,\n",
    "    \"chan\":1,\n",
    "    \"starttime\":1,\n",
    "    \"endtime\":1,\n",
    "}\n",
    "sort_clause = [\n",
    "    (\"net\",1),\n",
    "    (\"sta\",1),\n",
    "    (\"chan\",1),\n",
    "    (\"starttime\",1),\n",
    "  ]\n",
    "cursor=db.channel.find({},filter_clause).sort(sort_clause).limit(6)\n",
    "doclist=[]\n",
    "for doc in cursor:\n",
    "    doclist.append(doc)\n",
    "from obspy import UTCDateTime\n",
    "for doc in doclist:\n",
    "    doc['starttime']=UTCDateTime(doc['starttime'])\n",
    "    doc['endtime']=UTCDateTime(doc['endtime'])\n",
    "print_as_table(doclist)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca2bde8-69c8-48f1-ab6f-b7bec85d4c65",
   "metadata": {},
   "source": [
    "Noting:\n",
    "1.  The \"sort\" function call appears after the find function with arguments.   That is the syntax because \"sort\" is a Cursor \"method\".\n",
    "2.  I added a second qualifier, limit, to only return the first 6 documents.  I did that just to keep the volume of the output under control.   The number return is much larger if you remove the `.limit(6)` qualifier.\n",
    "3.  I did a projection and used the `print_as_table` function we defined to make a more readable report. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51f6788-46b9-4b45-93fa-b895faecfdde",
   "metadata": {},
   "source": [
    "### The read_data method\n",
    "Now that  you have a basic understanding of MQL and the two \"Read\" operators in MongoDB called `find_one` and `find`, we return to the MsPASS workhorse method of [Database](http://www.mspass.org/python_api/mspasspy.db.html#module-mspasspy.db.database) called [read_data](http://www.mspass.org/python_api/mspasspy.db.html#mspasspy.db.database.Database.read_data).  It is the serial processing tool for loading data in MsPASS.   (In the third class of this course we will use the parallel version that is a function called [read_distributed_data](http://www.mspass.org/python_api/mspasspy.io.html#mspasspy.io.distributed.read_distributed_data).)  From the docstring realize first that [read_data](http://www.mspass.org/python_api/mspasspy.db.html#mspasspy.db.database.Database.read_data) is a method of [Database](http://www.mspass.org/python_api/mspasspy.db.html#module-mspasspy.db.database) and does NOT accept MQL commands at all.   What it does is driven by arg0 which must be one of two things or it will throw an exception:\n",
    "\n",
    "1.  A python dictionary with content sufficient to construct a `TimeSeries` or `Seismogram` object.  The simplest way to say that is it is a document from one of the \"wf\" collections of MsPASS:  \"wf_miniseed\", \"wf_TimeSeries\", or \"wf_Seismogram\".   Note we populated all of those already in our first class.\n",
    "2.  A [CommandCursor](https://pymongo.readthedocs.io/en/stable/api/pymongo/command_cursor.html) that points to one of the \"wf\" collections.\n",
    "\n",
    "For case 1 `read_data` will return an atomic datum (i.e. a `TimeSeries` or `Seismogram`) and the second will return an ensemble.   Although there are defaults it is is good practice to ALWAY add a value for the \"collection\" argument of `read_data` both for clarity and because it will abort in many cases if you don't.   The following block contains some variants of sections of code from our first class that are used here to show examples of what is we are discussing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce011d55-8ead-4aaa-9bbc-0817229281a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of return from read_data= <class 'mspasspy.ccore.seismic.TimeSeries'>\n",
      "Type of return from read_data= <class 'mspasspy.ccore.seismic.Seismogram'>\n",
      "Type of return from read_data= <class 'mspasspy.ccore.seismic.TimeSeries'>\n",
      "Type of return from read_data= <class 'mspasspy.ccore.seismic.TimeSeriesEnsemble'>\n",
      "Number of members in this ensemble= 1311\n",
      "Type of return from read_data= <class 'mspasspy.ccore.seismic.SeismogramEnsemble'>\n",
      "Number of members in this ensemble= 437\n"
     ]
    }
   ],
   "source": [
    "# atomic read from wf_miniseed\n",
    "doc = db.wf_miniseed.find_one()\n",
    "d = db.read_data(doc,collection='wf_miniseed')\n",
    "print(\"Type of return from read_data=\",type(d))\n",
    "# atomic read from wf_Seismogram\n",
    "doc = db.wf_Seismogram.find_one()\n",
    "d = db.read_data(doc,collection='wf_Seismogram')\n",
    "print(\"Type of return from read_data=\",type(d))\n",
    "# atomic read from default wf_TimeSeries\n",
    "doc = db.wf_TimeSeries.find_one()\n",
    "d = db.read_data(doc)\n",
    "print(\"Type of return from read_data=\",type(d))\n",
    "# ensemble read demonstration using source_id.   First get a valid source_id and then construct a query\n",
    "doc = db.source.find_one()\n",
    "sid=doc['_id']\n",
    "query={'source_id' : sid, 'data_tag' : 'serial_preprocessed'}\n",
    "# read ensemble from wf_TimeSeries with a cursor \n",
    "cursor = db.wf_TimeSeries.find(query)\n",
    "d = db.read_data(cursor,collection='wf_TimeSeries')  # collection could be dropped here but clearer to specify it\n",
    "print('Type of return from read_data=',type(d))\n",
    "print('Number of members in this ensemble=',len(d.member))\n",
    "# repeat for wf_Seismogram\n",
    "cursor = db.wf_Seismogram.find(query)\n",
    "d = db.read_data(cursor,collection='wf_Seismogram')  # collection could be dropped here but clearer to specify it\n",
    "print('Type of return from read_data=',type(d))\n",
    "print('Number of members in this ensemble=',len(d.member))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaad90e7-6dd6-4fe5-8dda-56a7efbf7ff7",
   "metadata": {},
   "source": [
    "## Update\n",
    "One has to do an \"update\" to a MongoDB database if you need to change the contents of one or more documents.  Database updates happen in the modern world in inconceivably huge numbers every day in commericial operations.  e.g. if you order something from Amazon all those tracking stages from your clicking history to the time a package is delivered to your home invoke a series of database transactions including, I presume, a lot of updates.  \n",
    "\n",
    "Although updates are a common requirement in commercial databases, a less obvious thing to most people is that updates are rarely if ever needed in data processing with a system like MsPASS.   Most data processing involves three stages:  1) read the data set, 2) process the data set, and 3) save the results.   Some processors may need to do read operations from the database, but updates are rarely needed.  They are also highly undesirable in a data-driven workflow like that because database transactions, from the computer's perspective, are like a human talking to someone on Jupiter; a response to the request for an update takes forever in terms of computer clock cycles.  For that reason, updates should be avoided in any workflow and should absolutely never be embedded in a large, parallel processing sequence. \n",
    "\n",
    "In MsPASS updates can nearly always be avoided by a simple, alternative approach:   if a change is needed that needs to be saved (e.g. you compute a set of new attributes from the data) simply post that data to the associated object's `Metadata` container.   In that model, when the final results are saved the newly computed attributes will be saved with the data.  Then the overhead of writing to the database is absorbed in the normally essential save step anyway.  \n",
    "\n",
    "With that long caveat, there are two standard ways to do updates:  `update_one` changes one document at a time, and `update_many` updates multiple documents with one client-server transaction.  Most people can understand usage of these two methods better by examples.  The examples below focus on updates to \"normalizing\" collections as that, from my experience, is the most common need for updates when using MsPASS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82811e21-bd1c-4a73-8d2c-155876523417",
   "metadata": {},
   "source": [
    "Finally, we emphasize that the idea is that all MsPASS processing is normally driven by a list of documents from a wf collection.   Sometimes we process the entire collection.  An example is the first waveform processing loop in session 1 that we drove with wf_miniseed.  There we used this construct:\n",
    "```\n",
    "cursor=db.wf_miniseed.find({})   # {} mean all - now that you know MQL rules you should understand why\n",
    "for doc in cursor:\n",
    "    d = db.read_data(doc,collection='wf_miniseed')\n",
    "```\n",
    "Most processing, however, uses some form of query to limit what is passed through the processing chain.   A nearly universal one is a limit on the \"data_tag\" attribute you should always used to define the result of a particular save at a particular stage of processing.  For example, the box below does nothing but read all the data wf_Seismogram with the source_id value we set above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2447e2f2-1f98-4b4b-884d-7bb72f19728d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents with source_id= 666d53c3f63ce193f69019b0  is  1311\n",
      "Time to read  1311  TimeSeries objects was  8.648014783859253\n"
     ]
    }
   ],
   "source": [
    "query={'source_id' : sid}\n",
    "n = db.wf_TimeSeries.count_documents(query)  \n",
    "print(\"Number of documents with source_id=\",sid,\" is \",n)\n",
    "t0 = time.time()\n",
    "cursor = db.wf_TimeSeries.find(query)\n",
    "for doc in cursor:\n",
    "    d = db.read_data(doc,collection='wf_TimeSeries')\n",
    "t = time.time()\n",
    "print(\"Time to read \",n,' TimeSeries objects was ',t-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd08833-75e8-4999-b12a-09b0edba1d52",
   "metadata": {},
   "source": [
    "## Update\n",
    "One has to do an \"update\" to a MongoDB database if you need to change the contents of one or more documents.  Database updates happen in the modern world in inconceivably huge numbers every day in commericial operations.  e.g. if you order something from Amazon all those tracking stages from your clicking history to the time a package is delivered to your home invoke a series of database transactions including, I presume, a lot of updates.  \n",
    "\n",
    "Although updates are a common requirement in commercial databases, a less obvious thing to most people is that updates are rarely if ever needed in data processing with a system like MsPASS.   Most data processing involves three stages:  1) read the data set, 2) process the data set, and 3) save the results.   Some processors may need to do read operations from the database, but updates are rarely needed.  They are also highly undesirable in a data-driven workflow like that because database transactions, from the computer's perspective, are like a human talking to someone on Jupiter; a response to the request for an update takes forever in terms of computer clock cycles.  For that reason, updates should be avoided in any workflow and should absolutely never be embedded in a large, parallel processing sequence. \n",
    "\n",
    "In MsPASS updates can nearly always be avoided by a simple, alternative approach:   if a change is needed that needs to be saved (e.g. you compute a set of new attributes from the data) simply post that data to the associated object's `Metadata` container.   In that model, when the final results are saved the newly computed attributes will be saved with the data.  Then the overhead of writing to the database is absorbed in the normally essential save step anyway.  \n",
    "\n",
    "With that long caveat, there are two standard ways to do updates:  `update_one` changes one document at a time, and `update_many` updates multiple documents with one client-server transaction.  Most people can understand usage of these two methods better by examples.  The examples below focus on updates to \"normalizing\" collections as that, from my experience, is the most common need for updates when using MsPASS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7c6dd2-6bed-4ff4-a4e3-fdf1b0003639",
   "metadata": {},
   "source": [
    "### update_one example\n",
    "Suppose we learned that the recording period for a seismic station are wrong.  That is, with SEED data station information has a time period for which the data are considered valid.   That period is defined by two attributes with the keys \"starttime\" and \"endtime\"  Changing these fields would be highly unusual for data downloaded from the FDSN, but is not at all uncommon for portable deployments while the experiment is in progress.  Our example is contrived as what we are about to do will make the entry we edit wrong.   So the hypothetical situation we are modeling is that we imagine we learned we the \"endtime\" for station O34A is wrong.  We first query the site collection to verify what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68d6d8f8-0eae-4605-a3e1-45871e6780d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents for station O34A =  1\n",
      "O34A 2010-06-11T00:00:00.000000Z 2012-04-18T23:59:59.000000Z\n"
     ]
    }
   ],
   "source": [
    "from obspy import UTCDateTime\n",
    "query={'sta' : 'O34A'}\n",
    "# verify there is only one entry - not always true with this query\n",
    "ndocs=db.site.count_documents(query)\n",
    "print('Number of documents for station O34A = ',ndocs)\n",
    "doc=db.site.find_one(query)\n",
    "print(doc['sta'],\n",
    "    UTCDateTime(doc['starttime']), UTCDateTime(doc['endtime']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97d575f-583b-4eed-8eb8-75de29ffd722",
   "metadata": {},
   "source": [
    "We say, \"ahh the endtime should have been on March 19 not March 18 and our field notes show the actual time was 13:44 UTC. \"   We can make that change with this use of update one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a6f88d4-091d-4654-9538-80c724d3d324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated data for O34A\n",
      "O34A 2010-06-11T00:00:00.000000Z 2012-04-19T13:44:00.000000Z\n"
     ]
    }
   ],
   "source": [
    "new_time=UTCDateTime('2012-04-19T13:44:00.0Z')\n",
    "update_doc={ '$set' :\n",
    "            {'endtime' : new_time.timestamp}\n",
    "           }\n",
    "db.site.update_one(query,update_doc)\n",
    "print('Updated data for O34A')\n",
    "doc=db.site.find_one(query)\n",
    "print(doc['sta'],\n",
    "    UTCDateTime(doc['starttime']), UTCDateTime(doc['endtime']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e75e52-7a6b-4900-9867-48e109f2e2d4",
   "metadata": {},
   "source": [
    "Notice update_one has two required arguments: arg0 is a query operator and arg1 is required to be an 'operator' meaning in has to use one of the 'dollar' operators discussed above.  This one uses '$set' with means replace the value.  In my experience, that is the most common operator for updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea967de6-3bff-4458-8c97-8fac0740d19a",
   "metadata": {},
   "source": [
    "### update_many example\n",
    "The basic argument structure required for `update_many` is the same as `update_one`.   The difference is you should use `update_many` when the query in arg0 is expected to return more than one document that are to be modified.  The example below is the same as  for `update_one` but applied to the \"channel\" collection.   As the `count_documents` output shows below the same query yields 3 documents for channel because the site has a three component sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "109cc173-033c-455e-9ebd-d30a9438a669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of channel documents for O34A= 3\n",
      "Updated data for O34A\n",
      "O34A BHE 2010-06-11T00:00:00.000000Z 2012-04-19T13:44:00.000000Z\n",
      "O34A BHN 2010-06-11T00:00:00.000000Z 2012-04-19T13:44:00.000000Z\n",
      "O34A BHZ 2010-06-11T00:00:00.000000Z 2012-04-19T13:44:00.000000Z\n"
     ]
    }
   ],
   "source": [
    "ndocs=db.channel.count_documents(query)\n",
    "print('number of channel documents for O34A=',ndocs)\n",
    "# we use the same query and update_doc as above\n",
    "db.channel.update_many(query,update_doc)\n",
    "print('Updated data for O34A')\n",
    "cursor=db.channel.find(query)\n",
    "for doc in cursor:\n",
    "    print(doc['sta'],doc['chan'],\n",
    "      UTCDateTime(doc['starttime']), \n",
    "      UTCDateTime(doc['endtime']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772c4d65-c431-42d4-a521-c413a959d898",
   "metadata": {},
   "source": [
    "## Delete\n",
    "The API for deleting documents is very similar to that for find.  There is a `delete_one` method to delete a single document and a `delete_many` method that more-or-less does a find followed by deleting each document the query found.  For instance, the following deletes what we just updated in channel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d641966-3dd2-4493-8215-7291e06dae17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of channel documents for O34A before delete= 3\n",
      "number of channel documents for O34A after delete_many= 0\n"
     ]
    }
   ],
   "source": [
    "# repeating this query to be clear but not required in this context\n",
    "query={'sta' : 'O34A'}\n",
    "ndocs=db.channel.count_documents(query)\n",
    "print('number of channel documents for O34A before delete=',ndocs)\n",
    "ret=db.channel.delete_many(query)\n",
    "ndocs=db.channel.count_documents(query)\n",
    "print('number of channel documents for O34A after delete_many=',ndocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489e35e4-2450-44a7-8233-985a00ed4f37",
   "metadata": {},
   "source": [
    "Handling deletions of waveform data is a much more difficult problem.   In MsPASS there is a special method of our `Database` class called `delete_data`.  That method has to do a lot more than just call the `delete_one` method to remove the database document.  There are two reasons for that:\n",
    "1.  In MsPASS the sample data, which are typically orders of magnitude larger than the \"document\" saved in MongoDB, are stored separately from the \"document\" of name-value pairs.\n",
    "2.  MsPASS also support multiple \"storage modes\" for how to handle the sample data.   It also allow multiple \"format\"s for how that data is represented externally (e.g. miniseed is a \"format\" that is light years from the natural representation of seismic data). At this time there are three basic \"storage modes\":  (1) \"file\", (2) \"gridfs\", and \"url\".  How they need to be handled with a \"delete\" operation is very different.  When \"storage_mode\" is set to \"file\" the sample data are stored in a file system in a set of files.  There the problem is one file should normally contain many waveforms so if a lot of editing is done data will be stranded.  MsPASS has a way to automatically delete files that no longer contain a reference in the database to reduce debris, but it only works if the entire file content is deleted.   Using \"gridfs\" storage is a simpler problem as our waveform delete operator will automatically clear sample data stored in the gridfs system.  If your application requires a lot of editing to remove stale waveforms, gridfs is by far the best choice.  Finally, \"URL\" is pretty much defined to be read-only so the only thing that happens for data indexed that way is that the document vanishes. For data access via the cloud with the new Earthscope system this mode may become common.     \n",
    "\n",
    "One common application of `delete_data` is to clear some temporary save copy that is no longer needed.  In MsPASS when data are saved we recommend ALWAYS using the \"data_tag\" argument to provide a unique tag for data at a specific stage of processing.   With that understand, suppose we saved an intermediate copy of a working dataset with the `data_tag=\"preprocessed\"` and we wanted to clear the disk space associated with that intermediate copy.  The following simple code box would do that (Note it will do nothing here because the db we have been using contains no waveform data so I disabled the code box):  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "50a91782-6463-4b75-a491-96f6ef4a5a77",
   "metadata": {},
   "source": [
    "query={'data_tag' : 'preprocessed'}\n",
    "cursor = db.wf_TimeSeries.find(query)\n",
    "for doc in cursor:\n",
    "    db.delete_data(doc['_id'],\"TimeSeries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe12f2de-4ef0-49eb-88fc-632d924bab49",
   "metadata": {},
   "source": [
    "Note arg0 of this method (currently) requires the ObjectId of the document to be deleted.  arg1 must be either \"TimeSeries\" or \"Seismogram\" or the method will throw an exception."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a889dc-d1a1-4095-b3e6-98ea1cf0b032",
   "metadata": {},
   "source": [
    "## Importing Tablular Data\n",
    "A final point we want to teach in this session is the utility of MongoDB for importing all kinds of weird data.   You can find a more lengthy discussion of the ideas in [this section](http://www.mspass.org/user_manual/importing_tabular_data.html) of the User Manual.  There are two key points we highlight to motivate why you should listen to this:\n",
    "1.  Cutting-edge research often involves reading and managing nonstandard data.   MongoDB is the best solution we know of for managing weird data because a \"document\" is a container than can hold just about anything we have encountered. \n",
    "2.  A large fraction of open data are distributed completely or in part as tables of information.  As a result there is a rich ecosystem for handling tabular data in python that are automatically available and packaged with MsPASS.   Examples include readers for csv files, fixed format text files, and readers to interact with any SQL database server.\n",
    "\n",
    "We encourage you to read the User Manual page in the link above at your leisure along with examples found in a similar tutorial to this one found [here](https://github.com/mspass-team/mspass_tutorial/blob/master/notebooks/mongodb_tutorial.ipynb). \n",
    "\n",
    "A special case in seismology is interaction with relational database systems.   Most regional networks today use some form of relational database to manage some or all of their data.   If, in your work, you need to interact with the information system of some provider that utilizes an SQL server and you can get read access to the database, follow the link above to our User Manual section discussing this topic.   (There are standard tools in Pandas and dask to interact with SQL servers.)   A special case in our community is the \"flat file\" database system developed originally in the 1980s for the IRIS Joint Seismic Program originally called \"Datascope\".   At the end of the Joint Seismic Program the authors of Datascope spun of the software company called [Boulder Real Time Technologies](https://brtt.com/) using it as the the framework for their real-time seismic network monitoring software they called \"Antelope\".   Their software is used in several US seismic networks and many others around the world.  It is also used by PASSCAL for some elements of experimental data handling.  Furthermore, US seismolog research scientists not operating seismic networks can obtain a license for their software at no cost.    i.e. there are many places you can find Datascope tables that contain useful data for research in our community.   A type example is the phase picks made by the USArray network facility we will examine here that were originally downloaded from the [Array Network Facillity (ANF) website](https://anf.ucsd.edu/tools/events/).  We close this session with a brief demonstration of the special tool recently developed for MsPASS for working with a Datascope database. \n",
    "\n",
    "The MsPASS tool for working with Datascope is a special database class we call [DatascopeDatabase](http://www.mspass.org/python_api/mspasspy.preprocessing.html#mspasspy.preprocessing.css30.datascope.DatascopeDatabase).  In the working directory for this tutorial is a directory  containing picks made by the ANF from the Earthscope TA in January of 2011.  We downloaded and unpacked from the ANF site referenced above.  It produced the content of the directory you should see in the jupyter lab file pane called \"events_usarray_2011_11\".   The database tables are in that directory and all begin with the \"database name\" of \"usarray_2011_11\".  With that background we can create a `DatascopeDatabase` handle to that data with this incantation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ecff759-b70a-4193-b58a-5c8b8995cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mspasspy.preprocessing.css30.datascope import DatascopeDatabase\n",
    "# Temporary until bug is repaired - should be able to remove pffile arg when that is repaired\n",
    "dsdb = DatascopeDatabase(\"events_usarray_2011_01/usarray_2011_01\",pffile=\"DatascopeDatabase.pf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96318ee8-4e77-4183-8921-be2dbc5c5729",
   "metadata": {},
   "source": [
    "The [DatascopeDatabase](http://www.mspasdf = dscopeDb.CSS30Catalog2df()\n",
    "dfs.org/python_api/mspasspy.preprocessing.html#mspasspy.preprocessing.css30.datascope.DatascopeDatabase) class has a number of useful methods, but the most useful one for this example is one called `CSS30Catalog2df`.   It creates a large table that is the full \"catalog\" of picks that includes cross-referencing \"joins\" defining source locations associated with each pick.  This final box is a terse example that illlustrates this functionality.  This approach can be used to jump start any study with TA data that would benefit from these phase picks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "768d3b0f-ce1c-4af0-beee-5c3792937904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           evid evname    prefor        auth  commid        lddate      lat  \\\n",
      "0      212668.0      -  349724.0  QED_weekly    -1.0  1.321650e+09  27.2470   \n",
      "1      212668.0      -  349724.0  QED_weekly    -1.0  1.321650e+09  27.2470   \n",
      "2      212668.0      -  349724.0  QED_weekly    -1.0  1.321650e+09  27.2470   \n",
      "3      212668.0      -  349724.0  QED_weekly    -1.0  1.321650e+09  27.2470   \n",
      "4      212668.0      -  349724.0  QED_weekly    -1.0  1.321650e+09  27.2470   \n",
      "...         ...    ...       ...         ...     ...           ...      ...   \n",
      "52427       NaN    NaN       NaN         NaN     NaN           NaN      NaN   \n",
      "52428  213019.0      -  350200.0    ANF:tcox    -1.0  1.320339e+09  47.5331   \n",
      "52429       NaN    NaN       NaN         NaN     NaN           NaN      NaN   \n",
      "52430       NaN    NaN       NaN         NaN     NaN           NaN      NaN   \n",
      "52431       NaN    NaN       NaN         NaN     NaN           NaN      NaN   \n",
      "\n",
      "            lon    depth          time  ...  amp  per  logat  clip  fm  \\\n",
      "0      143.1660  10.0000  1.293840e+09  ... -1.0 -1.0 -999.0     -   -   \n",
      "1      143.1660  10.0000  1.293840e+09  ... -1.0 -1.0 -999.0     -   -   \n",
      "2      143.1660  10.0000  1.293840e+09  ... -1.0 -1.0 -999.0     -   -   \n",
      "3      143.1660  10.0000  1.293840e+09  ... -1.0 -1.0 -999.0     -   -   \n",
      "4      143.1660  10.0000  1.293840e+09  ... -1.0 -1.0 -999.0     -   -   \n",
      "...         ...      ...           ...  ...  ...  ...    ...   ...  ..   \n",
      "52427       NaN      NaN           NaN  ... -1.0 -1.0 -999.0     -   -   \n",
      "52428  -92.6627   7.6549  1.296497e+09  ... -1.0 -1.0 -999.0     -   -   \n",
      "52429       NaN      NaN           NaN  ... -1.0 -1.0 -999.0     -   -   \n",
      "52430       NaN      NaN           NaN  ... -1.0 -1.0 -999.0     -   -   \n",
      "52431       NaN      NaN           NaN  ... -1.0 -1.0 -999.0     -   -   \n",
      "\n",
      "           snr  qual     auth_arrival commid_arrival  lddate_arrival  \n",
      "0      -1.0000     -   dbp:tcox:11307             -1    1.320348e+09  \n",
      "1      -1.0000     -   dbp:tcox:11322             -1    1.321650e+09  \n",
      "2      -1.0000     -   dbp:tcox:11307             -1    1.320348e+09  \n",
      "3      -1.0000     -   dbp:tcox:11307             -1    1.320348e+09  \n",
      "4      -1.0000     -   dbp:tcox:11322             -1    1.321650e+09  \n",
      "...        ...   ...              ...            ...             ...  \n",
      "52427  13.5500     -         dbevproc             -1    1.337040e+09  \n",
      "52428  -1.0000     -  dbp:vladik:1103             -1    1.296500e+09  \n",
      "52429  17.4140     -         dbevproc             -1    1.337040e+09  \n",
      "52430   4.8019     -         dbevproc             -1    1.337040e+09  \n",
      "52431  14.4180     -         dbevproc             -1    1.337040e+09  \n",
      "\n",
      "[52432 rows x 74 columns]\n"
     ]
    }
   ],
   "source": [
    "df = dsdb.CSS30Catalog2df()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5621fbc9-2bb2-4ad0-bfc9-ca6c7d62a359",
   "metadata": {},
   "source": [
    "Noting the NaNs result form \"unassociated\" picks meaning an analyst picked phase that couldn't be associated with any know earthquake.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa229040-cc2f-464b-977e-4a9ec9a3b93c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
